{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# LangGraph 篇\n",
    "\n",
    "LangGraph 是一个用于编排、运行和监控有状态、多行动者（multi-actor）应用程序的框架，特别适用于构建基于大型语言模型（LLM）的复杂代理系统。它建立在 LangChain 的基础之上，并扩展了 LangChain 表达式语言（LCEL），使其能够以循环方式协调多个链（或行动者）进行多步计算 。LangGraph 允许开发者将应用程序逻辑组织成有向图，从而更直观、更灵活地构建复杂的对话流和代理行为 。   \n",
    "\n",
    "与 LangChain 主要关注任务的线性链接不同，LangGraph 专注于通过基于图的编排、持久状态管理和多智能体协调来处理更复杂的工作流 。这使得 LangGraph 非常适合需要动态决策、迭代过程和人类参与的应用程序 。开发者选择 LangGraph 的原因包括其可靠性与可控性、底层可扩展性以及一流的流式处理支持 。通过状态检查点和人工循环审批，可以引导代理行为，而持久化上下文则确保长时间运行工作流的连贯性 。其底层原语允许构建高度定制化的代理，避免了限制性抽象，并支持可扩展的多智能体系统设计 。此外，LangGraph 对令牌级流式传输和中间步骤流式传输的支持，为用户提供了对代理推理和行动的实时可见性 。   \n",
    "\n",
    "LangGraph 并非要取代 LangChain，而是对其的扩展，提供了一个更易于访问、用户友好的可视化界面（通过 LangGraph Studio）和更强大的状态管理及循环控制能力 。许多现实世界的应用程序需要多个代理协同工作，LangGraph 提供的工具集有助于应对编排复杂性、可观察性和调试限制等挑战 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：LangGraph 简介"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph 是什么？\n",
    "LangGraph 是一个旨在构建有状态、多智能体应用程序的框架，它利用 LLM 来创建能够利用工具、API 和数据库的复杂 AI 代理工作流 。它将代理交互实现为有状态图，其中节点代表函数或计算步骤，通过边连接 。LangGraph 在所有节点和边之间维护一个共享的代理状态，支持内置持久化、人工干预以及处理包含循环和分支的复杂工作流 。   \n",
    "\n",
    "与传统的 LangChain 链和代理相比，LangGraph 允许实现更复杂的代理工作流，并具有更高的灵活性 。例如，代理可以循环调用工具，检索信息，然后再次调用相同或不同的工具以获取后续信息 。这种循环图的能力是 LangGraph 的一个显著特点，使其能够处理需要迭代细化或动态决策的场景 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph 与 LangChain 的关系\n",
    "\n",
    "LangGraph 构建于 LangChain 之上，是 LangChain 生态系统的一部分，旨在扩展 LangChain 的能力，特别是针对需要更复杂控制流（如循环和条件分支）和持久状态管理的应用 。LangChain 擅长创建线性的、简单的 LLM 工作流，通过链接不同的语言模型操作来实现内容生成、翻译等任务 。而 LangGraph 则通过图结构来编排这些操作，允许更复杂的交互和状态管理 。   \n",
    "\n",
    "可以认为 LangChain 提供了构建 LLM 应用的基础模块和工具，而 LangGraph 则提供了一种将这些模块组织成更强大、更灵活的代理系统的方法 。LangGraph 并非取代 LangChain，而是增强它，特别是在构建需要多个代理协作、具有记忆能力以及能够在不确定情况下进行迭代处理的复杂应用时 。许多 LangChain 的核心组件和概念（如 LCEL、工具、模型集成）在 LangGraph 中仍然适用和重要。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 核心优势与适用场景\n",
    "1. LangGraph 因其独特的功能组合，在构建高级 AI 应用方面展现出显著优势：\n",
    "\n",
    "**可靠性与可控性 (Reliability and Controllability)**：LangGraph 允许通过审核检查和“人机协同”（Human-in-the-Loop, HIL）审批来精确控制代理的行为。它能够为长时间运行的工作流持久化上下文，确保代理按照既定方向执行任务 。 \n",
    "\n",
    "**底层与可扩展性 (Low-level and Extensible)**：LangGraph 提供描述性的底层原语，使开发者能够构建完全定制化的代理，而不受限于可能阻碍定制化需求的僵硬抽象。这使得设计可扩展的多智能体系统成为可能，其中每个智能体都可以根据具体用例定制其角色和功能 。\n",
    "\n",
    "**一流的流式处理支持 (First-class Streaming Support)**：LangGraph 支持令牌级流式传输和中间步骤的流式传输。这为用户提供了代理推理过程和行为的实时清晰可见性 。   \n",
    "循环图与复杂工作流 (Cyclical Graphs and Complex Workflows)：与许多只支持有向无环图（DAG）的框架不同，LangGraph 支持循环图。这使得代理能够以循环方式与工具交互，例如，调用工具、检索信息，然后根据需要再次调用工具以获取更多信息或进行迭代细化。这种能力对于实现复杂的推理循环和自适应行为至关重要 。   \n",
    "\n",
    "**持久化与状态管理 (Persistence and State Management)**：LangGraph 内置持久化机制，引入了在工作流中所有节点和边之间共享的代理状态概念。这不仅支持自动错误恢复，还使得工作流能够从中断处继续执行。此外，它还支持短期和长期记忆，以维护对话历史和上下文 。   \n",
    "\n",
    "**人机协同 (Human-in-the-Loop)**：原生支持在代理工作流中进行人工干预。图执行可以在特定点中断，允许人工审查、批准或编辑代理的建议响应，然后继续执行 。   \n",
    "\n",
    "2. 适用场景包括：\n",
    "\n",
    "**高级聊天机器人**：需要灵活和定制化交互的复杂聊天机器人 。   \n",
    "\n",
    "**交互式 AI 系统**：需要管理和执行复杂语言任务，并具有适应性和可定制运行时的系统 。   \n",
    "\n",
    "**自动化工作流**：例如报告生成、数据处理或邮件推广等重复性任务的自动化 。   \n",
    "\n",
    "**多智能体协作系统**：如需要多个专业代理协同完成复杂任务的场景，例如 LinkedIn 的 AI 招聘系统、Uber 的代码迁移、Replit 的软件构建助手、Elastic 的实时威胁检测系统 。   \n",
    "\n",
    "**信息检索的 RAG 管道**：构建复杂的检索增强生成（RAG）管道，特别是在数据有限或嘈杂的领域 。   \n",
    "\n",
    "**需要长期记忆和个性化服务的应用**：例如，能够记住用户偏好并据此调整响应的客服代理 。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 第二部分：LangGraph 快速入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### 配置环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建虚拟环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# !conda create -n langgraph python=3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 安装 LangGraph 及相关依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langgraph langsmith \"langchain[anthropic]\"\n",
    "# !pip install langgraph langchain langchain-openai langchain-deepseek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果计划使用 LangGraph 平台进行部署或本地开发服务器，还需要安装 LangGraph CLI ：   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Python >= 3.11\n",
    "# !pip install \"langgraph-cli[inmem]\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 配置 API 密钥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 配置 LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# os.environ[\"LANGCHAIN_API_KEY\"] = \"ls__...\" # 你的 LangSmith API 密钥\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"pr-upbeat-soda-93\" # 可选的项目名称"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 构建第一个简单的图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建 LangGraph 应用的核心是定义一个状态图 (StateGraph)，向其添加节点 (Node) 和边 (Edge)，然后编译它。\n",
    "\n",
    "以下是一个极简的示例，演示了图的基本构建块："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleState 定义了图在执行过程中将跟踪的数据结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedDict\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph, END, START\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 1. 定义图的状态 (State)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleState\u001b[39;00m(TypedDict):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'"
     ]
    }
   ],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "# 1. 定义图的状态 (State)\n",
    "class SimpleState(TypedDict):\n",
    "    input_message: str\n",
    "    output_message: str\n",
    "\n",
    "# 2. 定义节点函数 (Nodes)\n",
    "def process_message_node(state: SimpleState):\n",
    "    # 节点接收当前状态，执行逻辑，并返回对状态的更新\n",
    "    processed_text = state[\"input_message\"] + \" - Processed!\"\n",
    "    return {\"output_message\": processed_text}\n",
    "\n",
    "# 3. 构建图 (Graph Construction)\n",
    "# 初始化 StateGraph，并传入状态定义\n",
    "builder = StateGraph(SimpleState)\n",
    "\n",
    "# 添加节点\n",
    "builder.add_node(\"processor\", process_message_node) # \"processor\" 是节点名称\n",
    "\n",
    "# 定义图的边 (Edges)\n",
    "builder.set_entry_point(\"processor\") # 设置入口节点\n",
    "builder.add_edge(\"processor\", END) # 从 \"processor\" 节点连接到特殊结束节点 END\n",
    "\n",
    "# 4. 编译图 (Compile the Graph)\n",
    "app = builder.compile()\n",
    "\n",
    "# 5. 执行图 (Invoke the Graph)\n",
    "initial_state = {\"input_message\": \"Hello LangGraph\"}\n",
    "final_state = app.invoke(initial_state)\n",
    "print(final_state)\n",
    "# 预期输出: {'input_message': 'Hello LangGraph', 'output_message': 'Hello LangGraph - Processed!'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以使用 get_graph 方法和其中一种“绘制”方法（如 draw_ascii 或 draw_png）来可视化图。这些 draw 方法各自需要额外的依赖项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个简单的例子展示了 LangGraph 的核心工作流程：定义状态、创建节点、连接边、编译图，最后执行图。更复杂的应用将在此基础上扩展，加入条件边、工具、持久化和人机交互等特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：核心构建模块详解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph 的核心在于其三大构建模块：状态（State）、节点（Node）和边（Edge）。这些组件协同工作，定义了应用程序的数据流和控制流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义图状态 (State)\n",
    "\n",
    "状态是 LangGraph 应用程序的核心，它是一个共享的数据结构，代表了应用程序在任何给定时间点的快照，并在节点之间传递 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schema 定义:\n",
    "状态的 schema 定义了图的结构，并作为图中所有节点和边的输入 schema 。   \n",
    "\n",
    "- TypedDict: 这是定义状态 schema 最常见且推荐的方式，可以清晰地指定状态中每个键的名称和类型 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from typing import List, Dict\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    input_query: str\n",
    "    intermediate_results: List[str]\n",
    "    final_answer: str\n",
    "    error_message: str\n",
    "    # 更多状态字段..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pydantic BaseModel: 也可以使用 Pydantic 的 BaseModel 来定义状态 schema。这提供了额外的好处，如默认值设定和运行时数据验证 。但需要注意，根据文档，运行时验证主要发生在节点输入时，而非节点输出，且验证错误追溯可能不直接指向出错节点 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入/输出 Schemas\n",
    "默认情况下，图的输入 schema 和输出 schema 由其整体状态 schema 决定 。然而，LangGraph 允许定义明确的、不同的输入和输出 schema，这在某些状态键仅用于内部传递而不需要暴露给外部调用者时非常有用 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 状态更新与 Reducers\n",
    "\n",
    "节点执行后会返回一个字典，其中包含对状态的更新 。状态如何根据这些更新进行演变，由 Reducer 函数控制。   \n",
    "\n",
    "- **默认行为 (覆盖)**: 如果没有为状态中的某个键指定 Reducer，那么来自节点的任何对该键的更新都将直接覆盖其现有值 。   \n",
    "\n",
    "- **自定义 Reducers**: Reducer 是决定如何将新数据合并到现有状态键中的函数。通过 typing.Annotated，可以为状态字典中的特定字段指定一个 Reducer 函数 。 例如，要将新项附加到列表中，而不是覆盖整个列表，可以使用 operator.add 作为 Reducer："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated\n",
    "\n",
    "class MyListState(TypedDict):\n",
    "    my_list: Annotated[list, operator.add]\n",
    "    # 如果另一个节点返回 {\"my_list\": [\"new_item\"]}\n",
    "    # 并且当前状态是 {\"my_list\": [\"old_item\"]}\n",
    "    # 更新后的状态将是 {\"my_list\": [\"old_item\", \"new_item\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- add_messages Reducer: 这是一个 LangGraph 内置的、专门用于处理消息列表（如对话历史）的 Reducer 。\n",
    "\n",
    "    - 它能智能地将新消息追加到现有消息列表。\n",
    "    - 更重要的是，它能够根据消息 ID 更新已存在的消息，这对于实现人机协同场景（例如，人工编辑了代理的某条消息）至关重要，避免了简单追加导致的重复或冲突。\n",
    "    - add_messages 还会尝试将输入的消息（可以是字典格式）反序列化为 LangChain 的 Message 对象。\n",
    "    - 为了方便，LangGraph 提供了预定义的 MessagesState，它是一个 TypedDict，其中包含一个名为 messages 的键，并已使用 add_messages 作为其 Reducer 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducer 是控制状态演化的关键机制。它们允许开发者在状态定义中以声明方式精确控制状态转换逻辑。这对于维护数据完整性和在有状态应用中实现期望行为至关重要，特别是在处理列表、计数器或需要复杂合并逻辑的对象时。add_messages 的设计也反映了 LangGraph 对常见聊天应用模式的内置支持。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建节点 (Nodes)\n",
    "\n",
    "节点是 LangGraph 图的基本计算单元，代表工作流中的离散处理步骤或功能模块 。   \n",
    "\n",
    "#### 定义与实现\n",
    "节点通常实现为 Python 函数（可以是同步或异步的）。它们也可以是 LangChain 表达式语言（LCEL）中的 Runnable 对象 。   \n",
    "\n",
    "#### 节点函数签名\n",
    "- 节点的第一个位置参数总是当前的图状态 (State) 对象 。   \n",
    "- （可选）第二个位置参数可以是 RunnableConfig 对象，用于传递运行时配置，如 thread_id 或 user_id 。   \n",
    "- 节点函数执行其逻辑后，应返回一个字典，其中包含需要更新的状态键及其新值。框架将使用这些更新来修改中心状态 。\n",
    "\n",
    "#### 添加节点到图\n",
    "使用 StateGraph 构建器的 add_node(name: str, action: Union) 方法来添加节点 。   \n",
    "\n",
    "- name: 节点的唯一字符串标识符。\n",
    "- action: 节点要执行的 Python 函数或 Runnable。 如果只传递一个函数给 add_node 而不指定名称，则函数名将默认成为节点名 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f24af47ec0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "class AppState(TypedDict):\n",
    "    input: str\n",
    "    processed_input: str\n",
    "    tool_output: str\n",
    "\n",
    "def process_data_node(state: AppState, config: RunnableConfig):\n",
    "    # 示例：访问配置和状态\n",
    "    user_id = config.get(\"configurable\", {}).get(\"user_id\", \"default_user\")\n",
    "    print(f\"Node processing for user: {user_id}\")\n",
    "    return {\"processed_input\": state[\"input\"].upper()}\n",
    "\n",
    "def tool_node(state: AppState):\n",
    "    # 模拟工具调用\n",
    "    return {\"tool_output\": f\"Tool result for {state['processed_input']}\"}\n",
    "\n",
    "graph_builder = StateGraph(AppState)\n",
    "graph_builder.add_node(\"data_processor\", process_data_node)\n",
    "graph_builder.add_node(\"external_tool\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 特殊节点\n",
    "- START: 一个虚拟的特殊节点，代表图的入口点，用于指定当图接收到初始输入时首先应该执行哪个（或哪些）节点 。   \n",
    "- END: 一个特殊的终止节点。当图的执行流到达一个指向 END 的边时，该执行路径结束 。\n",
    "\n",
    "LangGraph 中的节点设计促进了模块化和松耦合。每个节点接收完整的当前状态，执行其特定逻辑，并仅返回其希望对状态进行的更改。这种“共享黑板”（状态）模式使得节点开发可以相对独立，并且只要状态交互接口保持一致，修改或替换节点逻辑就不会影响图的其他部分。RunnableConfig 的引入进一步增强了这种灵活性，允许在不改变节点核心逻辑或状态定义的情况下，向节点传递运行时参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义边与控制流 (Edges)\n",
    "边在 LangGraph 中定义了节点之间的连接和执行流程，决定了计算如何在图中从一个步骤流向下一个步骤 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置入口点 (Entry Point)\n",
    "\n",
    "图的执行需要一个起点。\n",
    "\n",
    "- 可以使用 `graph_builder.set_entry_point(node_name: str)` 方法来指定当图首次被调用时，哪个节点应该首先执行 。   \n",
    "- 另一种等效的方法是添加一条从特殊 START 节点到初始节点的边：`graph_builder.add_edge(START, node_name) `。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 普通边 (Normal Edges / Unconditional Transitions)\n",
    "\n",
    "当一个节点完成后总是需要转换到另一个特定节点时，使用普通边。\n",
    "\n",
    "- 通过 graph_builder.add_edge(start_key: Union[str, list[str]], end_key: str) 方法定义 。   \n",
    "- start_key 是起始节点的名称（或名称列表）。\n",
    "- end_key 是目标节点的名称。\n",
    "- 如果 start_key 是一个节点列表，则 end_key 节点会在所有 start_key 节点都执行完毕后才执行（这实现了“扇入”逻辑）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f24af47ec0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 示例：从 data_processor 节点到 external_tool 节点\n",
    "graph_builder.add_edge(\"data_processor\", \"external_tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件边 (Conditional Edges)\n",
    "条件边允许图根据当前状态动态地选择下一个执行路径，这对于实现复杂的逻辑和代理行为至关重要。\n",
    "\n",
    "- 使用 `graph_builder.add_conditional_edges(source: str, path: Callable, path_map: Optional[dict] = None) `方法定义 。   \n",
    "- `source`: 条件判断所依据的源节点的名称。\n",
    "- `path`: 一个 Python 可调用对象（函数），它接收当前图状态作为输入，并返回一个字符串（或字符串列表，用于并行执行到多个节点），该字符串指示下一个要路由到的节点的名称。\n",
    "- `path_map` (可选): 一个字典，用于将 path 函数的返回值映射到实际的节点名称或特殊节点 END。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1f24af47ec0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decide_next_step(state: AppState) -> str:\n",
    "    if \"error\" in state.get(\"tool_output\", \"\"):\n",
    "        return \"handle_error_node\" # 假设有这个节点\n",
    "    elif len(state[\"tool_output\"]) > 10:\n",
    "        return \"summarize_output_node\" # 假设有这个节点\n",
    "    else:\n",
    "        return END # 结束执行\n",
    "\n",
    "# 假设 external_tool 节点后需要做决策\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"external_tool\",\n",
    "    decide_next_step,\n",
    "    {\n",
    "        \"handle_error_node\": \"error_handler\", # 映射到名为 error_handler 的节点\n",
    "        \"summarize_output_node\": \"output_summarizer\", # 映射到 output_summarizer 节点\n",
    "        END: END # 直接映射到结束\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置结束点 (Finish Points)\n",
    "\n",
    "图的执行路径可以通过转换到特殊的 END 节点来终止 。当条件边的 path 函数返回的值（或通过 path_map 映射后）是 END 时，该条执行路径便会结束。   \n",
    "\n",
    "条件边是 LangGraph 实现智能体决策能力的核心机制。path 函数通过检查当前状态（可能包含 LLM 的最新消息、工具的输出等）来决定图的下一步走向。这使得图能够动态地改变其执行路径，形成循环（例如，LLM → 工具 → LLM 的 ReAct 循环）或分支到不同的子流程，从而实现复杂的代理行为。将这些条件显式定义为 Python 函数，也使得代理的决策过程比隐藏在单一代理抽象内部更加透明和易于调试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：图的执行与管理\n",
    "\n",
    "定义好图的结构（状态、节点、边）之后，下一步是编译图并执行它。LangGraph 提供了一套标准的接口来管理图的生命周期。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 图的编译 (compile() 方法)\n",
    "\n",
    "compile() 方法是将声明式定义的 StateGraph（或更通用的 Graph）转换为一个可执行的 CompiledStateGraph（或 CompiledGraph）对象的关键步骤 。这个编译后的图对象实现了 LangChain Core 中的 Runnable 接口，这意味着它可以被调用（invoke）、流式处理（stream）、批量处理（batch）以及异步执行 。   \n",
    "\n",
    "编译过程涉及：\n",
    "\n",
    "- 结构最终化：固定图的节点和边配置。\n",
    "- 结构检查：执行基本的验证，例如检查是否存在孤立节点（即没有入边或出边的节点，除了合法的入口和出口）。   \n",
    "- 运行时参数配置：允许在编译时指定重要的运行时组件，例如：\n",
    "    - `checkpointer`：用于实现持久化和状态恢复的对象 。   \n",
    "    - `interrupt_before / interrupt_after`：用于定义断点，支持人机协同工作流 。   \n",
    "    - `debug`：一个布尔标志，用于启用调试模式 。   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设 graph_builder 是一个已经定义好节点和边的 StateGraph 实例\n",
    "# from langgraph.checkpoint.memory import MemorySaver # 示例：使用内存检查点\n",
    "# memory = MemorySaver()\n",
    "# compiled_graph = graph_builder.compile(checkpointer=memory, debug=True)\n",
    "compiled_graph = graph_builder.compile() # 最简单的编译"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编译步骤是声明式图定义与高效执行之间的桥梁。它使得 LangGraph 能够对图结构进行潜在的优化，验证其有效性，并在任何执行发生之前设置必要的运行时组件。这种分离抽象了底层 Pregel 计算模型的复杂性，使用户能够专注于定义图的业务逻辑，同时为运行时注入如持久化或调试设置等图范围的配置提供了一个清晰的切入点。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 执行已编译的图\n",
    "\n",
    "编译后的图对象（如 CompiledStateGraph）作为 Runnable，提供了多种执行方法以适应不同的应用需求 。 \n",
    "#### invoke(input, config=None, **kwargs)\n",
    "\n",
    "- 用于同步执行图，接收单个输入，并返回图执行完毕后的最终状态或指定的输出。这是最基础的请求-响应式调用方式 。   \n",
    "- input：图的初始状态或输入数据。\n",
    "- config：一个 RunnableConfig 对象，用于传递运行时参数，如 thread_id（用于持久化会话）、tags（用于 LangSmith 追踪）等 。   \n",
    "- 输出的具体内容和格式可能受到 stream_mode（尽管主要用于流式方法）和 output_keys 参数的影响 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stream(input, config=None, stream_mode=\"values\", **kwargs)\n",
    "\n",
    "- 用于以流式方式执行图，并逐步产生输出。这对于需要实时反馈给用户的应用（如聊天机器人逐步显示回复）非常有用 。   \n",
    "- 它会产生一个迭代器，该迭代器在图的每个步骤完成后（或根据 stream_mode 的设置）产生中间状态、更新或自定义事件。\n",
    "- stream_mode 参数控制流中产生的内容类型 ：   \n",
    "    - \"values\"：在每个步骤后（包括中断）发出状态中的所有值。\n",
    "    - \"updates\"：仅发出每个步骤后节点或任务返回的更新。\n",
    "    - \"messages\"：逐个令牌地流式传输 LLM 消息及元数据。\n",
    "    - \"custom\"：发出节点内部使用 StreamWriter 写入的自定义数据。\n",
    "    - \"debug\"：为每个步骤发出包含最详尽信息的调试事件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：以 \"updates\" 模式流式处理\n",
    "# for event in compiled_graph.stream({\"input_query\": \"Tell me a joke\"}, stream_mode=\"updates\"):\n",
    "#     print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch(inputs: list, config=None, **kwargs)\n",
    "\n",
    "- 用于并行处理多个独立的输入。它接收一个输入列表，并返回一个对应于每个输入的输出列表 。   \n",
    "- 这可以显著提高处理大量独立请求时的性能。\n",
    "- RunnableConfig 中的 maxConcurrency 属性可以用来控制并行调用的最大数量 。   \n",
    "- 虽然 CompiledStateGraph 实现了 Runnable 接口，但直接针对其 batch 方法的详细 Python 文档在提供的材料中相对较少。然而，Runnable 接口的通用文档确认了 batch 是标准方法之一 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 异步对应方法\n",
    "LangGraph 也为上述方法提供了异步版本，如 ainvoke()、astream()、abatch()，以及更高级的 astream_events()，它们允许在异步 Python 代码（使用 async/await）中非阻塞地执行图 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangGraph 对 Runnable 接口的采用，使其能够开箱即用地提供一套多功能的执行方法。开发者可以根据具体用例选择最合适的方法，无论是简单的同步调用、实时流式界面，还是用于提高效率的批处理，而无需自行实现这些复杂的执行模式。RunnableConfig 则进一步标准化了运行时参数（如用于持久化的 thread_id 或用于追踪的 tags）在这些不同执行风格中的传递方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现循环与迭代\n",
    "\n",
    "在 LangGraph 中，循环或迭代逻辑是构建复杂代理（如 ReAct 风格代理）和需要反复试验或细化过程的应用的关键。这通常通过条件边实现，这些边将执行流导回图中的先前节点，形成一个循环 。   \n",
    "\n",
    "#### 实现机制\n",
    "\n",
    "1. 条件边：循环的核心是至少一个条件边，其 path 函数根据当前状态决定是继续循环（路由到循环中的上一个或下一个节点）还是退出循环（路由到 END 或循环外的其他节点）。   \n",
    "2. 状态更新：在循环的每次迭代中，节点会更新图状态。这个更新后的状态会被传递到下一次迭代或用于条件判断。\n",
    "3. 终止条件：必须在 path 函数中定义明确的终止条件，以防止无限循环。当满足此条件时，path 函数应返回 END 或指向循环外部的节点名称 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Max iterations reached. Exiting loop.\n",
      "Final state: {'counter': 5, 'history': ['Processed iteration 1', 'Processed iteration 2', 'Processed iteration 3', 'Processed iteration 4', 'Processed iteration 5']}\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "from typing import Annotated, Literal, List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class LoopState(TypedDict):\n",
    "    counter: int\n",
    "    history: Annotated[List[str], operator.add] # 使用 operator.add 来追加历史记录\n",
    "\n",
    "MAX_ITERATIONS = 5\n",
    "\n",
    "def increment_counter_node(state: LoopState):\n",
    "    print(f\"Iteration {state['counter'] + 1}\")\n",
    "    return {\"counter\": state[\"counter\"] + 1, \"history\": [f\"Processed iteration {state['counter'] + 1}\"]}\n",
    "\n",
    "def check_condition_to_continue(state: LoopState) -> Literal:\n",
    "    if state[\"counter\"] < MAX_ITERATIONS:\n",
    "        return \"increment_counter_node\" # 继续循环\n",
    "    else:\n",
    "        print(\"Max iterations reached. Exiting loop.\")\n",
    "        return END # 退出循环\n",
    "\n",
    "loop_builder = StateGraph(LoopState)\n",
    "loop_builder.add_node(\"increment_counter_node\", increment_counter_node)\n",
    "\n",
    "loop_builder.set_entry_point(\"increment_counter_node\") # 循环从这里开始\n",
    "loop_builder.add_conditional_edges(\n",
    "    \"increment_counter_node\", # 每次计数器增加后，检查条件\n",
    "    check_condition_to_continue,\n",
    "    {\n",
    "        \"increment_counter_node\": \"increment_counter_node\", # 如果继续，回到自身（或循环中的下一个节点）\n",
    "        END: END # 如果结束，则到 END\n",
    "    }\n",
    ")\n",
    "# 注意：在这个特定例子中，increment_counter_node 既是循环体也是条件判断的起点。\n",
    "# 更复杂的循环可能有多个节点，例如 A -> B -> (condition? A : END)\n",
    "\n",
    "loop_graph = loop_builder.compile()\n",
    "\n",
    "# 初始化状态并执行\n",
    "initial_loop_state = {\"counter\": 0, \"history\":[]}\n",
    "final_loop_state = loop_graph.invoke(initial_loop_state)\n",
    "print(\"Final state:\", final_loop_state)\n",
    "# 预期输出将显示5次迭代的打印信息和最终状态"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 递归限制 (Recursion Limit)\n",
    "\n",
    "为了防止因逻辑错误或意外情况导致的无限循环，LangGraph 对图的执行设有递归限制，即图可以执行的最大“超步数”（super-steps）。   \n",
    "\n",
    "- 默认限制通常是 25 。   \n",
    "- 可以在调用图时通过 config 参数进行配置：`compiled_graph.invoke(initial_state, {\"recursion_limit\": 10}) `。   \n",
    "- 如果执行超步数超过此限制，将引发 `GraphRecursionError` 。   \n",
    "- 可以通过在状态中添加 `RemainingSteps` 类型的注解字段，并在条件逻辑中检查剩余步数，来实现基于递归限制的优雅终止，而不是硬性报错 。   \n",
    "\n",
    "许多高级 AI 任务，如 ReAct 代理（Reason-Act 循环）或对计划、内容的迭代式优化，本质上都是循环的 。代理可能需要思考、调用工具、观察结果，然后再次思考——这是一个典型的循环过程。LangGraph 通过条件边对循环的明确支持，使得这些模式得以直接实现。在这些循环中，状态得以持久化并不断演进，允许代理逐步构建上下文或在多次迭代中改进其方法。递归限制则充当了一个重要的安全网，尤其是在 LLM 驱动的决策可能具有不可预测性的情况下，防止了失控循环的发生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：LangGraph 高级特性\n",
    "LangGraph 不仅仅是构建简单的顺序流程，它提供了一系列高级特性，以支持更复杂、更健壮和更具交互性的 AI 应用。\n",
    "\n",
    "### 持久化与状态管理 (Checkpointers)\n",
    "\n",
    "持久化是 LangGraph 实现有状态应用的核心机制，它允许图的状态在执行步骤之间甚至多次调用之间得以保存和恢复。这对于实现记忆、人机协同、时间旅行调试和容错至关重要 。   \n",
    "\n",
    "机制：持久化通过 Checkpointer 对象实现。在编译图时（通过 compile() 方法的 checkpointer 参数）提供一个 Checkpointer 实例，该检查点记录器会在每个“超步”（super-step，即图中一个或多个并行节点执行完成的单位）结束时，自动保存图状态的一个快照 (StateSnapshot)。这些快照被组织在一个“线程” (thread) 中，通过唯一的 thread_id 进行标识和访问 。\n",
    "\n",
    "\n",
    "#### 配置与使用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **指定 thread_id**\n",
    "\n",
    "当调用一个配置了检查点记录器的已编译图时，必须在 config 参数的 configurable 字段中提供一个 thread_id。例如：{\"configurable\": {\"thread_id\": \"conversation_123\"}} 。这个 thread_id 用于将当前的执行与特定的持久化状态序列关联起来。   \n",
    "\n",
    "##### **选择 Checkpointer 实现**\n",
    "选择 Checkpointer 实现：LangGraph 提供了多种检查点记录器实现，以适应不同的需求场景 ：   \n",
    "\n",
    "- MemorySaver: (来自 langgraph.checkpoint.memory 或 langgraph.checkpoint) 将检查点保存在内存中。它非常适合教程、快速原型验证和测试，因为不需要外部依赖。但缺点是程序退出后状态会丢失 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory_checkpointer = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory_checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **SqliteSaver / AsyncSqliteSaver**\n",
    "\n",
    "(来自 langgraph.checkpoint.sqlite 或 langgraph.checkpoint.aiosqlite) 使用 SQLite 数据库文件存储检查点。适用于本地开发、小型项目或需要简单文件持久化的场景。需要额外安装 langgraph-checkpoint-sqlite 包 。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "db_conn = sqlite3.connect(\"my_checkpoints.sqlite\")\n",
    "sqlite_checkpointer = SqliteSaver(conn=db_conn)\n",
    "graph = builder.compile(checkpointer=sqlite_checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 对于异步操作，应使用 AsyncSqliteSaver。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **PostgresSaver / AsyncPostgresSaver**\n",
    "(来自 langgraph.checkpoint.postgres) 使用 PostgreSQL 数据库存储检查点。这是生产环境中推荐的选项，因为它提供了更强的健壮性和扩展性。需要额外安装 `langgraph-checkpoint-postgres` 和 `psycopg[binary,pool]` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph-checkpoint-postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    "DB_CONNECTION_STRING = \"postgresql://user:password@host:port/database\"\n",
    "# PostgresSaver.from_conn_string() 通常在 with 语句中使用以确保连接关闭\n",
    "# with PostgresSaver.from_conn_string(DB_CONNECTION_STRING) as pg_checkpointer:\n",
    "#     graph = builder.compile(checkpointer=pg_checkpointer)\n",
    "#     # 首次使用可能需要 pg_checkpointer.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **RedisSaver**\n",
    "RedisSaver: (来自 langgraph.checkpoint.redis) 使用 Redis 存储检查点，适用于需要快速读写和分布式缓存的场景 。   \n",
    "\n",
    "\n",
    "##### **不同检查点记录器**\n",
    "\n",
    "\n",
    "\n",
    "| Checkpointer Class   | Package                          | Persistence Medium | Scalability                | Async Support                   | Primary Use Case                  |\n",
    "| -------------------- | -------------------------------- | ------------------ | -------------------------- | ------------------------------- | --------------------------------- |\n",
    "| `MemorySaver`        | `langgraph.checkpoint.memory`    | In-memory          | Low (single process)       | Sync (usable in async contexts) | Testing, Debugging, Tutorials     |\n",
    "| `SqliteSaver`        | `langgraph.checkpoint.sqlite`    | SQLite DB file     | Moderate (local workflows) | Sync                            | Local dev, Small projects         |\n",
    "| `AsyncSqliteSaver`   | `langgraph.checkpoint.aiosqlite` | SQLite DB file     | Moderate (local workflows) | Async                           | Local dev (async), Small projects |\n",
    "| `PostgresSaver`      | `langgraph.checkpoint.postgres`  | PostgreSQL DB      | High (production)          | Sync                            | Production                        |\n",
    "| `AsyncPostgresSaver` | `langgraph.checkpoint.postgres`  | PostgreSQL DB      | High (production)          | Async                           | Production (async)                |\n",
    "| `RedisSaver`         | `langgraph.checkpoint.redis`     | Redis DB           | High (production)          | Sync/Async (client dependent)   | Production (caching/state)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 状态操作\n",
    "配置了检查点记录器后，可以对持久化的状态进行操作 ：   \n",
    "\n",
    "- `graph.get_state(config)`：获取指定 thread_id 的最新状态快照。\n",
    "- `graph.get_state_history(config)`：获取指定 thread_id 的所有历史状态快照。\n",
    "- `graph.update_state(config, values, as_node=None)`：修改特定线程的状态。可以用于人工干预后更新状态，或从特定检查点创建分支。\n",
    "- 回溯 (Replay)：通过在 config 中同时提供 thread_id 和 checkpoint_id 来调用图，可以从该检查点之前的历史重放执行，并在该检查点之后“分叉”出一个新的执行路径。\n",
    "\n",
    "#### 内存存储 (Store 接口)\n",
    "\n",
    "除了线程内的短期记忆（通过检查点），LangGraph 还提供了 Store 接口（例如 InMemoryStore）用于实现跨线程的长期记忆，例如存储用户特定的全局偏好 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查点机制不仅仅是数据存储功能，它是 LangGraph 实现高级有状态操作的基石。通过在每个关键执行步骤保存状态，LangGraph 能够支持对话记忆（如在同一 thread_id 下继续对话 ），实现人机协同（允许图在等待人工输入时暂停，然后从完全相同的状态恢复 ），进行时间旅行式调试（回溯到先前的状态以分析问题或探索不同路径 ），以及提供容错能力（从最后成功的状态重启以应对节点故障 ）。检查点后端的选择（内存、SQLite、Postgres 等）则取决于应用的具体规模和可靠性需求。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 集成外部工具\n",
    "\n",
    "为了让 LangGraph 应用能够与外部世界交互或执行专门的计算任务，工具集成是必不可少的。LangGraph 继承并扩展了 LangChain 在工具使用方面的能力。\n",
    "\n",
    "#### 定义工具\n",
    "\n",
    "- 工具可以是普通的 Python 函数 。   \n",
    "- 更推荐的方式是使用 `langchain_core.tools` 中的 @tool 装饰器。这个装饰器允许为工具添加名称、描述以及参数的 schema（通常通过类型提示或 Pydantic模型定义）。这些元数据对于 LLM 理解何时以及如何使用工具至关重要 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain langchain-openai langchain-tavily tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.tools import tool\n",
    "# from langchain_tavily import TavilySearch\n",
    "\n",
    "# @tool\n",
    "# def magic_multiply(a: int, b: int) -> int:\n",
    "#     \"\"\"Multiplies two integers together. Use this for multiplication tasks.\"\"\"\n",
    "#     return a * b\n",
    "\n",
    "# search_tool = TavilySearch(max_results=2, name=\"web_search\") # 可以直接使用 LangChain 的工具类\n",
    "\n",
    "# tools_list = [magic_multiply, search_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 在 LangGraph 中使用工具\n",
    "\n",
    "LangGraph 提供了一些预构建的组件来简化工具的调用和结果处理：\n",
    "\n",
    "##### ToolNode (来自 langgraph.prebuilt)：\n",
    "\n",
    "- 这是一个预构建的图节点，专门用于执行 LLM 在其最近的 AIMessage 的 tool_calls 属性中请求的工具 。   \n",
    "- 当 ToolNode 执行时，它会查找 AIMessage 中的工具调用请求，并执行相应的工具。\n",
    "- 如果 LLM 请求并行执行多个工具调用，ToolNode 支持并行执行这些调用 。   \n",
    "- 执行后，ToolNode 会返回一个包含 ToolMessage 对象的列表，每个 ToolMessage 对应一个工具调用的结果。这些 ToolMessage 通常会被添加回图状态的 messages 列表中，供 LLM 在下一步中处理 。   \n",
    "- 通常在 StateGraph 中使用，并期望状态中有一个名为 \"messages\" (或由 messages_key 参数指定的其他名称) 的键，其中包含消息列表 。   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "# tool_node_instance = ToolNode(tools_list)\n",
    "# graph_builder.add_node(\"execute_tools\", tool_node_instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ToolExecutor (来自 langgraph.prebuilt)\n",
    "\n",
    "- 这是一个 Runnable，它接收一个 ToolInvocation 对象（或列表）并执行指定的工具 。   \n",
    "- ToolInvocation 是一个简单的数据类，包含 tool (工具名称) 和 tool_input (工具输入参数) 属性 。   \n",
    "- ToolExecutor 可以在自定义节点内部用于更细致地控制工具的执行逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tools_condition (来自 langgraph.prebuilt)：\n",
    "\n",
    "- 这是一个实用函数，常用于图中的条件边。它的作用是检查状态中的最新消息是否包含工具调用请求 。   \n",
    "- 如果存在工具调用，它通常会路由到执行工具的节点（如 ToolNode，通常命名为 \"action\" 或 \"tools\"）。\n",
    "- 如果没有工具调用，它通常会路由到 END（表示代理已完成思考并准备回复用户）或图中的其他逻辑分支。\n",
    "- 这对于构建 ReAct (Reason-Act) 风格的代理循环非常有用：LLM 思考 -> (可选)调用工具 -> LLM 处理工具结果并再次思考。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态工具调用流程\n",
    "\n",
    "- LLM（配置了可用工具）在其响应中生成一个或多个 tool_calls。\n",
    "- 这个包含 tool_calls 的 AIMessage 被添加到图状态中。\n",
    "- 图的条件逻辑（通常使用 tools_condition）将执行流导向 ToolNode。\n",
    "- ToolNode 解析 tool_calls，并行执行请求的工具。\n",
    "- 工具的输出被格式化为 ToolMessage 对象，并更新到图状态中。\n",
    "- 执行流通常返回到 LLM 节点，LLM 现在可以根据工具的输出继续其推理过程。    \n",
    "\n",
    "ToolNode 作为 LLM 的工具使用意图与实际工具执行之间的标准化桥梁，极大地简化了工具型代理的创建。它负责处理工具调用的解析、分发和结果收集的机制，使开发者能更专注于定义工具本身和 LLM 的核心推理逻辑。其并行执行能力也为性能带来益处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MCP工具\n",
    "\n",
    "https://github.com/langchain-ai/langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_mcp_adapters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_mcp_adapters\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclient\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiServerMCPClient\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StateGraph, MessagesState, START\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprebuilt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ToolNode, tools_condition\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_mcp_adapters'"
     ]
    }
   ],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.graph import StateGraph, MessagesState, START\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "model = init_chat_model(\"openai:gpt-4.1\")\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"math\": {\n",
    "            \"command\": \"python\",\n",
    "            # Make sure to update to the full absolute path to your math_server.py file\n",
    "            \"args\": [\"./examples/math_server.py\"],\n",
    "            \"transport\": \"stdio\",\n",
    "        },\n",
    "        \"weather\": {\n",
    "            # make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/mcp\",\n",
    "            \"transport\": \"streamable_http\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.bind_tools(tools).invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(call_model)\n",
    "builder.add_node(ToolNode(tools))\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_conditional_edges(\n",
    "    \"call_model\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"call_model\")\n",
    "graph = builder.compile()\n",
    "math_response = await graph.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "weather_response = await graph.ainvoke({\"messages\": \"what is the weather in nyc?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 人机协同 (HIL) 工作流\n",
    "\n",
    "人机协同（Human-in-the-Loop, HIL）是 LangGraph 的一项关键特性，它允许在自动化流程的任何节点引入人工干预，用于验证、修正、提供额外上下文或做出关键决策 。这对于处理 LLM 输出可能需要审查、模型能力不足或涉及敏感操作的场景尤为重要。   \n",
    "\n",
    "#### 核心能力\n",
    "\n",
    "- **持久化执行状态**：依赖于检查点机制，LangGraph 可以在定义的节点处无限期暂停执行，等待人工审查或输入，而不会丢失上下文。这支持异步的人工处理流程 。   \n",
    " **灵活的集成点**：HIL 逻辑可以被设计在工作流的任何位置，实现有针对性的人工参与，例如批准 API 调用、修正生成内容或引导对话方向 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 实现机制\n",
    "\n",
    "LangGraph 提供了多种方式来实现 HIL：\n",
    "\n",
    "##### **静态断点 (interrupt_before, interrupt_after)**：\n",
    "\n",
    "允许在特定节点执行之前或之后暂停图的执行。这些断点可以在图编译时（通过 compile() 的参数）或运行时（通过 invoke/stream 的参数）设置 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译时设置断点\n",
    "# graph = graph_builder.compile(\n",
    "#     interrupt_before=[\"sensitive_tool_node\"],\n",
    "#     interrupt_after=[\"llm_decision_node\"],\n",
    "#     checkpointer=checkpointer\n",
    "# )\n",
    "\n",
    "# 运行时设置断点\n",
    "# config_with_interrupt = {\"configurable\": {\"thread_id\": \"hil_thread_1\"}}\n",
    "# graph.invoke(\n",
    "#     initial_state,\n",
    "#     interrupt_before=[\"sensitive_tool_node\"],\n",
    "#     config=config_with_interrupt\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当图执行到断点时，它会暂停。之后可以通过再次调用 invoke 或 stream 方法（通常传递 None 作为输入，或使用 Command 对象）并使用相同的 thread_id 来恢复执行 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **interrupt() 函数 (来自 langgraph.types)**\n",
    "\n",
    "- 这是一种更动态的方式，允许在节点内部的逻辑中触发暂停。interrupt() 函数会暂停图的执行，并可以将一个 JSON 可序列化的载荷（payload）呈现给人工审查者 。   \n",
    "- 使用 interrupt() 同样需要配置检查点记录器，并在调用图时提供 thread_id 。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import interrupt, Command\n",
    "# class HIL_State(TypedDict):\n",
    "#     some_text: str\n",
    "#     human_feedback: str\n",
    "\n",
    "# def human_review_node(state: HIL_State):\n",
    "#     # 呈现 state[\"some_text\"] 给人工审查\n",
    "#     feedback_payload = interrupt({\"text_to_revise\": state[\"some_text\"]})\n",
    "#     # feedback_payload 将是人工提供的值，在恢复时由 Command(resume=...) 传入\n",
    "#     return {\"human_feedback\": feedback_payload, \"some_text\": feedback_payload} # 示例：用反馈更新原文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Command 原语 (来自 langgraph.types)**\n",
    "当图因 interrupt() 或静态断点暂停后，使用 Command 对象来恢复执行。Command(resume=human_input_value) 会将 human_input_value 作为 interrupt() 函数的返回值（或作为后续节点的输入，取决于具体实现）注入回图中，从而继续执行流程 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设图在 human_review_node 因 interrupt() 而暂停\n",
    "# config = {\"configurable\": {\"thread_id\": \"hil_thread_1\"}}\n",
    "# edited_text_from_human = \"This is the human-edited text.\"\n",
    "# graph.invoke(Command(resume=edited_text_from_human), config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 常见用例\n",
    "- **审查工具调用**：在执行敏感工具（如数据库写入、API 交易）之前，让人工审查、编辑或批准工具的参数和调用本身 。   \n",
    "- **验证 LLM 输出**：人工审查、编辑或批准 LLM 生成的内容，如报告草稿、邮件回复等 。   \n",
    "- **提供上下文或澄清**：当代理遇到歧义或信息不足时，可以主动请求人工输入以获取更多细节或指导 。\n",
    "\n",
    "**状态修改**：在 HIL 暂停期间，人工不仅可以提供输入，还可以通过 graph.update_state() 方法直接修改图的当前状态。图将在恢复时使用这个更新后的状态继续执行 。   \n",
    "\n",
    "HIL 是构建可信赖、可靠的生产级 AI 代理系统的关键。它将代理从纯粹的自主系统转变为协作工具，人类的专业知识可以在其中指导、纠正和验证关键行为。LangGraph 提供的这种能力，结合其持久化机制，使得长时间的、异步的人工参与成为可能，这对于企业环境中对准确性、安全性和问责制有严格要求的应用至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建多智能体系统\n",
    "\n",
    "当单个 AI 代理难以覆盖多个专业领域或有效管理大量工具时，将复杂任务分解给多个协同工作的、专门化的智能体就成为一种有效的策略 。LangGraph 为构建和编排此类多智能体系统提供了强大的支持。   \n",
    "\n",
    "#### 核心概念\n",
    "\n",
    "**智能体通信与切换 (Agent Communication/Handoffs)**：多智能体系统中的关键在于智能体之间如何传递控制权和上下文信息 。   \n",
    "\n",
    "- Command 对象：切换可以通过让智能体节点或其内部工具返回 Command 对象来实现。Command 对象可以指定 destination（目标智能体/节点的名称）和 payload（要传递给目标的状态更新）。   \n",
    "- Send() 原语：可用于在切换过程中将数据直接发送给工作智能体（目标节点）。   \n",
    "- Command.PARENT：允许子图中的节点或工具影响父图的流程，从而实现向父图中其他智能体节点的切换 。   \n",
    "\n",
    "**协调 (Coordination)**：管理智能体之间的任务分配、执行顺序和整体工作流 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 预构建架构与组件\n",
    "\n",
    "LangGraph 社区和官方提供了一些预构建的模式和库来简化多智能体系统的开发，这些通常与 LangGraph 核心库紧密集成或作为其扩展\n",
    "\n",
    "\n",
    "##### 监督者模式 (Supervisor)\n",
    "\n",
    "- 通常由一个中心的“监督者”智能体来协调多个专门的“工作者”智能体。监督者负责接收任务、分解任务、将子任务分配给合适的工作者智能体，并汇总结果 。   \n",
    "- 监督者控制所有通信流和任务委派，根据当前上下文和任务需求决定调用哪个智能体 。   \n",
    "- 可以使用如 langgraph-supervisor 这样的库（或 LangGraph 未来可能内置的 langgraph.prebuilt.Supervisor）来实现。\n",
    "##### 集群模式 (Swarm)\n",
    "\n",
    "- 这是一种更去中心化的方法，其中专门化的智能体根据其特长动态地相互切换控制权。系统通常会记住最后活跃的智能体，以便在后续交互中无缝恢复对话 。   \n",
    "- 智能体之间通常通过专门的“切换工具”（handoff tools）直接通信和传递任务 。   \n",
    "- 可以使用如 langgraph-swarm 这样的库（或 LangGraph 未来可能内置的 langgraph.prebuilt.Swarm）来实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建步骤\n",
    "\n",
    "- **定义单个智能体**：每个专门的智能体本身可以是一个 LangGraph 图（例如，一个 StateGraph 实例）或使用 create_react_agent 等预构建工具创建。\n",
    "- **实现切换机制**：\n",
    "    - 对于 Swarm 模式，可以创建切换工具（例如，使用 langgraph-swarm 中的 create_handoff_tool ）。   \n",
    "    - 对于 Supervisor 模式，监督者智能体的逻辑将决定如何路由到工作者智能体。\n",
    "    - 通常，切换涉及从一个智能体（或其工具）返回一个 Command，指示下一个要激活的智能体节点。\n",
    "- **组合智能体**：将这些单个智能体作为节点组合到一个更高层次的 LangGraph 图中（例如，监督者图，其节点是各个工作者智能体子图）。\n",
    "- **状态管理**：仔细设计主协调图和各个子智能体图之间的状态共享或转换逻辑。可能需要适配器函数来映射状态 。\n",
    "\n",
    "#### 多轮对话\n",
    "\n",
    "在多智能体系统中实现多轮对话，通常涉及一个协调节点（如监督者或一个专门的对话管理器），该节点使用 interrupt 来收集用户输入，然后将该输入路由回当前活跃的或最合适的智能体 。   \n",
    "\n",
    "LangGraph 通过其灵活的图结构和状态管理能力，使得开发者能够构建出模块化的、可扩展的复杂 AI 系统，这些系统能够模仿人类团队协作的方式，由不同领域的专家共同解决问题。选择 Supervisor 还是 Swarm 模式，取决于应用是需要更集中的控制，还是更动态、自组织的协作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调试与可视化\n",
    "\n",
    "鉴于 LangGraph 应用可能涉及复杂的逻辑、状态转换和多个行动者，有效的调试和可视化工具对于开发和维护至关重要。\n",
    "\n",
    "#### LangSmith 集成\n",
    "\n",
    "- LangSmith 是 LangChain 生态中用于可观察性、调试、测试和监控 LLM 应用的平台，与 LangGraph 紧密集成 。   \n",
    "- 通过设置环境变量（如 `LANGCHAIN_TRACING_V2=\"true\"` 和 `LANGCHAIN_API_KEY`），LangGraph 执行的每个步骤、节点输入输出、状态变化以及 LLM 调用都会被自动追踪并发送到 LangSmith 。   \n",
    "- 这为开发者提供了对代理行为的深入洞察，有助于诊断错误、分析性能瓶颈和评估代理轨迹 。   \n",
    "\n",
    "#### 图可视化\n",
    "\n",
    "- LangGraph 提供了内置的功能来可视化已定义的图结构，帮助理解其流程和连接关系 。   \n",
    "- graph.get_graph().draw_mermaid_png()：此方法将图渲染为 PNG 图片。它首先将图转换为 Mermaid 标记语言的语法，然后使用 Mermaid.Ink API（默认）、本地安装的 Pyppeteer 或 Graphviz 将其渲染成图像 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "# 假设 'app' 是一个已编译的 LangGraph 图\n",
    "# try:\n",
    "#     display(Image(app.get_graph().draw_mermaid_png()))\n",
    "# except Exception as e:\n",
    "#     print(f\"Error generating graph image: {e}\")\n",
    "#     # Mermaid 渲染可能需要额外的依赖或网络访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`graph.get_graph().draw_mermaid()`：此方法直接返回图的 Mermaid 语法字符串，可以用于其他支持 Mermaid 的工具或文档中 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 编译时的 debug=True 标志\n",
    "- `StateGraph.compile()` 方法接受一个 debug布尔参数 。   \n",
    "- 虽然具体行为细节在提供的材料中没有详尽阐述，但通常这类标志会启用更详细的日志记录或在 LangSmith 中产生更丰富的追踪信息。例如，LangChain 的全局 `set_debug(True)` 会导致非常详细的原始输入输出日志 。LangGraph 的 `stream_mode=\"debug\"` 也会提供最大量的执行信息 。`compile(debug=True)` 可能与这些机制协同工作，或启用 LangGraph 特有的内部调试输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 流式处理模式用于调试：\n",
    "\n",
    "在调用 stream() 方法时，使用 `stream_mode=\"debug\"` 或 `stream_mode=\"updates\"` 可以实时观察图的执行步骤和状态变化，有助于理解执行流程和定位问题 。 \n",
    "\n",
    "#### LangGraph Studio\n",
    "\n",
    "这是一个专为 LangGraph 开发设计的集成开发环境（IDE），提供了图形化的界面来构建、可视化、实时交互和调试 LangGraph 应用 。它旨在将多种调试和开发功能整合到一个统一的平台。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有效的 LangGraph 调试通常是多种工具和技术的结合。LangSmith 提供了深入的、基于追踪的分析；图可视化帮助理解静态结构和潜在流程；特定的流式模式和调试标志则在运行时提供即时反馈。LangGraph Studio 则试图将这些能力集成，提供更流畅的开发体验。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六部分：LangGraph 实战项目\n",
    "本部分将概述几个可以使用 LangGraph 构建的实际项目，展示其在不同场景下的应用潜力。这些项目将利用前面讨论的核心概念和高级特性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目一：开发多轮对话智能体\n",
    "\n",
    "目标：创建一个能够进行多轮对话、根据需要使用外部工具（如网络搜索获取实时信息），并能记住对话历史的聊天机器人。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心组件\n",
    "##### **状态 (State)**\n",
    "\n",
    "使用 `langgraph.graph.MessagesState` 或自定义的 TypedDict，其中必须包含一个用于存储对话消息的键，例如 `messages: Annotated[list, add_messages]`。\n",
    "可以包含其他字段来存储工具调用的中间结果、用户意图或其他上下文信息。\n",
    "\n",
    "##### **节点 (Nodes)**\n",
    "- `llm_node (或 agent_node)`：负责调用大型语言模型（LLM）。它接收当前对话历史（来自状态），并让 LLM 生成回复或决定是否需要调用工具。\n",
    "- `tool_node`：使用 `langgraph.prebuilt.ToolNode` 来执行 LLM 请求的任何工具。\n",
    "- （可选）`human_input_node`：如果需要人工澄清用户意图或提供额外信息，可以使用 `interrupt()` 实现。\n",
    "\n",
    "##### **边 (Edges)**\n",
    "\n",
    "- 图的入口点 (START) 连接到 `llm_node`。\n",
    "- 从 `llm_node` 出发设置条件边 (e.g., 使用 `langgraph.prebuilt.tools_condition`)：\n",
    "    - 如果 LLM 请求工具调用：路由到 `tool_node`。\n",
    "    - 如果 LLM 直接生成回复（无需工具）：路由到 END (或一个专门处理最终回复的节点)。\n",
    "      （可选）如果需要人工输入：路由到 `human_input_node`。\n",
    "- `tool_node` 的输出（工具执行结果）通常会路由回 llm_node，以便 LLM 处理这些新信息并继续对话。\n",
    "- （可选）`human_input_node` 的输出也会路由回 `llm_node`。\n",
    "\n",
    "##### **工具 (Tools)**\n",
    "根据需求定义一个或多个工具。例如，使用 langchain_tavily.TavilySearchResults 实现网络搜索功能。    \n",
    "\n",
    "##### **记忆 (Memory)**\n",
    "\n",
    "在编译图时，配置一个检查点记录器 (checkpointer)，如 MemorySaver (用于快速测试) 或 SqliteSaver (用于本地持久化)。\n",
    "\n",
    "在每次与聊天机器人交互时，通过 config 参数传递一个唯一的 thread_id，以确保对话历史被正确保存和加载。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键 LangGraph 概念应用\n",
    "\n",
    "`StateGraph`, `add_messages` (或 `MessagesState`), `ToolNode`, `tools_condition`, 检查点机制 (`MemorySaver`, `SqliteSaver`), `compile()`, `stream()/invoke()`, 条件边。\n",
    "\n",
    "项目价值：此项目旨在演示 `LangGraph` 如何管理对话状态（记忆），在对话流程中集成和使用外部工具，并通过检查点机制实现跨多轮交互的上下文保持。这是构建实用聊天机器人的基础。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目二：构建研究助手 (RAG - 检索增强生成)\n",
    "\n",
    "目标：创建一个能够根据提供的文档集合回答用户问题的智能体。如果文档中没有答案或信息不足，它可以选择性地使用网络搜索作为补充或后备。\n",
    "\n",
    "#### 核心组件\n",
    "##### **状态 (State)**\n",
    "至少包含以下字段：`question` (用户的问题), `documents` (从向量数据库检索到的文档列表), `web_search_needed` (布尔值，指示是否需要网络搜索), `generation` (最终生成的答案), `sources` (引用的来源)。    \n",
    "\n",
    "##### **节点 (Nodes)**\n",
    "- `retrieve_documents_node`：负责从向量数据库（如 ChromaDB, LanceDB, FAISS）中检索与用户问题相关的文档。\n",
    "- `grade_documents_node`：评估检索到的文档的相关性和充分性。如果文档不相关或信息不足以回答问题，则将状态中的 `web_search_needed` 标志设置为 True。\n",
    "- `web_search_node`：如果 `web_search_needed` 为 True，则执行网络搜索（例如，使用 Tavily API）以获取额外信息。\n",
    "- `generate_answer_node`：基于检索到的相关文档和/或网络搜索结果，使用 LLM 生成最终答案。\n",
    "- （可选）`rewrite_query_node`：在检索之前，如果初始用户问题表述不清或过于宽泛，此节点可以使用 LLM 重写查询以提高检索质量。\n",
    "\n",
    "\n",
    "##### **边 (Edges)**\n",
    "- `START` → `retrieve_documents_node` (或 `rewrite_query_node` → `retrieve_documents_node`)。\n",
    "- `retrieve_documents_node` → `grade_documents_node`。\n",
    "- 从 `grade_documents_node` 出发设置条件边：\n",
    "    - 如果文档充分且相关 (`web_search_needed` 为 False)：路由到 `generate_answer_node`。\n",
    "    - 如果需要网络搜索 (`web_search_needed` 为 True)：路由到 `web_search_node`。\n",
    "- `web_search_node` → `generate_answer_node`。\n",
    "- `generate_answer_node` → `END`。\n",
    "\n",
    "##### **工具/数据 (Tools/Data)**\n",
    "- 向量数据库设置（加载文档、创建嵌入、索引）。\n",
    "- 文档加载器、文本分割器、嵌入模型。\n",
    "- 网络搜索工具（如 Tavily）。\n",
    "\n",
    "##### **记忆 (Memory) (可选)**\n",
    "如果研究助手设计为交互式的，可以像聊天机器人一样使用检查点记录器来保存对话历史和先前研究的上下文。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键 LangGraph 概念应用\n",
    "条件路由（基于文档相关性判断），精细的状态管理（跟踪 RAG 流程的各个阶段和中间产物），工具集成（网络搜索），模块化节点设计。\n",
    "\n",
    "**项目价值**：此项目将展示如何使用 LangGraph 构建一个复杂的信息检索和生成管道，其中包含基于数据评估的决策点（是否需要网络搜索），从而生成更准确、更全面的答案。这是实现高级问答系统和知识库交互的核心模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目三：创建书籍撰写辅助工具 (Plan-and-Execute 模式)\n",
    "\n",
    "目标：开发一个 AI 助手，能够协助用户撰写书籍。该助手首先会与用户一起或自主生成一个写作计划（例如，书籍大纲或章节列表），然后逐步执行计划中的每个步骤（例如，草拟每个章节的内容），并允许用户在关键阶段进行审查和迭代。\n",
    "\n",
    "#### 核心组件\n",
    "##### **状态 (State)**\n",
    "\n",
    "- 使用类似 `PlanExecute` 的 `TypedDict` 。   \n",
    "- 包含字段如：`topic` (书籍主题), `plan` (写作计划，如章节标题列表), `current_step_index` (当前执行到计划的第几步), `drafted_content` (当前草稿或已完成章节的集合), `human_feedback` (用户反馈), `final_manuscript` (最终书稿)。\n",
    "\n",
    "##### **节点 (Nodes)**\n",
    "- `planner_node`：接收书籍主题，并生成一个初步的写作计划（大纲）。这通常涉及 LLM 调用。\n",
    "- `execute_step_node`：根据计划中的当前步骤（例如，一个章节标题），生成该步骤的内容（草拟章节）。此节点也可能使用 LLM，并可能集成研究工具进行资料查找。\n",
    "- `human_review_node`：使用 `interrupt()` 暂停执行，将生成的计划或草拟的章节内容呈现给用户进行审查。用户可以提供反馈、批准或请求修改。\n",
    "- `update_plan_node` (可选)：如果用户反馈需要修改计划，此节点处理反馈并更新状态中的 plan。\n",
    "- `incorporate_feedback_node` (可选)：如果用户对草稿有修改意见，此节点（或 `execute_step_node` 的重入逻辑）将用户反馈融入草稿的修订中。\n",
    "- `compile_book_node`：当所有计划步骤完成且得到批准后，此节点将所有草拟的内容整合成最终的书稿。\n",
    "\n",
    "\n",
    "##### **边 (Edges)**\n",
    "\n",
    "- `START` → `planner_node`。\n",
    "- `planner_node` → `human_review_node` (审查初始计划)。\n",
    "- 从 `human_review_node` (计划审查后) 出发设置条件边：\n",
    "- 若计划批准：路由到 `execute_step_node` (开始执行第一个计划步骤)。\n",
    "- 若计划需修改：路由到 `update_plan_node` (然后可能回到 `planner_node` 或再次到 `human_review_node`)。\n",
    "- `execute_step_node` → `human_review_node` (审查当前步骤的草稿)。\n",
    "- 从 `human_review_node` (草稿审查后) 出发设置条件边：\n",
    "    - 若草稿批准且计划中还有未完成步骤：路由到 `execute_step_node` (执行下一个步骤)。\n",
    "    - 若草稿批准且所有步骤完成：路由到 `compile_book_node`。\n",
    "    - 若草稿需修改：路由到 `execute_step_node` (重新执行当前步骤，状态中可能包含用户反馈) 或 `incorporate_feedback_node`。\n",
    "- `compile_book_node` → `END`。\n",
    "\n",
    "\n",
    "##### **工具 (Tools)**\n",
    "- LLM 用于规划（生成大纲）和写作（草拟章节）。\n",
    "\n",
    "- （可选）网络搜索工具或文档检索引擎，用于在撰写特定章节时进行资料研究。\n",
    "\n",
    "##### **持久化 (Persistence)**\n",
    "对于这种长期、迭代的写作过程，强大的持久化机制至关重要。推荐使用 SqliteSaver 或 PostgresSaver，以便用户可以随时中断和恢复写作过程。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关键 LangGraph 概念应用\n",
    "\n",
    "Plan-and-Execute 代理模式，人机协同 (HIL) 实现迭代式创作和反馈循环，复杂的条件逻辑和状态转换，用于管理计划、草稿和反馈的精细状态，以及强大的持久化支持。\n",
    "\n",
    "项目价值：此项目旨在展示 LangGraph 如何支持复杂的、多阶段的、需要人类深度参与的创作流程。它不仅是一个内容生成工具，更是一个协作平台，AI 负责草拟和研究，人类负责指导、审查和最终决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目四：开发趣味交互式应用 (例如，文本冒险游戏)\n",
    "\n",
    "目标：创建一个简单的文本冒险游戏。游戏中，用户通过输入文本指令（如“向北走”，“查看物品”）与游戏世界互动，游戏状态（如玩家位置、物品栏、故事进展）会根据用户的选择和游戏规则动态演变。游戏还可能与外部 API 交互以增加动态性（例如，如果游戏场景中有天气变化，可以调用天气 API）。\n",
    "\n",
    "#### 核心组件\n",
    "\n",
    "##### **状态 (State)**\n",
    "\n",
    "- `current_location`: 玩家当前在游戏地图中的位置。\n",
    "- `inventory`: 玩家物品栏中的物品列表。\n",
    "- `story_log`: 记录游戏叙事和用户输入的对话历史（可以使用 `add_messages`）。\n",
    "- `current_prompt_for_user`: 当前呈现给用户的场景描述和可选行动。\n",
    "- （可选）其他游戏相关的状态变量，如玩家生命值、任务标记等。\n",
    "\n",
    "##### **节点 (Nodes)**\n",
    "\n",
    "- `game_logic_node`：核心游戏逻辑处理节点。它接收用户的输入（选择的行动），根据当前游戏状态和预设的游戏规则（或由 LLM 动态生成规则/叙事）更新玩家位置、物品栏等状态。然后，它生成新的场景描述、故事进展和下一步可供用户选择的行动，并更新 `current_prompt_for_user` 和 `story_log`。此节点通常会大量使用 LLM 来理解用户输入、生成叙事和决策。\n",
    "- `user_input_node`：使用 `interrupt()` 函数。它将 `current_prompt_for_user` 的内容呈现给用户（例如，场景描述和选项 \"你面前有两条路，一条通往森林，一条通往山洞。你要选择哪条路？\"），并暂停图的执行，等待用户输入下一个行动。\n",
    "- （可选）`external_api_node`：如果游戏需要与外部服务交互（例如，获取当前游戏内“城市”的真实天气），此节点将调用相应的工具（如天气 API 工具）。\n",
    "\n",
    "##### **边 (Edges)**\n",
    "\n",
    "- `START` → `game_logic_node` (用于游戏初始化，呈现第一个场景和选项)。\n",
    "- `game_logic_node` → `user_input_node` (在处理完逻辑并生成新提示后，等待用户输入)。\n",
    "- `user_input_node` → `game_logic_node` (用户输入后，返回游戏逻辑节点处理，形成主游戏循环)。\n",
    "- （可选）从 `game_logic_node` 到 `external_api_node` 的条件边（如果游戏逻辑判断需要调用 API），然后从 `external_api_node` 返回到 `game_logic_node`（处理 API 结果）。\n",
    "- 从 `game_logic_node` 到 `END` 的条件边（例如，当玩家达成游戏胜利条件或失败条件时）。\n",
    "\n",
    "##### **工具 (Tools)** (可选)\n",
    "\n",
    "- 可以集成外部 API 工具，如天气查询、随机事件生成器、知识问答（如果游戏中有 NPC 可以回答特定问题）等。\n",
    "\n",
    "##### **持久化 (Persistence)**\n",
    "\n",
    "- 使用 `MemorySaver` (用于简单测试) 或 `SqliteSaver` (用于允许玩家保存和加载游戏进度)。每个玩家的游戏会话对应一个 `thread_id`。\n",
    "\n",
    "##### **关键 LangGraph 概念应用**\n",
    "\n",
    "- 通过条件边和节点间的循环连接实现核心游戏循环，精细的状态管理（跟踪游戏世界的所有动态元素），使用 `interrupt()` 实现与用户的高度交互，（可选）通过工具集成外部服务以增强游戏体验，以及通过检查点实现游戏存档/读档功能。\n",
    "\n",
    "##### **项目价值**\n",
    "\n",
    "- 此项目旨在展示 LangGraph 不仅能用于传统的代理任务，还能用于创建动态的、有状态的交互式体验。它强调了如何通过图的控制流来响应用户选择，并管理一个不断演变的游戏世界。这为利用 LLM 和 LangGraph 构建更具沉浸感和创造性的应用开辟了道路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  第七部分：探索 LangGraph 生态\n",
    "\n",
    "LangGraph 不仅仅是一个独立的库，它正围绕着一个不断发展的生态系统，旨在支持从开发到部署和管理的整个 AI 应用生命周期。\n",
    "\n",
    "\n",
    "#### LangGraph 平台 (LangGraph Platform)\n",
    "\n",
    "这是一个为部署、扩展和管理 LangGraph 应用而设计的服务。它提供了用于构建代理用户体验的 API、一个集成的开发者工作室 (LangGraph Studio) 以及生产级的运维功能。\n",
    "\n",
    "平台特性包括：托管持久化（如使用 Postgres）、自动扩展、容错机制、定时任务（Cron jobs）、监控集成等。\n",
    "\n",
    "提供多种部署选项：完全托管的云 SaaS 服务、混合部署（控制平面在云端，数据平面自托管）以及完全自托管的企业版。\n",
    "\n",
    "\n",
    "#### LangGraph Studio\n",
    "\n",
    "一个可视化的集成开发环境 (IDE)，专为 LangGraph 设计。它支持实时图表可视化、交互式测试、集成调试工具，旨在简化复杂代理应用的开发和调试流程。\n",
    "\n",
    "\n",
    "#### LangGraph CLI (命令行界面)\n",
    "\n",
    "用于本地开发、项目脚手架搭建、配置管理以及将应用部署到 LangGraph 平台或本地服务器。\n",
    "\n",
    "\n",
    "#### 官方资源与示例\n",
    "\n",
    "-   **官方文档**：包含入门指南、操作方法、概念解释和 API 参考。\n",
    "-   **LangChain Academy 课程**：提供关于 LangGraph 基础知识的免费结构化课程。\n",
    "-   **官方模板**：LangChain 团队维护了一系列预构建的应用模板，覆盖常见的代理模式，如 ReAct 代理、带记忆的代理、RAG 系统、数据扩充流程等。这些模板可以直接克隆和修改，加速开发进程。\n",
    "-   **\"Awesome LangGraph\" 仓库**：一个由社区维护的精选列表，收集了大量优秀的 LangGraph 项目、资源、工具和示例应用，是发现新思路和学习实践的好地方。\n",
    "\n",
    "\n",
    "#### 社区库与预构建代理\n",
    "\n",
    "围绕 LangGraph 已经出现了一些社区贡献的库和更高级别的预构建代理，例如：\n",
    "-   `langgraph-supervisor`：用于构建监督者模式的多智能体系统。\n",
    "-   `langgraph-swarm`：用于构建集群模式的多智能体系统。\n",
    "-   `langgraph-reflection`：实现了带有反思步骤的代理架构。\n",
    "-   以及其他针对特定功能（如代码生成、长期记忆增强）的库。\n",
    "\n",
    "\n",
    "LangGraph 正在从一个核心库演变成一个全面的生态系统。虽然核心库提供了强大的原语来构建有状态的多行动者应用，但周边的工具（如平台、Studio、CLI）和丰富的社区资源则旨在支持代理应用的整个生命周期——从最初的设计和编码，到部署、监控和迭代优化。这种生态系统方法对于使开发者能够大规模构建和运营复杂的 LangGraph 应用至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第八部分：预构建代理 (Prebuilt Agents)\n",
    "\n",
    "LangGraph 提供了一系列预构建的代理（如通过 create_react_agent 创建的 ReAct 代理），这些代理本身已经具备了强大的功能。然而，在复杂的应用场景中，我们往往需要对其进行更深层次的定制和扩展。\n",
    "\n",
    "### `create_react_agent` 详解\n",
    "\n",
    "`create_react_agent` 是快速构建基于 ReAct 模式的工具调用代理的便捷方法。除了基础用法外，它还提供了丰富的参数以支持高级定制。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态提示\n",
    "\n",
    "create_react_agent 的 prompt 参数不仅可以接受静态字符串，还可以是一个 Callable (可调用对象，如函数) 或 Runnable。这使得提示可以根据当前的 AgentState (代理状态) 和 RunnableConfig (运行时配置，例如包含 user_id) 动态生成，从而实现更灵活和个性化的代理行为。    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage, BaseMessage, HumanMessage, SystemMessage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableConfig\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_agent_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentState\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 🔧 定义一个简单的工具函数（用于查询天气）\n",
    "\n",
    "这个工具会被代理调用，当用户询问天气时，模型会触发该函数获取城市天气信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Dict, Any\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AIMessage, BaseMessage, HumanMessage, SystemMessage, ToolMessage, AnyMessage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunnables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RunnableConfig\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_agent_executor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AgentState \u001b[38;5;66;03m# AgentState 通常是一个 TypedDict，包含 messages 列表等\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"获取指定城市的天气信息。\"\"\"\n",
    "    if city == \"北京\":\n",
    "        return \"北京今天晴朗，25摄氏度。\"\n",
    "    elif city == \"上海\":\n",
    "        return \"上海今日有雨，22摄氏度。\"\n",
    "    else:\n",
    "        return f\"抱歉，我没有 {city} 的天气信息。\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  定义动态生成提示的函数（作为 prompt 参数传入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_prompt_function(state: AgentState, config: RunnableConfig) -> List[BaseMessage]:\n",
    "    # 获取用户 ID，作为个性化提示的一部分\n",
    "    user_id = config[\"configurable\"].get(\"user_id\", \"尊敬的用户\")\n",
    "\n",
    "    # 获取当前的对话历史\n",
    "    current_messages = state.get(\"messages\", [])\n",
    "\n",
    "    # 构建动态系统提示内容\n",
    "    system_prompt_content = f\"您好，{user_id}！我是您的智能助手。请根据用户问题和上下文进行回复。\"\n",
    "\n",
    "    # 创建 SystemMessage，并将其与当前消息历史组合\n",
    "    system_msg = SystemMessage(content=system_prompt_content)\n",
    "\n",
    "    return [system_msg] + current_messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是 create_react_agent 的 动态 prompt 函数。\n",
    "\n",
    "每次代理运行时，它都会根据当前状态（对话历史）和运行时配置（如 user_id）来生成个性化的系统提示。\n",
    "\n",
    "返回的是一个 `List[BaseMessage]` 类型的消息列表，第一个是系统消息，后续是用户对话历史。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化模型并创建代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ChatDeepSeek' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 初始化 OpenAI 模型\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m ChatDeepSeek(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeepseek-chat\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 创建代理，指定 tools 和 prompt（使用动态函数）\u001b[39;00m\n\u001b[0;32m      5\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m create_react_agent(\n\u001b[0;32m      6\u001b[0m     llm,\n\u001b[0;32m      7\u001b[0m     tools\u001b[38;5;241m=\u001b[39m[get_weather],\n\u001b[0;32m      8\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mdynamic_prompt_function\n\u001b[0;32m      9\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ChatDeepSeek' is not defined"
     ]
    }
   ],
   "source": [
    "# 初始化 OpenAI 模型\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0)\n",
    "\n",
    "# 创建代理，指定 tools 和 prompt（使用动态函数）\n",
    "agent_executor = create_react_agent(\n",
    "    llm,\n",
    "    tools=[get_weather],\n",
    "    prompt=dynamic_prompt_function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的 `prompt=dynamic_prompt_function` 是关键，它会在每次对话中被调用，用来定制提示内容。\n",
    "\n",
    "- 执行对话，传入消息和运行时配置（包含 user_id）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟用户提问\n",
    "response = agent_executor.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"你好，我是张三，我想知道北京的天气。\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"张三\"}}\n",
    ")\n",
    "\n",
    "# 输出模型回复\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 钩子\n",
    "\n",
    "在使用 create_react_agent 创建智能代理时，我们可以通过 hook 函数在模型调用前后动态插入逻辑，增强代理行为。\n",
    "\n",
    "##### **pre_model_hook（模型调用前执行）**\n",
    "**作用**：\n",
    "- 修改将发送给 LLM 的 消息历史（messages）\n",
    "- 注入动态的 系统提示\n",
    "- 添加 个性化上下文（如角色设定、时间、规则）\n",
    "- 压缩、总结、过滤 长对话记录\n",
    "\n",
    "**参数与返回**：\n",
    "- 参数： state: AgentState，表示当前状态（包含历史消息）\n",
    "\n",
    "- 返回：\n",
    "    - 修改 状态中 messages：{\"messages\": [...]}（将更新状态）\n",
    "    - 仅影响这次调用的输入：{\"llm_input_messages\": [...]}（不更新状态）\n",
    "\n",
    "**示例：动态注入系统提示 + 修剪消息历史（pre_model_hook）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, AnyMessage\n",
    "from langgraph.graph.message import REMOVE_ALL_MESSAGES\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "def pre_hook_message_management(state: AgentState) -> Dict[str, Any]:\n",
    "    print(\"⚙️ [pre_model_hook] 执行中...\")\n",
    "\n",
    "    # 获取原始消息历史\n",
    "    original_messages: List[AnyMessage] = state.get(\"messages\", [])\n",
    "\n",
    "    # 构造动态系统提示（例如限制回复长度）\n",
    "    system_prompt = \"请注意，您的回复长度不应超过100个字符。\"\n",
    "    new_system_message = SystemMessage(content=system_prompt)\n",
    "\n",
    "    # 修剪历史，仅保留最近3条对话（可调整）\n",
    "    relevant_history = original_messages[-3:]\n",
    "\n",
    "    # 新的输入消息：系统提示 + 精简后的对话历史\n",
    "    updated_messages = [new_system_message] + relevant_history\n",
    "\n",
    "    print(f\"📨 将发送给 LLM 的消息数: {len(updated_messages)}\")\n",
    "\n",
    "    # 方式 1：修改 AgentState 中的 messages（推荐）\n",
    "    return {\n",
    "        \"messages\": updated_messages\n",
    "    }\n",
    "\n",
    "    # 方式 2：仅影响当前 LLM 调用（不改变状态）\n",
    "    # return {\n",
    "    #     \"llm_input_messages\": updated_messages\n",
    "    # }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如何接入到 Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_pre_hook = create_react_agent(\n",
    "    llm,\n",
    "    tools=[get_weather],\n",
    "    pre_model_hook=pre_hook_message_management  # 接入前置钩子\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 示例调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输入多轮对话模拟\n",
    "input_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"第一条消息\"),\n",
    "        AIMessage(content=\"这是第一条的回复\"),\n",
    "        HumanMessage(content=\"第二条消息\"),\n",
    "        AIMessage(content=\"这是第二条的回复\"),\n",
    "        HumanMessage(content=\"第三条消息\"),\n",
    "        AIMessage(content=\"这是第三条的回复\"),\n",
    "        HumanMessage(content=\"第四条消息，查询北京天气。\"),\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 执行\n",
    "response = agent_with_pre_hook.invoke(input_state)\n",
    "print(\"🧠 模型回复：\", response['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **post_model_hook（模型调用后执行）**\n",
    "> ⚠️ 仅适用于 create_react_agent(..., version=\"v2\")，v2 是默认版本\n",
    "\n",
    "**作用**：\n",
    "- 对 **模型输出结果进行检查、过滤或增强**\n",
    "\n",
    "- 实现 **护栏（guardrails）**：如限制用词、情绪判断\n",
    "\n",
    "- 接入 **人工审核流程（Human-in-the-loop）**\n",
    "\n",
    "- 自定义输出格式，或根据结果触发后续逻辑\n",
    "\n",
    "\n",
    "**参数与返回**：\n",
    "- 参数： state: AgentState（包含 LLM 的新输出）\n",
    "\n",
    "- 返回： 修改后的 state 字典，如："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例：添加内容审核 + 系统提示**\n",
    "\n",
    "- 步骤一：定义 post_model_hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_hook_guardrail(state: AgentState) -> dict:\n",
    "    print(\"⚙️ [post_model_hook] 执行中...\")\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    latest = messages[-1] if messages else None\n",
    "\n",
    "    # 审查模型回复是否含有违禁词\n",
    "    flagged_words = [\"炸弹\", \"攻击\", \"暴力\"]\n",
    "    if isinstance(latest, AIMessage):\n",
    "        for word in flagged_words:\n",
    "            if word in latest.content:\n",
    "                warning = SystemMessage(\n",
    "                    content=\"⚠️ 检测到可能不当的内容，已记录审查，请谨慎处理。\"\n",
    "                )\n",
    "                return {\"messages\": messages + [warning]}\n",
    "    \n",
    "    # 如果没问题，不改动\n",
    "    return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步骤二：接入到代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_with_post_hook = create_react_agent(\n",
    "    llm,\n",
    "    tools=[get_weather],\n",
    "    post_model_hook=post_hook_guardrail  # 接入后置钩子\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 步骤三：运行代理，测试效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 模拟用户输入含敏感词\n",
    "response = agent_with_post_hook.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"如果我想制造炸弹，你能教我吗？\")\n",
    "    ]\n",
    "})\n",
    "\n",
    "# 输出最后一条消息\n",
    "for m in response[\"messages\"]:\n",
    "    print(f\"[{m.type}] {m.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**post_model_hook 常见场景举例**\n",
    "| 功能                 | 实现方法（post_model_hook 内部逻辑）                         |\n",
    "| -------------------- | ------------------------------------------------------------ |\n",
    "| 🚫 敏感词拦截         | 检查 `AIMessage.content` 是否包含违规词，追加警告 `SystemMessage` |\n",
    "| 🪧 自动免责声明附加   | 在每次模型回复后追加一句说明，如“此内容仅供参考”             |\n",
    "| 🔄 输出格式标准化     | 如将内容统一包装成 Markdown 或添加前缀、标签                 |\n",
    "| 👁️‍🗨️ 空回复/无效检查   | 若模型输出为空或过短，则自动补充说明（“对不起，我没理解你的问题”） |\n",
    "| 🛑 LLM 输出中断或撤回 | 检测敏感内容后，清空消息或替换成“该内容不予显示”             |\n",
    "| 🧠 结果再处理         | 对输出调用额外逻辑处理，如送入总结模型、分类器、embedding 搜索等 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### response_format 用法详解\n",
    "\n",
    "`response_format` 是 `create_react_agent` 的参数之一，用于要求代理 返回结构化输出（如 JSON），而不仅仅是自然语言回答。\n",
    "\n",
    "**场景：当你希望 AI 返回 机器可读结构（如 JSON 格式）时，比如：**\n",
    "- 自动评分系统返回 {score: float, reason: str}\n",
    "\n",
    "- 问答系统返回 {answer_text: str, confidence: float}\n",
    "\n",
    "- 对表单类输入解析结果 {name, email, reason}\n",
    "\n",
    "\n",
    "**机制说明**\n",
    "- 结构定义： 你提供一个 TypedDict 或 Pydantic 模型，作为输出的结构模板\n",
    "\n",
    "- 自动格式化： 在代理的 ReAct 循环结束后，LangGraph 会再次调用一次 LLM，要求其严格按照你提供的结构模式返回最终答案\n",
    "\n",
    "- 输出位置： 返回结果中会多出一个键：structured_response，里面就是结构化后的输出（对象或字典）\n",
    "\n",
    "\n",
    "**示例：返回回答 + 置信度**\n",
    "- 导入依赖 & 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel, Field\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_openai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprebuilt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_react_agent\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义结构化输出模型（Pydantic）\n",
    "class SimpleResponse(BaseModel):\n",
    "    answer_text: str = Field(..., description=\"对用户问题的直接回答\")\n",
    "    confidence: float = Field(..., description=\"回答的置信度（0.0 到 1.0）\", ge=0.0, le=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义一个简单工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_simple_answer(query: str) -> str:\n",
    "    \"\"\"根据查询提供一个简洁的答案。\"\"\"\n",
    "    if \"你好\" in query:\n",
    "        return \"你好！我是智能助手。\"\n",
    "    return \"我正在思考中，请稍后。\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建代理，指定 response_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[get_simple_answer],\n",
    "    prompt=\"你是一个高效、简洁的问答助手。回答要直接、有信心。\",\n",
    "    response_format=SimpleResponse  # 结构化输出\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 调用代理并解析结构化结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"你好，世界！\"\n",
    "response = structured_agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=user_query)]\n",
    "})\n",
    "\n",
    "# 提取自然语言回复\n",
    "print(\"🧠 模型自然语言回复:\")\n",
    "print(response[\"messages\"][-1].content)\n",
    "\n",
    "# 提取结构化输出\n",
    "print(\"\\n📦 结构化输出:\")\n",
    "structured_output = response.get(\"structured_response\")\n",
    "if structured_output:\n",
    "    if isinstance(structured_output, SimpleResponse):\n",
    "        print(f\"- 回答: {structured_output.answer_text}\")\n",
    "        print(f\"- 置信度: {structured_output.confidence}\")\n",
    "    else:\n",
    "        print(f\"- 回答: {structured_output.get('answer_text')}\")\n",
    "        print(f\"- 置信度: {structured_output.get('confidence')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意事项**\n",
    "\n",
    "| 事项           | 说明                                                         |\n",
    "| ------------ | ---------------------------------------------------------- |\n",
    "| ✅ 支持类型       | `Pydantic BaseModel`（推荐），或 Python `TypedDict`              |\n",
    "| ✅ 多字段支持      | 可以定义任意结构，包含嵌套对象、列表等                                        |\n",
    "| ⚠️ 会额外调用 LLM | 完成 ReAct 循环后，LangGraph 会**调用一次额外的格式化 prompt**              |\n",
    "| ⚠️ 不是强制约束    | 如果模型无法格式化成正确结构，`structured_response` 可能为空或报错（可 try-except） |\n",
    "| ⚠️ 不替代自然语言回复 | `messages` 中的 AI 回复仍然存在，两者可并行使用                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工具返回 Command 更新状态\n",
    "\n",
    "##### **什么是 Command**\n",
    "\n",
    "在 LangGraph 中，工具不仅可以返回字符串或数值，还可以返回一个特殊对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Command\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langgraph'"
     ]
    }
   ],
   "source": [
    "from langgraph.types import Command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个对象可以：\n",
    "\n",
    "✅ 向代理状态中添加或更新自定义字段（如日志、计数、缓存等）\n",
    "\n",
    "✅ 注入新的消息（如 ToolMessage）到 messages 列表中\n",
    "\n",
    "✅ 支持图中的自动合并（reducer）逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **为什么要用 Command？**\n",
    "\n",
    "| 传统返回值            | Command 返回值                |\n",
    "| ---------------- | -------------------------- |\n",
    "| 仅返回文本或数据         | 可以 **动态更新整个代理状态**          |\n",
    "| 只能影响 ToolMessage | 可以 **同时更新多字段（如计数器、缓存、记录）** |\n",
    "| 被动参与对话           | 可 **主动影响下一轮推理上下文和代理行为**    |\n",
    "\n",
    "##### **工具返回 Command：完整流程图**\n",
    "```sql\n",
    "+-----------+          +--------------+            +-------------+\n",
    "|  用户输入   |   -->    |   LLM 调用    |   -->     | 工具调用（Tool）|\n",
    "+-----------+          +--------------+            +-------------+\n",
    "                                                      |\n",
    "                                                      v\n",
    "                                            +---------------------+\n",
    "                                            | 返回 Command(update) |\n",
    "                                            +---------------------+\n",
    "                                                      |\n",
    "                                                      v\n",
    "                                  状态更新: messages / 自定义字段 / 等等\n",
    "```\n",
    "\n",
    "##### **示例回顾 + 改进说明**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def record_action_and_get_length(\n",
    "        text_input: str,\n",
    "        state: Annotated[dict, ...],  # 如果需要状态的话\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],  # ✅ 正确注入工具调用 ID\n",
    ") -> Command:\n",
    "    \"\"\"记录文本长度并更新调用计数。\"\"\"\n",
    "    length = len(text_input)\n",
    "    count = state.get(\"tool_execution_count\", 0) + 1\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                ToolMessage(\n",
    "                    content=f\"文本长度是 {length}。这是第 {count} 次调用。\",\n",
    "                    tool_call_id=tool_call_id\n",
    "                )\n",
    "            ],\n",
    "            \"tool_execution_count\": count,\n",
    "            \"last_tool_name\": \"record_action_and_get_length\"\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ 注意：\n",
    "- tool_call_id 必须包含在返回的 ToolMessage 中，才能与 LLM 推理闭环对齐\n",
    "\n",
    "- 若希望状态字段（如 tool_execution_count）能累加，要么：\n",
    "\n",
    "    1. 显式读取并计算新值（如上例）\n",
    "    \n",
    "    2. 配合定义 reducer，如 operator.add（可在自定义 graph 中用）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **创建代理：支持 Command 的完整配置**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# 自定义状态类型，添加额外字段\n",
    "class StateWithToolUpdate(AgentState):\n",
    "    last_tool_name: str | None = None\n",
    "    tool_execution_count: int = 0\n",
    "\n",
    "# 创建代理\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[record_action_and_get_length],\n",
    "    state_schema=StateWithToolUpdate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **运行代理并查看状态变更**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "result = agent.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"请帮我记录：'这是一段测试文本'\")\n",
    "    ]\n",
    "})\n",
    "\n",
    "# 打印返回状态\n",
    "print(\"📨 最后消息:\", result[\"messages\"][-1].content)\n",
    "print(\"🛠️ 最后调用工具:\", result.get(\"last_tool_name\"))\n",
    "print(\"🔢 工具调用次数:\", result.get(\"tool_execution_count\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **进阶提示：Command 的常见用法**\n",
    "| 场景              | 描述                                          |\n",
    "| --------------- | ------------------------------------------- |\n",
    "| 🔁 计数器递增        | 如工具调用次数、任务完成次数等                             |\n",
    "| 🕓 记录时间戳 / 执行历史 | 可记录每次执行时间、上下文等                              |\n",
    "| 📥 缓存中间结果       | 如用户偏好、token 用量等                             |\n",
    "| 🧹 清空 messages  | 返回 `Command(update={\"messages\": []})`       |\n",
    "| 🔁 替换 / 重写状态字段  | `Command(update={\"some_field\": new_value})` |\n",
    "| ✅ 多工具之间协同更新状态   | 各工具可写入状态中的不同片段（如子任务进度等）                     |\n",
    "\n",
    "\n",
    "##### **总结：为什么推荐使用 Command**\n",
    "\n",
    "| 优点                   | 说明                                             |\n",
    "| -------------------- | ---------------------------------------------- |\n",
    "| ✅ 解耦逻辑 & 状态管理        | 工具只管业务逻辑，状态更新交由框架合并                            |\n",
    "| ✅ 易于调试与扩展            | 所有状态变更集中于 `update` 字典中                         |\n",
    "| ✅ 与 LangGraph 框架原生兼容 | create\\_react\\_agent 的 ToolNode 自动处理 `Command` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InjectedState: 工具函数访问「当前对话状态」\n",
    "\n",
    "##### 🌟 作用\n",
    "允许工具访问 LangGraph 图中的“短期状态”（如历史消息、计数器、用户偏好），不需要 LLM 显式传参。\n",
    "\n",
    "##### 使用方式\n",
    "- 在工具参数中通过 `Annotated[<Type>, InjectedState]` 或更显式指定字段名 `Annotated[<Type>, InjectedState(\"字段名\")]` 实现注入。\n",
    "\n",
    "- 注入的是当前 Graph/Agent 的状态字段，必须在 `state_schema` 中声明。\n",
    "\n",
    "⚠️ 注意事项\n",
    "- Reducer 函数一定要匹配字段更新方式：如你在 `update={\"user_query_history\": [xxx]}` 中传的是列表，就需要 reducer 支持合并列表。\n",
    "\n",
    "- 字段必须提前定义，否则运行时报错。\n",
    "\n",
    "- 如果 `state_schema` 用 TypedDict 自定义而非继承 AgentState，要注意显式添加 remaining_steps 字段，否则 LangGraph 报错。\n",
    "\n",
    "##### 示例回顾（追加历史查询）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyToolState(TypedDict):\n",
    "    messages: Annotated[List[ToolMessage], add_messages]\n",
    "    user_query_history: Annotated[List[str], lambda cur, new: (cur or []) + (new or [])]\n",
    "    remaining_steps: int | None\n",
    "\n",
    "\n",
    "# 工具函数：记录并回显用户查询\n",
    "@tool\n",
    "def remember_and_echo_query(\n",
    "        current_query: str,\n",
    "        state: Annotated[MyToolState, InjectedState],\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId]\n",
    ") -> Command:\n",
    "    \"\"\"记住当前查询并回显所有历史查询。\"\"\"\n",
    "    history = state.get(\"user_query_history\", [])\n",
    "    reply = f\"当前查询: '{current_query}'\\n历史查询: {history}\"\n",
    "    return Command(update={\n",
    "        \"messages\": [ToolMessage(content=reply, tool_call_id=tool_call_id)],\n",
    "        \"user_query_history\": [current_query]\n",
    "    })\n",
    "\n",
    "\n",
    "# 初始化 LLM 和代理\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[remember_and_echo_query],\n",
    "    state_schema=MyToolState\n",
    ")\n",
    "\n",
    "# 第一次调用\n",
    "first_input = {\n",
    "    \"messages\": [HumanMessage(content=\"苹果有什么营养？\")],\n",
    "    \"user_query_history\": []\n",
    "}\n",
    "res1 = agent.invoke(first_input)\n",
    "print(f\"\\n【第一次调用结果】\\n{res1['messages'][-1].content}\")\n",
    "\n",
    "# 第二次调用（使用更新后的历史）\n",
    "res2 = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"橘子有维C吗？\")] + res1[\"messages\"],\n",
    "    \"user_query_history\": res1[\"user_query_history\"]\n",
    "})\n",
    "print(f\"\\n【第二次调用结果】\\n{res2['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### InjectedStore: 工具函数访问「图外部持久存储」\n",
    "\n",
    "##### 🌟 作用\n",
    "用于访问图之外的存储资源（如向量数据库、KV 存储、文件系统等），支持工具具备“长期记忆”能力。\n",
    "\n",
    "##### 使用方式\n",
    "创建 agent 时指定 store=store_instance\n",
    "\n",
    "工具内声明：`store: Annotated[BaseStore, InjectedStore]`\n",
    "\n",
    "##### 注意事项\n",
    "InjectedStore 注入的是 BaseStore 实现，如 InMemoryStore, RedisStore, WeaviateStore 等\n",
    "\n",
    "键值要规范：(namespace_tuple, key) 结构，比如 (\"user123\", \"notebook\")\n",
    "\n",
    "##### 示例回顾（访问 FAQ 知识库）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tool' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@tool\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlookup_faq\u001b[39m(topic: \u001b[38;5;28mstr\u001b[39m, store: Annotated[BaseStore, InjectedStore]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      3\u001b[0m     item \u001b[38;5;241m=\u001b[39m store\u001b[38;5;241m.\u001b[39mget((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglobal_kb\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaq\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlanggraph_def\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tool' is not defined"
     ]
    }
   ],
   "source": [
    "# 初始化一个 LLM（你也可以换成自己的模型）\n",
    "# ✅ 初始化 Store，并手动存入一条 FAQ 数据\n",
    "store = InMemoryStore()\n",
    "store.put((\"global_kb\", \"faq\"), key=\"langgraph_def\", value={\n",
    "    \"answer\": \"LangGraph 是一个构建有状态、多智能体应用的框架。\"\n",
    "})\n",
    "\n",
    "# ✅ 工具函数：使用 InjectedStore 来查询知识\n",
    "@tool\n",
    "def lookup_faq(topic: str, store: Annotated[BaseStore, InjectedStore]) -> str:\n",
    "    \"\"\"根据主题从知识库中查询 FAQ 答案。\"\"\"\n",
    "    # 简化：假设我们知道 key (或通过其他方式获取)\n",
    "    item = store.get((\"global_kb\", \"faq\"), \"langgraph_def\")\n",
    "    if item:\n",
    "        return f\"答案是: {item.value.get('answer')}\"\n",
    "    return f\"找不到关于 {topic} 的内容。\"\n",
    "\n",
    "# ✅ 创建代理\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[lookup_faq],\n",
    "    store=store  # 将 store 注入整个图中\n",
    ")\n",
    "\n",
    "# ✅ 调用代理\n",
    "response = agent.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"请告诉我 LangGraph 是什么？\")]\n",
    "})\n",
    "\n",
    "# ✅ 输出结果\n",
    "print(\"🤖 最终回复:\", response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多智能体系统预构建模式\n",
    "\n",
    "LangGraph 通过外部库（如 langgraph-supervisor 和 langgraph-swarm）支持常见的的多智能体协作模式。这些库通常提供了便捷的函数（如 create_supervisor 和 create_swarm）来快速搭建这些架构。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 主管模式 (Supervisor Pattern)\n",
    "\n",
    "Supervisor 模式是一种**中心化**的多智能体架构。一个“主管” (Supervisor) 代理负责协调多个“工作者” (Worker) 代理。主管接收任务、分解任务、委派给合适的工作者，并整合结果。通信通常是 Supervisor -> Worker -> Supervisor。langgraph-supervisor 库提供了 create_supervisor 函数来帮助构建。核心在于主管代理使用“切换工具” (Handoff tools) 将控制权和任务描述传递给工作者。    \n",
    "\n",
    "- **任务委派 (task_description 与 Send)**: 主管可以明确地为工作者制定任务描述，并通过 `Send()` 原语传递，使工作者只接收特定任务输入。    \n",
    "- **Command.PARENT**: 工作者代理（通常是子图）内部的工具或节点可以使用 `Command(graph=Command.PARENT, goto=\"...\")` 将控制权交还给父图（主管图）中的其他节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **代码示例 (概念性 Supervisor)**\n",
    "###### **目标：构建一个 “主管-工作者” 智能体协作系统**\n",
    "\n",
    "将学会如何：\n",
    "\n",
    "- 创建多个智能体（如研究员、写手）\n",
    "\n",
    "- 通过 Supervisor 协调他们的任务执行\n",
    "\n",
    "- 使用 langgraph-supervisor 简化管理流程\n",
    "\n",
    "- 理解底层是如何实现任务委派与子图切换的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **安装所需库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph-supervisor langchain langchain-deepseek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List, TypedDict\n",
    "import operator # 👈 导入 operator\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **定义工作者工具（Tools）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tool\n\u001b[0;32m      3\u001b[0m \u001b[38;5;129m@tool\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresearch_tool\u001b[39m(topic: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m      5\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"根据主题进行研究并返回摘要\"\"\"\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def research_tool(topic: str) -> str:\n",
    "    \"\"\"\n",
    "    根据主题进行研究并返回摘要。\n",
    "    :param topic: 主题\n",
    "    :return: 摘要\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 研究员：正在研究 '{topic}'... ---\")\n",
    "    return f\"关于 '{topic}' 的研究结果摘要：LangGraph是一个强大的库，它允许用户使用类似图的结构来编排、组合和运行大语言模型（LLM）应用。它特别适合构建复杂的、有状态的多 Agent 系统。\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def writing_tool(research_summary: str, tone: str) -> str:\n",
    "    \"\"\"\n",
    "    根据研究摘要和语气撰写内容。\n",
    "    :param research_summary: 摘要\n",
    "    :param tone: 语气\n",
    "    :return: 内容\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 写手：根据摘要 '{research_summary[:20]}...' 以 '{tone}' 语气撰写... ---\")\n",
    "    return f\"这是一篇以'{tone}'语气撰写的关于LangGraph的文章：\\n\\n**LangGraph：构建复杂大语言模型应用的利器**\\n\\n{research_summary}\\n\\n总而言之，LangGraph为开发者提供了一套强大而灵活的工具，是构建下一代智能应用的关键组件。\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **创建工作者代理 (Worker Agents)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "DEEPSEEK_API_KEY = \"xxxxxxxx\"\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\", \n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "research_worker = create_react_agent(llm, tools=[research_tool], name='researcher')\n",
    "writing_worker = create_react_agent(llm, tools=[writing_tool], name='writer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **自定义 StateSchema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class MySupervisorState(BaseModel):\n",
    "    messages: List[BaseMessage]\n",
    "    remaining_steps: Optional[int] = 10  # 默认最多运行10步\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **为主管预绑定工具**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTask(BaseModel):\n",
    "    task_description: str = Field(description=\"需要代理执行的具体任务描述。\")\n",
    "    tone: str = Field(description=\"内容的语气，例如：正式、非正式、幽默等。\", default=\"正式\")\n",
    "\n",
    "\n",
    "@tool(\"transfer_to_researcher\", args_schema=AgentTask)\n",
    "def transfer_to_researcher_tool(task_description: str, tone: str):\n",
    "    \"\"\"将需要研究的任务路由给研究员代理。\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "@tool(\"transfer_to_writer\", args_schema=AgentTask)\n",
    "def transfer_to_writer_tool(task_description: str, tone: str):\n",
    "    \"\"\"将需要撰写的任务路由给写手代理。\"\"\"\n",
    "    pass\n",
    "\n",
    "supervisor_llm = llm.bind_tools([transfer_to_researcher_tool, transfer_to_writer_tool])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个 pass 函数扮演了两个关键的、但非执行性的角色：\n",
    "\n",
    "- **为LLM提供API契约**：它们是“只读”的。它们的存在是为了被langchain内省（Introspection），提取出它们的名称、描述和参数，并构建成一份给LLM看的工具清单。这是LLM决定**“能做什么”和“如何做”**的依据。\n",
    "- **为Graph提供路由的“钩子”**：工具的名称（如 transfer_to_researcher）就像一个唯一的ID或“钩子”。Graph通过这个ID来匹配并调用**真正干活**的代理（research_worker）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **定义状态和创建 Supervisor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_supervisor import create_supervisor # 👈 使用标准导入\n",
    "\n",
    "# ⭐️⭐️⭐️【最终修正点】⭐️⭐️⭐️\n",
    "# 使用 TypedDict 和 Annotated 来定义状态，以允许消息列表的合并\n",
    "class SupervisorState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "    remaining_steps: int = 10\n",
    "\n",
    "\n",
    "\n",
    "# *** 这是关键的修改点 ***\n",
    "# 为 Supervisor 提供一个清晰的工作流程指令\n",
    "supervisor_prompt = \"\"\"你是一个项目主管，负责监督一个由研究员和写手组成的团队。\n",
    "你的工作是根据用户的请求，将任务分解并按顺序分配给合适的专员。\n",
    "\n",
    "工作流程如下:\n",
    "1.  **研究阶段**: 用户的请求首先需要进行研究。你必须使用 `transfer_to_researcher_tool` 工具将任务分配给研究员。\n",
    "2.  **写作阶段**: 在研究员完成工作并返回研究摘要后（你会在对话历史中看到来自 `research_tool` 的 `ToolMessage`），你必须使用 `transfer_to_writer_tool` 工具，将研究摘要和指定的语气一起分配给写手。\n",
    "3.  **完成**: 在写手完成后，整个任务就结束了。\n",
    "\n",
    "你的唯一职责就是通过调用 `transfer_to_...` 工具来分配任务，绝对不要自己完成研究或写作。\n",
    "请根据当前的对话历史，决定下一步应该由谁来处理。\n",
    "\"\"\"\n",
    "\n",
    "# 创建主管，现在它将使用新的、正确的状态定义\n",
    "supervisor_agent_graph = create_supervisor(\n",
    "    model=supervisor_llm,\n",
    "    agents=[research_worker, writing_worker],\n",
    "    state_schema=SupervisorState,\n",
    "    prompt=supervisor_prompt,  # 👈 传入我们定制的“工作手册”\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **执行流程**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_graph = supervisor_agent_graph.compile()\n",
    "\n",
    "# 初始输入\n",
    "initial_input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"请研究LangGraph并撰写一篇介绍性文章，语气要正式。\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 流式运行\n",
    "for event in compiled_graph.stream(initial_input, {\"recursion_limit\": 10}):\n",
    "    for key, value in event.items():\n",
    "        print(f\"--- Event: Node '{key}' ---\")\n",
    "        if 'messages' in value:\n",
    "            # 为了更清晰的展示，只打印 message 的内容\n",
    "            for msg in value['messages']:\n",
    "                print(f\"    {type(msg).__name__}: {getattr(msg, 'content', 'No content')}\")\n",
    "        else:\n",
    "            print(value)\n",
    "    print(\"\\n========================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **深入理解 supervospr**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated, List, Optional, TypedDict\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "import operator\n",
    "\n",
    "\n",
    "# --- 1. 定义工具 (和之前一样) ---\n",
    "@tool\n",
    "def research_tool(topic: str) -> str:\n",
    "    \"\"\"根据主题进行研究并返回摘要。\"\"\"\n",
    "    print(f\"\\n--- 研究员：正在研究 '{topic}'... ---\")\n",
    "    return f\"关于 '{topic}' 的研究结果摘要：LangGraph是一个强大的库...\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def writing_tool(research_summary: str, tone: str) -> str:\n",
    "    \"\"\"根据研究摘要和语气撰写内容。\"\"\"\n",
    "    print(f\"\\n--- 写手：根据摘要 '{research_summary[:20]}...' 以 '{tone}' 语气撰写... ---\")\n",
    "    return f\"以'{tone}'语气撰写的关于'{research_summary[:20]}...'的内容...\"\n",
    "\n",
    "\n",
    "tools = [research_tool, writing_tool]\n",
    "\n",
    "# --- 2. 绑定工具到 LLM，这是我们将要重复使用的模型 ---\n",
    "from langchain_deepseek import ChatDeepSeek\n",
    "\n",
    "# 为了安全和方便，建议使用环境变量\n",
    "# DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "DEEPSEEK_API_KEY = \"sk-...\"\n",
    "\n",
    "# 将工具绑定到模型。这将使模型能够在其响应中生成 tool_calls\n",
    "llm = ChatDeepSeek(model=\"deepseek-chat\", temperature=0, api_key=DEEPSEEK_API_KEY).bind_tools(tools)\n",
    "\n",
    "\n",
    "# --- 3. 定义代理状态 (Agent State) ---\n",
    "# 这是整个图共享的数据结构\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "# --- 4. 定义图的节点 (Nodes) ---\n",
    "\n",
    "# 4.1 定义“主管”节点\n",
    "# 这个节点负责决定下一步做什么\n",
    "def supervisor_node(state: AgentState):\n",
    "    print(\"\\n--- 主管：正在决策... ---\")\n",
    "    # 获取所有历史消息\n",
    "    messages = state['messages']\n",
    "    # 调用LLM进行决策\n",
    "    response = llm.invoke(messages)\n",
    "    # response 将是一个 AIMessage，可能包含 content 或 tool_calls\n",
    "    # 我们直接将这个决策消息返回，它将被添加到状态中\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 4.2 定义“工具”节点\n",
    "# 这个节点负责实际执行工具\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "# --- 5. 定义图的路由逻辑 (Conditional Edges) ---\n",
    "# 这是最关键的部分，它决定了执行完一个节点后，接下来应该去哪里\n",
    "def router(state: AgentState) -> str:\n",
    "    print(\"\\n--- 路由器：正在判断路径... ---\")\n",
    "    # 获取最后一条消息\n",
    "    last_message = state['messages'][-1]\n",
    "\n",
    "    # 如果最后一条消息是 AIMessage 并且包含 tool_calls...\n",
    "    if isinstance(last_message, AIMessage) and last_message.tool_calls:\n",
    "        # ...那么下一步就是去执行工具\n",
    "        print(\"路由器决策：调用工具。\")\n",
    "        return \"call_tool\"\n",
    "\n",
    "    # 否则，意味着工具已经执行完毕，或者AI给出了最终回复\n",
    "    # 我们需要结束流程\n",
    "    print(\"路由器决策：结束流程。\")\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "# --- 6. 构建图 (Graph) ---\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "graph = StateGraph(AgentState)\n",
    "\n",
    "# 添加节点到图中\n",
    "graph.add_node(\"supervisor\", supervisor_node)\n",
    "graph.add_node(\"call_tool\", tool_node)\n",
    "\n",
    "# 设置入口点\n",
    "graph.set_entry_point(\"supervisor\")\n",
    "\n",
    "# 添加条件路由\n",
    "graph.add_conditional_edges(\n",
    "    \"supervisor\",  # 从 supervisor 节点出来后，调用 router 函数判断\n",
    "    router,\n",
    "    {\n",
    "        \"call_tool\": \"call_tool\",  # 如果 router 返回 \"call_tool\"，则去 call_tool 节点\n",
    "        \"end\": END  # 如果 router 返回 \"end\"，则结束图的执行\n",
    "    }\n",
    ")\n",
    "\n",
    "# 当工具执行完毕后，总是返回给主管进行下一次决策\n",
    "graph.add_edge(\"call_tool\", \"supervisor\")\n",
    "\n",
    "# 编译图\n",
    "runnable_graph = graph.compile()\n",
    "\n",
    "# --- 7. 运行图 ---\n",
    "initial_input = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"请研究LangGraph并撰写一篇关于它的介绍性文章，语气要正式。\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 使用 stream 模式查看每一步的输出\n",
    "for output in runnable_graph.stream(initial_input, config={\"recursion_limit\": 10}):\n",
    "    # output 是每个节点执行后的状态快照\n",
    "    print(\"--- 图执行步骤输出 ---\")\n",
    "    print(output)\n",
    "    print(\"======================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 群体模式 (Swarm Pattern)\n",
    "\n",
    "Swarm 模式是一种去中心化的多智能体架构，其中代理根据其专长动态地相互移交控制权，而无需中央控制器。系统通常会记住最后一个活动的代理。`langgraph-swarm` 库提供了 `create_swarm` 函数和 `create_handoff_tool` 来帮助构建此类系统。切换逻辑同样依赖于 `Command.PARENT`。    \n",
    "\n",
    "**智能切换与冲突解决**: `langgraph-swarm` 的基础实现依赖于代理 LLM 基于其提示和切换工具的描述来决定切换。对于更智能的切换（例如，当多个代理都声称能处理任务时），可能需要在主群体图中实现一个自定义的路由节点，或者为代理设计更复杂的提示和切换逻辑。文档中未详细说明 langgraph-swarm 内置的高级冲突解决机制 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Langgraph-Swarm 多智能体协作示例**\n",
    "\n",
    "本教程演示如何使用 langgraph-swarm 库，实现多个智能体（Agent）协作，通过“切换工具”（handoff tool）在智能体间切换任务处理，实现复杂任务拆分与协同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **安装依赖**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph-swarm langchain langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **导入必要库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_swarm import create_swarm, create_handoff_tool\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **定义智能体工具（工具函数）**\n",
    "智能体会依赖的功能模块，比如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_stock_price(ticker: str) -> str:\n",
    "    \"\"\"获取股票价格。\"\"\"\n",
    "    print(f\"金融代理：查询股票 '{ticker}' 价格...\")\n",
    "    return f\"股票 '{ticker}' 的价格是 $100。\"\n",
    "\n",
    "@tool\n",
    "def analyze_news_sentiment(news_article_url: str) -> str:\n",
    "    \"\"\"分析新闻文章的情感。\"\"\"\n",
    "    print(f\"新闻分析代理：分析新闻 '{news_article_url}'...\")\n",
    "    return f\"新闻 '{news_article_url}' 的情感是积极的。\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **初始化语言模型（LLM）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **创建切换工具（Handoff Tools）**\n",
    "\n",
    "切换工具用于智能体间任务转移："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handoff_to_news_analyst = create_handoff_tool(\n",
    "    agent_name=\"news_analyst\",\n",
    "    description=\"当需要分析新闻文章的情感时，将任务移交给新闻分析员。\"\n",
    ")\n",
    "\n",
    "handoff_to_finance_expert = create_handoff_tool(\n",
    "    agent_name=\"financial_expert\",\n",
    "    description=\"当需要获取股票信息时，将任务移交给金融专家。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **创建智能体代理（Agents）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[get_stock_price, handoff_to_news_analyst],\n",
    "    name=\"financial_expert\",\n",
    "    prompt=( # 👈 修改 Prompt\n",
    "        \"你是一个金融专家。你的任务是处理所有与金融相关的请求。\\n\"\n",
    "        \"请遵循以下步骤：\\n\"\n",
    "        \"1. 首先，使用你的工具（如 get_stock_price）来回答所有你能回答的金融问题。\\n\"\n",
    "        \"2. 在你完成了所有金融任务之后，如果用户的请求中还包含需要新闻分析的部分，请只使用 'transfer_to_news_analyst' 工具来移交任务。\\n\"\n",
    "        \"3. 不要将调用普通工具和调用移交工具在同一步进行。\"\n",
    "    )\n",
    ")\n",
    "\n",
    "news_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[analyze_news_sentiment, handoff_to_finance_expert],\n",
    "    name=\"news_analyst\",\n",
    "    prompt=( # 👈 修改 Prompt\n",
    "        \"你是一个新闻分析员。你的任务是处理所有与新闻分析相关的请求。\\n\"\n",
    "        \"请遵循以下步骤：\\n\"\n",
    "        \"1. 首先，使用你的工具（如 analyze_news_sentiment）来完成所有你能做的新闻分析。\\n\"\n",
    "        \"2. 在你完成了所有新闻分析任务之后，如果用户的请求中还包含需要金融信息的部分，请只使用 'transfer_to_finance_expert' 工具来移交任务。\\n\"\n",
    "        \"3. 不要将调用普通工具和调用移交工具在同一步进行。\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个智能体有自己的工具和切换工具，名字需与切换工具中 agent_name 保持一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  **创建 Swarm 群体，编译图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_swarm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m compiled_swarm_graph \u001b[38;5;241m=\u001b[39m create_swarm(\n\u001b[0;32m      2\u001b[0m     agents\u001b[38;5;241m=\u001b[39m[finance_agent, news_agent],\n\u001b[0;32m      3\u001b[0m     default_active_agent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m金融专家\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mcompile()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'create_swarm' is not defined"
     ]
    }
   ],
   "source": [
    "compiled_swarm_graph = create_swarm(\n",
    "    agents=[finance_agent, news_agent],\n",
    "    default_active_agent=\"financial_expert\"\n",
    ").compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **输入初始任务，执行流式处理**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_swarm_input = {\n",
    "    \"messages\": [HumanMessage(content=\"请帮我查找 AAPL 的股价，并分析这篇关于他们新产品的文章的情感：example.com/news\")]\n",
    "}\n",
    "\n",
    "for event in compiled_swarm_graph.stream(initial_swarm_input, {\"recursion_limit\": 10}):\n",
    "    print(event)\n",
    "\n",
    "# AIMessage：content='AAPL 的当前股价是 $100。关于他们新产品的文章情感分析显示为积极。'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第九部分：深入LangGraph 核心机制和高级特性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高级状态管理\n",
    "\n",
    "状态是 LangGraph 应用的核心，它在节点间传递并演化。\n",
    "\n",
    "#### 状态定义方式：TypedDict vs. Pydantic BaseModel\n",
    "在 LangGraph 中，状态是核心概念，定义了各个节点如何读取、更新和传递数据。\n",
    "\n",
    "可以通过两种方式定义状态：\n",
    "\n",
    "\n",
    "##### **1. TypedDict - 标准 & 轻量**\n",
    "TypedDict 是 Python 标准库 typing 中提供的类型提示工具，用于定义类似字典的结构。\n",
    "\n",
    "特点：\n",
    "\n",
    "- ✅ 无需安装额外依赖\n",
    "\n",
    "- ✅ 更轻量，运行无开销\n",
    "\n",
    "- ❌ 无运行时验证（出错时容易难以定位）\n",
    "\n",
    "##### **2. Pydantic BaseModel - 安全 & 更强大\n",
    "Pydantic.BaseModel 提供运行时的类型验证、默认值、字段描述等，更适合需要输入验证的图。\n",
    "\n",
    "特点：\n",
    "\n",
    "- ✅ 自动校验输入字段类型\n",
    "\n",
    "- ✅ 支持默认值、字段描述\n",
    "\n",
    "- ✅ 与 LangChain 的消息类型良好集成（使用 AnyMessage）\n",
    "\n",
    "- ❌ 图最终输出为 dict，不是 Pydantic 实例\n",
    "\n",
    "- ❌ 类型强制转换有时导致“静默出错”\n",
    "\n",
    "- ❌ 验证失败时无法直接定位到出错节点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **状态字段中包含 LangChain 消息时注意**\n",
    "\n",
    "建议字段类型为：\n",
    "```python\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "history: List[AnyMessage]\n",
    "\n",
    "```\n",
    "因为 AnyMessage 是 LangChain 所有消息类型（如 HumanMessage、AIMessage）的统一父类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Pydantic 状态使用示例**\n",
    "\n",
    "以下示例展示如何使用 Pydantic BaseModel 定义图状态，并实现自动校验：\n",
    "\n",
    "> 如果在构建 对话图、多节点数据依赖处理 或 Supervisor/Swarm 模式 时需要确保类型安全，推荐使用 Pydantic BaseModel。\n",
    "\n",
    "\n",
    "###### **第一步：定义状态模型**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_core'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseModel \u001b[38;5;28;01mas\u001b[39;00m PydanticBaseModel, Field \u001b[38;5;28;01mas\u001b[39;00m PydanticField\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AnyMessage\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyPydanticGraphState\u001b[39;00m(PydanticBaseModel):\n\u001b[0;32m      6\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m PydanticField(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m用户的查询语句\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langchain_core'"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel as PydanticBaseModel, Field as PydanticField\n",
    "from typing import List\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "class MyPydanticGraphState(PydanticBaseModel):\n",
    "    query: str = PydanticField(description=\"用户的查询语句\")\n",
    "    results: List[str] = PydanticField(default_factory=list, description=\"处理结果列表\")\n",
    "    history: List[AnyMessage] = PydanticField(default_factory=list)  # LangChain 消息记录\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第二步：定义图节点**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "def process_query_pydantic_node(state: MyPydanticGraphState) -> Dict[str, Any]:\n",
    "    print(f\"Pydantic节点：处理查询 '{state.query}'\")\n",
    "    return {\n",
    "        \"results\": [f\"处理 '{state.query}' 的结果\"],\n",
    "        \"history\": [AIMessage(content=f\"已处理查询: {state.query}\")]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第三步：构建图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "pydantic_builder = StateGraph(MyPydanticGraphState)\n",
    "pydantic_builder.add_node(\"process_query\", process_query_pydantic_node)\n",
    "pydantic_builder.add_edge(START, \"process_query\")\n",
    "pydantic_builder.add_edge(\"process_query\", END)\n",
    "\n",
    "compiled_pydantic_graph = pydantic_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第四步：调用图 - 正确输入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "valid_input = {\n",
    "    \"query\": \"LangGraph Pydantic\",\n",
    "    \"history\": [HumanMessage(content=\"你好\")]\n",
    "}\n",
    "\n",
    "output = compiled_pydantic_graph.invoke(valid_input)\n",
    "print(f\"\\n✅ 输出: {output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第五步：调用图 - 验证失败示例**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "invalid_input = {\"query\": 123}  # ❌ query 应该是 str 类型\n",
    "\n",
    "try:\n",
    "    compiled_pydantic_graph.invoke(invalid_input)\n",
    "except ValidationError as e:\n",
    "    print(f\"\\n❌ 捕获到验证错误: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义 Reducer 处理复杂状态更新\n",
    "\n",
    "在 LangGraph 中，每个状态字段（channel）都可以定义自己的合并策略（Reducer）。这在处理如下情况时非常关键：\n",
    "\n",
    "- 深度合并嵌套字典\n",
    "\n",
    "- 控制列表追加或去重\n",
    "\n",
    "- 自定义冲突解决逻辑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 默认行为：覆盖式合并\n",
    "默认情况下，LangGraph 对状态的更新采用“覆盖”策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state[key] = node_output[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自定义 Reducer 的能力\n",
    "通过 typing.Annotated，你可以为状态字段显式指定 Reducer 函数，例如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "user_settings: Annotated[dict, deep_update_reducer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **代码实战：深度合并 user_settings**\n",
    "我们来定义一个 Reducer，用于合并嵌套字典中的设置，而不是完全覆盖旧的设置。\n",
    "\n",
    "###### **第一步：定义 Reducer 函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_update_reducer(current_val: Optional[dict], new_update: Optional[dict]) -> dict:\n",
    "    if new_update is None:\n",
    "        return current_val if current_val is not None else {}\n",
    "    if current_val is None:\n",
    "        return new_update.copy()\n",
    "\n",
    "    merged = current_val.copy()\n",
    "    for key, value in new_update.items():\n",
    "        if isinstance(value, dict) and isinstance(merged.get(key), dict):\n",
    "            merged[key] = deep_update_reducer(merged[key], value)  # 递归合并\n",
    "        else:\n",
    "            merged[key] = value\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第二步：定义状态结构（TypedDict + Reducer）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List\n",
    "import operator  # 用于列表追加 Reducer\n",
    "\n",
    "class MyGraphStateWithCustomReducer(TypedDict):\n",
    "    user_settings: Annotated[dict, deep_update_reducer]\n",
    "    action_log: Annotated[List[str], operator.add]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第三步：定义节点函数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theme_node(state: MyGraphStateWithCustomReducer) -> dict:\n",
    "    print(\"🖌️ update_theme_node：设置 dark 模式\")\n",
    "    return {\n",
    "        \"user_settings\": {\"preferences\": {\"theme\": \"dark\"}},\n",
    "        \"action_log\": [\"主题已更新为 dark\"]\n",
    "    }\n",
    "\n",
    "def update_notifications_node(state: MyGraphStateWithCustomReducer) -> dict:\n",
    "    print(\"🔔 update_notifications_node：开启通知\")\n",
    "    return {\n",
    "        \"user_settings\": {\"preferences\": {\"notifications\": True}},\n",
    "        \"action_log\": [\"通知已开启\"]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第四步：构建 LangGraph 流程图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(MyGraphStateWithCustomReducer)\n",
    "builder.add_node(\"update_theme\", update_theme_node)\n",
    "builder.add_node(\"update_notifications\", update_notifications_node)\n",
    "builder.add_edge(START, \"update_theme\")\n",
    "builder.add_edge(\"update_theme\", \"update_notifications\")\n",
    "builder.add_edge(\"update_notifications\", END)\n",
    "\n",
    "compiled_graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **第五步：初始化状态并运行**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\n",
    "    \"user_settings\": {\n",
    "        \"name\": \"Alice\",\n",
    "        \"preferences\": {\"fontSize\": 12}\n",
    "    },\n",
    "    \"action_log\": [\"初始化\"]\n",
    "}\n",
    "\n",
    "final_state = compiled_graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n✅ 最终状态输出：\")\n",
    "print(final_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **输出预期**\n",
    "```json\n",
    "{\n",
    "  'user_settings': {\n",
    "    'name': 'Alice',\n",
    "    'preferences': {\n",
    "      'fontSize': 12,\n",
    "      'theme': 'dark',\n",
    "      'notifications': True\n",
    "    }\n",
    "  },\n",
    "  'action_log': [\n",
    "    '初始化',\n",
    "    '主题已更新为 dark',\n",
    "    '通知已开启'\n",
    "  ]\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 总结\n",
    "| 功能              | 方法                                 |\n",
    "| --------------- | ---------------------------------- |\n",
    "| 为每个字段指定 Reducer | 使用 `Annotated[type, reducer_func]` |\n",
    "| 嵌套字典合并          | 自定义 `deep_update_reducer`          |\n",
    "| 列表追加            | 使用内置 `operator.add`                |\n",
    "| 控制更新策略          | Reducer 决定如何合并新旧值                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 显式输入/输出接口 & 私有状态通道（Private Channels）\n",
    "LangGraph 支持在图构建阶段通过 input= 与 output= 显式定义输入输出接口，让图具有清晰的边界，并实现内部逻辑的封装。\n",
    "\n",
    "此外，图还支持使用未在输入/输出中声明的字段作为私有状态通道，用于节点间传递中间信息。这种机制适合保留调试信息、缓存等不希望暴露给外部调用者的数据。\n",
    "\n",
    "##### **核心概念**\n",
    "| 概念                | 说明                                             |\n",
    "| ----------------- | ---------------------------------------------- |\n",
    "| `input=`          | 指定图的公共输入结构（例如 TypedDict），控制外部调用时必须提供的键         |\n",
    "| `output=`         | 指定图的公共输出结构，控制最终 `graph.invoke()` 返回的键          |\n",
    "| `内部状态 (私有)`       | 定义在完整状态结构中、但未出现在 input/output 中的字段，供节点内部使用     |\n",
    "| `StateGraph(...)` | 需显式传入 OverallGraphState，同时指定 input 和 output 类型 |\n",
    "\n",
    "##### **示例代码：封装图逻辑 + 使用私有通道**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **1. 定义输入、输出、内部状态结构**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphInputSchema(TypedDict):\n",
    "    raw_text: str  # 来自外部用户\n",
    "\n",
    "class GraphOutputSchema(TypedDict):\n",
    "    summary: str   # 最终输出\n",
    "    processed_text: str  # 中间结果也可选择性暴露\n",
    "\n",
    "class OverallGraphState(TypedDict):\n",
    "    raw_text: str\n",
    "    processed_text: str\n",
    "    summary: str\n",
    "    internal_metadata: dict  # 私有字段，不暴露给外部\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **2. 定义处理节点**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processor_node(state: GraphInputSchema) -> dict:\n",
    "    print(f\"📄 [text_processor_node]：处理 '{state['raw_text']}'\")\n",
    "    processed = state['raw_text'].strip().capitalize()\n",
    "    return {\n",
    "        \"processed_text\": processed,\n",
    "        \"internal_metadata\": {\"length\": len(processed)}\n",
    "    }\n",
    "\n",
    "def summarizer_node(state: OverallGraphState) -> dict:\n",
    "    print(f\"📝 [summarizer_node]：为 '{state['processed_text']}' 生成摘要\")\n",
    "    return {\"summary\": f\"摘要内容: {state['processed_text'][:10]}...\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **3. 构建并编译 StateGraph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(\n",
    "    OverallGraphState,\n",
    "    input=GraphInputSchema,\n",
    "    output=GraphOutputSchema\n",
    ")\n",
    "\n",
    "builder.add_node(\"processor\", text_processor_node)\n",
    "builder.add_node(\"summarizer\", summarizer_node)\n",
    "builder.add_edge(START, \"processor\")\n",
    "builder.add_edge(\"processor\", \"summarizer\")\n",
    "builder.add_edge(\"summarizer\", END)\n",
    "\n",
    "compiled_graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **4. 调用图并输出结果**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_input = {\"raw_text\": \"   这是一个用于演示的文本。   \"}\n",
    "result = compiled_graph.invoke(graph_input)\n",
    "\n",
    "print(\"\\n✅ 最终输出 (符合 GraphOutputSchema):\")\n",
    "print(result)\n",
    "\n",
    "# {'summary': '摘要内容: 这是一个用于演示的文...', 'processed_text': '这是一个用于演示的文本。'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`internal_metadata` 字段不会出现在输出中，尽管它在图中流转，因为它不属于输出模式 GraphOutputSchema。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复杂的控制流\n",
    "\n",
    "LangGraph 不仅支持线性的节点序列，还通过其边（Edges）定义和特殊的 API（如 Send 和 Command）支持高度复杂的控制流，包括动态分支、并行处理和跨层级导航。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph Send API：动态 Map-Reduce 与并行任务分发\n",
    "\n",
    "LangGraph 中的 Send API 允许在运行时动态分发任务到多个目标节点，实现“条件并行执行”与“可编程 Map-Reduce 模式”。\n",
    "\n",
    "\n",
    "- Send API：在图中，某个节点（通常是 conditional edge）可以返回多个 Send 对象，用于将不同的数据并行发往多个目标节点。\n",
    "\n",
    "- 动态分支：每个 Send 可以包含不同的状态子集，实现“为每项数据生成独立执行路径”。\n",
    "\n",
    "- Reducer 合并：通过 `Annotated[... , reducer]` 指定如何合并每个并行任务的返回结果（如列表追加、字典合并等）。\n",
    "\n",
    "- Map-Reduce 模式：Map 阶段通过 Send 并行调用多个节点处理子任务，Reduce 阶段聚合所有结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ** 示例：并行生成“笑话”，并选出最佳笑话**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import Annotated, List, TypedDict\n",
    "from langgraph.types import Send\n",
    "from langgraph.graph import END, START, StateGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **定义状态结构**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TypedDict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 整体图的状态\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mOverallJokeState\u001b[39;00m(TypedDict):\n\u001b[0;32m      3\u001b[0m     topic: \u001b[38;5;28mstr\u001b[39m                       \u001b[38;5;66;03m# 用户输入的主题\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     subjects: List[\u001b[38;5;28mstr\u001b[39m]             \u001b[38;5;66;03m# 中间生成的子主题\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TypedDict' is not defined"
     ]
    }
   ],
   "source": [
    "# 整体图的状态\n",
    "class OverallJokeState(TypedDict):\n",
    "    topic: str                       # 用户输入的主题\n",
    "    subjects: List[str]             # 中间生成的子主题\n",
    "    jokes: Annotated[List[str], operator.add]  # 用于聚合每个笑话 (Reducer: operator.add)\n",
    "    best_joke: str | None           # 最后选出的最佳笑话\n",
    "\n",
    "# 用于并行节点的简化状态\n",
    "class SingleJokeState(TypedDict):\n",
    "    subject: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **节点定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1] 生成子主题（Map前置）\n",
    "def generate_subjects_node(state: OverallJokeState) -> dict:\n",
    "    print(f\"📌 [generate_subjects_node]：生成与 '{state['topic']}' 相关的主题...\")\n",
    "    subjects_map = {\"动物\": [\"猫\", \"狗\", \"大象\"], \"食物\": [\"披萨\", \"汉堡\"]}\n",
    "    return {\"subjects\": subjects_map.get(state[\"topic\"], [\"通用主题\"])}\n",
    "\n",
    "# [2] 每个主题生成一个笑话（Map 节点）\n",
    "def generate_joke_node(state: SingleJokeState) -> dict:\n",
    "    print(f\"🎭 [generate_joke_node]：为 '{state['subject']}' 生成笑话...\")\n",
    "    return {\"jokes\": [f\"这是一个关于『{state['subject']}』的笑话！\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **条件边：动态分发任务（Send API）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_subjects_to_joke_generators(state: OverallJokeState) -> List[Send]:\n",
    "    print(f\"🧭 [map_subjects_to_joke_generators]：分发 {len(state['subjects'])} 个任务...\")\n",
    "    return [Send(\"generate_joke_node\", {\"subject\": subj}) for subj in state[\"subjects\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **Reduce 节点：选择最佳笑话**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_joke_node(state: OverallJokeState) -> dict:\n",
    "    print(f\"🏆 [select_best_joke_node]：从 {len(state['jokes'])} 个笑话中选出一个...\")\n",
    "    best = state['jokes'][0] if state[\"jokes\"] else \"没有笑话\"\n",
    "    return {\"best_joke\": best}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### **构建 & 执行图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(OverallJokeState)\n",
    "\n",
    "# 添加节点\n",
    "builder.add_node(\"generate_subjects_node\", generate_subjects_node)\n",
    "builder.add_node(\"generate_joke_node\", generate_joke_node)\n",
    "builder.add_node(\"select_best_joke_node\", select_best_joke_node)\n",
    "\n",
    "# 添加图结构\n",
    "builder.add_edge(START, \"generate_subjects_node\")\n",
    "builder.add_conditional_edges(\"generate_subjects_node\", map_subjects_to_joke_generators)\n",
    "builder.add_edge(\"generate_joke_node\", \"select_best_joke_node\")\n",
    "builder.add_edge(\"select_best_joke_node\", END)\n",
    "\n",
    "compiled_graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {\"topic\": \"动物\", \"best_joke\": None}\n",
    "\n",
    "print(\"\\n🚀 开始执行图\")\n",
    "for event in compiled_graph.stream(initial_state):\n",
    "    print(f\"事件: {event}\")\n",
    "    print(\"------\")\n",
    "\n",
    "result = compiled_graph.invoke(initial_state)\n",
    "print(f\"\\n✅ 最终输出: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **运行流程图（简化）**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    START --> generate_subjects\n",
    "\n",
    "    generate_subjects --> joke_cat[generate_joke_node: 猫]\n",
    "    generate_subjects --> joke_dog[generate_joke_node: 狗]\n",
    "    generate_subjects --> joke_elephant[generate_joke_node: 大象]\n",
    "\n",
    "    joke_cat --> select_best\n",
    "    joke_dog --> select_best\n",
    "    joke_elephant --> select_best\n",
    "\n",
    "    select_best --> END\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 Command API：节点内同时更新状态与控制流程跳转\n",
    "\n",
    "\n",
    "在 LangGraph 中，除了返回状态字典，节点还可以返回一个 Command 对象，从而实现：\n",
    "- 状态更新（update 字段）\n",
    "- 跳转逻辑（goto 字段，指定下一个节点）\n",
    "\n",
    "这比传统的条件边（add_conditional_edges）更简洁，逻辑更聚合，特别适用于「节点内可直接决定下一步跳转」的场景。\n",
    "\n",
    "\n",
    "Command API 允许节点函数在一次返回中同时完成状态更新和指定下一个执行节点，从而将状态修改与控制流决策合并 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **状态结构定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "\n",
    "class CommandDemoState(TypedDict):\n",
    "    current_node_name: str\n",
    "    value: int\n",
    "    path_taken: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **节点定义**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **节点 A：执行计算 + 决定跳转目标**\n",
    "\n",
    "使用 Command 并指定 goto 时，节点函数的返回类型注解应指明所有可能的目标节点名称（例如 `Command[Literal[\"node_X\", \"node_Y\"]]`），以便正确渲染图 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "\n",
    "def node_a_with_command(state: CommandDemoState) -> Command[Literal[\"node_b_cmd\", \"node_c_cmd\"]]:\n",
    "    print(\"节点 [A] 执行中...\")\n",
    "    new_value = state.get(\"value\", 0) + 10\n",
    "\n",
    "    if random.random() < 0.5:\n",
    "        next_node = \"node_b_cmd\"\n",
    "        path = \"A -> B\"\n",
    "    else:\n",
    "        next_node = \"node_c_cmd\"\n",
    "        path = \"A -> C\"\n",
    "\n",
    "    return Command(\n",
    "        update={\"value\": new_value, \"current_node_name\": \"A\", \"path_taken\": path},\n",
    "        goto=next_node\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **节点 B：乘法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_b_cmd(state: CommandDemoState) -> dict:\n",
    "    print(\"节点 [B] 执行中...\")\n",
    "    return {\"current_node_name\": \"B\", \"value\": state[\"value\"] * 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### **节点 C：加法**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_c_cmd(state: CommandDemoState) -> dict:\n",
    "    print(\"节点 [C] 执行中...\")\n",
    "    return {\"current_node_name\": \"C\", \"value\": state[\"value\"] + 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **构建并编译图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "builder = StateGraph(CommandDemoState)\n",
    "builder.add_node(\"node_a_cmd\", node_a_with_command)\n",
    "builder.add_node(\"node_b_cmd\", node_b_cmd)\n",
    "builder.add_node(\"node_c_cmd\", node_c_cmd)\n",
    "\n",
    "builder.add_edge(START, \"node_a_cmd\")\n",
    "builder.add_edge(\"node_b_cmd\", END)\n",
    "builder.add_edge(\"node_c_cmd\", END)\n",
    "\n",
    "compiled_command_graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **执行图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 执行 Command API 示例图 ---\")\n",
    "initial_state = {\"value\": 0}\n",
    "\n",
    "final_state = compiled_command_graph.invoke(initial_state)\n",
    "print(f\"\\n最终状态: {final_state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **总结**\n",
    "\n",
    "| 特性               | 说明                   |\n",
    "| ---------------- | -------------------- |\n",
    "| `Command.update` | 与普通节点返回 dict 一样，更新状态 |\n",
    "| `Command.goto`   | 直接指定跳转到哪个节点，避免条件边    |\n",
    "| 用途推荐             | 流程明确、跳转逻辑在当前节点可定时    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 Command.PARENT：子图向父图汇报并跳转控制流\n",
    "\n",
    "在使用 **嵌套子图（Subgraph）** 构建 LangGraph 层级代理系统时，子图内部的某些节点可能希望：\n",
    "\n",
    "- ✅ 将处理结果返回给父图；\n",
    "\n",
    "- ✅ 控制父图的下一个跳转节点；\n",
    "\n",
    "- ✅ 同时更新父图状态。\n",
    "\n",
    "这时可使用 `Command(graph=Command.PARENT)`，它允许你从**子图内部**：\n",
    "\n",
    "- 指定要跳转到的**父图节点名称**；\n",
    "\n",
    "- 提供要**更新到父图的状态字典**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **状态结构定义**\n",
    "\n",
    "- **父图状态**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Any\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "class ParentGraphState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    task_status: str\n",
    "    worker_result: Any | None\n",
    "    # 在父图状态中明确定义 input_data\n",
    "    input_data: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **子图状态**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WorkerAgentState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    input_data: str\n",
    "    processed_data: str | None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **子图构建**\n",
    "- **节点1：处理数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_process_data_node(state: WorkerAgentState) -> dict:\n",
    "    \"\"\"普通的处理节点，只更新自己的状态\"\"\"\n",
    "    print(f\"--- 工作者：收到输入 '{state['input_data']}'，正在处理... ---\")\n",
    "    processed = f\"处理完成：{state['input_data'].upper()}\"\n",
    "    return {\"processed_data\": processed}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **节点2：汇报父图 + 跳转**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "from typing import Literal\n",
    "\n",
    "def worker_report_to_parent_node(state: WorkerAgentState) -> Command:\n",
    "    \"\"\"\n",
    "    这个节点是关键。它不返回字典，而是返回一个 Command 对象，\n",
    "    直接命令父图更新状态并跳转。\n",
    "    \"\"\"\n",
    "    print(\"--- 工作者：准备上报给父图 ---\")\n",
    "    result = state.get(\"processed_data\", \"无结果\")\n",
    "\n",
    "    return Command(\n",
    "        # `goto` 告诉父图，下一步应该跳转到名为 'supervisor_node' 的节点\n",
    "        goto=\"supervisor_node\",\n",
    "        # `update` 包含一个字典，用于更新 PARENT 的状态\n",
    "        update={\n",
    "            \"worker_result\": result,\n",
    "            \"task_status\": \"待监督者审核\",\n",
    "            \"messages\": [AIMessage(content=f\"工作者通过 Command 上报，请求审核。结果: {result}\")]\n",
    "        },\n",
    "        # `graph=Command.PARENT` 指明这个命令是发给父图的\n",
    "        graph=Command.PARENT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **构建子图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "worker_builder = StateGraph(WorkerAgentState)\n",
    "worker_builder.add_node(\"process_data\", worker_process_data_node)\n",
    "worker_builder.add_node(\"report_to_parent\", worker_report_to_parent_node)\n",
    "worker_builder.add_edge(START, \"process_data\")\n",
    "worker_builder.add_edge(\"process_data\", \"report_to_parent\")\n",
    "compiled_worker_agent = worker_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **父图构建**\n",
    "- **父图节点：prepare_worker_input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_worker_input(parent_state: ParentGraphState) -> WorkerAgentState:\n",
    "    \"\"\"从父状态中挑选出子图需要的字段，构建子图的输入。\"\"\"\n",
    "    return {\n",
    "        \"input_data\": parent_state[\"input_data\"],\n",
    "        \"messages\": parent_state[\"messages\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **父图节点：supervisor_node**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_node(state: ParentGraphState) -> dict:\n",
    "    print(\"--- 监督者：收到任务，正在审核... ---\")\n",
    "    return {\n",
    "        \"task_status\": \"已完成\",\n",
    "        \"messages\": [AIMessage(content=f\"监督者审核通过，结果: {state['worker_result']}\")]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **构建父图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_builder = StateGraph(ParentGraphState)\n",
    "\n",
    "# 在起始节点就提供所有数据\n",
    "parent_builder.add_node(\"start_task\", lambda state: {\n",
    "    \"task_status\": \"处理中\",\n",
    "    \"input_data\": \"来自父图的原始任务\" # 直接在这里提供数据\n",
    "})\n",
    "# 使用 `mapper | subgraph` 的模式添加子图\n",
    "worker_with_mapper = prepare_worker_input | compiled_worker_agent\n",
    "parent_builder.add_node(\"worker_agent\", worker_with_mapper)\n",
    "parent_builder.add_node(\"supervisor_node\", supervisor_node)\n",
    "\n",
    "parent_builder.set_entry_point(\"start_task\")\n",
    "parent_builder.add_edge(\"start_task\", \"worker_agent\")\n",
    "parent_builder.add_edge(\"supervisor_node\", END)\n",
    "# `worker_agent` 之后依然没有边，等待 Command 指令\n",
    "\n",
    "# --- 编译和运行 ---\n",
    "print(\"--- 编译父图 ---\")\n",
    "compiled_parent_graph = parent_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **执行图**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- 开始执行父图 ---\")\n",
    "# 初始调用时，只需提供最基本的信息\n",
    "initial_state = {\"messages\": [HumanMessage(content=\"顶级任务开始\")]}\n",
    "final_state = compiled_parent_graph.invoke(initial_state)\n",
    "\n",
    "print(f\"\\n✅ 最终父图状态:\\n{final_state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command.PARENT 是实现层级式多 Agent 系统或复杂模块化图控制流的主要机制。\n",
    "\n",
    "### 高级持久化机制\n",
    "Checkpointer 是 LangGraph 用于记录和恢复图执行状态的机制，常用于：\n",
    "\n",
    "- 多轮执行状态追踪\n",
    "\n",
    "- 容错恢复\n",
    "\n",
    "- 会话持久化\n",
    "\n",
    "所有 Checkpointer 均继承自 BaseCheckpointSaver，支持同步与异步接口。\n",
    "####  Checkpointer 接口与实现详解\n",
    "\n",
    "##### `BaseCheckpointSaver` 接口与自定义实现要点\n",
    "\n",
    "`BaseCheckpointSaver` 是所有 Checkpointer 实现的基类。自定义实现时需关注以下几个核心点：\n",
    "\n",
    "- **必要方法**：\n",
    "  - `get_tuple(run_id: str, name: str) -> tuple[state, config]`\n",
    "  - `put(run_id: str, name: str, config: Any, state: Any) -> None`\n",
    "  - `list(run_id: str) -> List[str]`\n",
    "  - 以及它们的异步版本：\n",
    "    - `aget_tuple(...)`\n",
    "    - `aput(...)`\n",
    "    - `alist(...)`\n",
    "\n",
    "- **serde 序列化**：\n",
    "  - Checkpointer 通常依赖一个 `SerializerProtocol` 实现对状态的序列化与反序列化。\n",
    "  - 例如，常用的 `JsonPlusSerializer` 可以处理标准 Python 对象和一些 LangChain 扩展类型。\n",
    "\n",
    "- **异步优先**：\n",
    "  - 推荐实现异步接口 (`aget_tuple`, `aput`, `alist`)，以避免在高并发或 I/O 密集场景中阻塞主线程。\n",
    "\n",
    "---\n",
    "\n",
    "#### 开箱即用的 Checkpointer 实现\n",
    "\n",
    "| 名称                 | 类型 | 场景          |\n",
    "| ------------------ | -- | ----------- |\n",
    "| `InMemorySaver`    | 内存 | 测试或临时执行     |\n",
    "| `SqliteSaver`      | 同步 | 轻量持久化，单线程场景 |\n",
    "| `AsyncSqliteSaver` | 异步 | 协程环境下使用     |\n",
    "| `PostgresSaver`    | 异步 | 生产级多线程场景    |\n",
    "\n",
    "\n",
    "##### **示例：设置 SqliteSaver 和 AsyncSqliteSaver**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langgraph-checkpoint-sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import asyncio\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "\n",
    "\n",
    "# ✅ 设置同步 SqliteSaver\n",
    "sqlite_conn_sync = sqlite3.connect(\":memory:\", check_same_thread=False)\n",
    "sync_sqlite_saver = SqliteSaver(conn=sqlite_conn_sync)\n",
    "# sync_sqlite_saver.setup()  # 如果使用的是文件数据库，首次使用需建表\n",
    "\n",
    "# ✅ 设置异步 AsyncSqliteSaver（推荐异步环境中使用）\n",
    "# import aiosqlite\n",
    "# async def setup_async_sqlite_saver():\n",
    "#     async_sqlite_saver = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "#     await async_sqlite_saver.setup()\n",
    "#     return async_sqlite_saver\n",
    "# async_saver = asyncio.run(setup_async_sqlite_saver())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 搭配 StateGraph 使用（以 SqliteSaver 为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Dict\n",
    "import uuid\n",
    "\n",
    "# 定义状态结构\n",
    "class SimpleState(TypedDict):\n",
    "    count: int\n",
    "\n",
    "# 定义节点函数\n",
    "def increment_node(state: SimpleState) -> Dict[str, int]:\n",
    "    return {\"count\": state[\"count\"] + 1}\n",
    "\n",
    "# 构建图\n",
    "builder = StateGraph(SimpleState)\n",
    "builder.add_node(\"inc\", increment_node)\n",
    "builder.set_entry_point(\"inc\")\n",
    "builder.add_edge(\"inc\", END)\n",
    "\n",
    "# 编译图，绑定持久化\n",
    "graph = builder.compile(checkpointer=sync_sqlite_saver)\n",
    "\n",
    "# 配置唯一线程ID，用于追踪每次执行路径\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# 执行前查看状态\n",
    "print(\"初始状态：\", graph.get_state(config))\n",
    "\n",
    "# 第一次执行\n",
    "graph.invoke({\"count\": 0}, config=config)\n",
    "sqlite_conn_sync.commit()\n",
    "print(\"执行1次后：\", graph.get_state(config).values)\n",
    "\n",
    "# 再次执行（读取上次状态）\n",
    "graph.invoke({\"count\": graph.get_state(config).values[\"count\"]}, config=config)\n",
    "print(\"执行2次后：\", graph.get_state(config).values)\n",
    "sqlite_conn_sync.commit()\n",
    "\n",
    "\n",
    "sqlite_conn_sync.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PostgreSQL 配置参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langgraph.checkpoint.postgres import PostgresSaver\n",
    "\n",
    "# DB_URI = \"postgresql://user:password@localhost:5432/mydb\"\n",
    "\n",
    "# try:\n",
    "#     saver = PostgresSaver.from_conn_string(DB_URI)\n",
    "#     # saver.setup()  # 首次建表\n",
    "#     print(\"PostgresSaver 已连接\")\n",
    "# except Exception as e:\n",
    "#     print(\"连接失败，请检查配置和数据库服务状态\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级 Checkpoint 管理：历史回溯、状态更新与分支探索\n",
    "\n",
    "LangGraph 的 Checkpointer 系统支持图执行的“时间旅行”特性，包括回溯历史状态、基于历史状态更新并继续执行、从任意检查点进行分支等操作：\n",
    "\n",
    "##### ** 常用方法**\n",
    "\n",
    "- `graph.get_state_history(config)`  \n",
    "  获取指定线程的 **执行历史**，按时间倒序返回一系列 `StateSnapshot`。\n",
    "\n",
    "- `graph.get_state(config)`  \n",
    "  获取线程的当前状态，或通过 `checkpoint_id` 获取特定检查点状态。\n",
    "\n",
    "- `graph.update_state(config, values, as_node=None)`  \n",
    "  从某个检查点分叉出新的状态，`values` 被合并进入状态。可选的 `as_node` 控制从哪个节点继续。\n",
    "\n",
    "- `graph.invoke(None, config=config_with_checkpoint_id)`  \n",
    "  重播执行历史，等价于“从某个历史检查点重新分支”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **示例：时间旅行 Time Travel**\n",
    "\n",
    "以下例子构建一个计数器图，使用 `InMemorySaver` 存储状态。执行路径如下：\n",
    "\n",
    "1. 连续运行多次，生成历史检查点。\n",
    "2. 获取历史列表并打印。\n",
    "3. 从历史中一个检查点 `count=1` 分支出新路径（重播）。\n",
    "4. 修改某个检查点的状态值并执行新的分支。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, List, Literal, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import operator, uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CounterState(TypedDict):\n",
    "    count: int\n",
    "    log: Annotated[List[str], operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计数节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_node(state: CounterState) -> Dict[str, Any]:\n",
    "    new_count = state[\"count\"] + 1\n",
    "    return {\"count\": new_count, \"log\": [f\"Count is now {new_count}\"]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 条件跳转"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_counter(state: CounterState) -> Literal[\"counter\", END]:\n",
    "    return END if state[\"count\"] >= 3 else \"counter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder_tt = StateGraph(CounterState)\n",
    "builder_tt.add_node(\"counter\", counter_node)\n",
    "builder_tt.set_entry_point(\"counter\")\n",
    "builder_tt.add_conditional_edges(\"counter\", should_continue_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用 InMemorySaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_tt = InMemorySaver()\n",
    "graph_tt = builder_tt.compile(checkpointer=memory_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 配置线程 ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread_id_tt = str(uuid.uuid4())\n",
    "config_tt = {\"configurable\": {\"thread_id\": thread_id_tt}}\n",
    "\n",
    "print(\"\\n--- 初次运行 ---\")\n",
    "for step in graph_tt.stream({\"count\": 0, \"log\": [\"Initial state\"]}, config_tt):\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查看历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = list(graph_tt.get_state_history(config_tt))\n",
    "print(\"\\n--- 历史状态快照（最新在前）---\")\n",
    "for i, snapshot in enumerate(history):\n",
    "    print(f\"Snapshot {i}: next={snapshot.next}, values={snapshot.values}, checkpoint_id={snapshot.config['configurable']['checkpoint_id']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 选取某个检查点重播/修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(history) > 2:\n",
    "    checkpoint_to_replay_from = history[1]  # count=1 的检查点\n",
    "    print(\"\\n准备从以下检查点重播:\")\n",
    "    print(checkpoint_to_replay_from.values)\n",
    "\n",
    "    # ✅ 方式 1：重播\n",
    "    config_replay = {\n",
    "        \"configurable\": {\n",
    "            \"thread_id\": thread_id_tt,\n",
    "            \"checkpoint_id\": checkpoint_to_replay_from.config[\"configurable\"][\"checkpoint_id\"]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- 从 checkpoint_id 重播 ---\")\n",
    "    for step in graph_tt.stream(None, config_replay):\n",
    "        print(step)\n",
    "\n",
    "    # ✅ 方式 2：修改状态后分支\n",
    "    config_updated_state = graph_tt.update_state(\n",
    "        checkpoint_to_replay_from.config,\n",
    "        {\"count\": 5, \"log\": [\"Manually updated count to 5 from 1\"]},\n",
    "        as_node=\"counter\"\n",
    "    )\n",
    "\n",
    "    print(\"当前状态：\", graph_tt.get_state(config_updated_state).values)\n",
    "    print(\"next 节点：\", graph_tt.get_state(config_updated_state).next)\n",
    "\n",
    "    print(\"\\n--- 从修改后的状态分支 ---\")\n",
    "    for step in graph_tt.stream(None, config_updated_state):\n",
    "        print(step)\n",
    "else:\n",
    "    print(\"\\n历史不足，无法进行回溯或修改演示。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MemoryStore 与语义搜索：构建长时记忆智能体\n",
    "\n",
    "MemoryStore（如 InMemoryStore、PostgresStore）提供了一种机制，用于跨线程（例如同一用户不同对话）共享和存储结构化记忆。配合嵌入模型，支持自然语言语义搜索，使智能体拥有类似“长期记忆”的能力。\n",
    "\n",
    "\n",
    "##### 核心能力\n",
    "\n",
    "- ✅ 命名空间隔离：支持通过 tuple(namespace, key) 来组织和隔离不同用户或任务的记忆。\n",
    "- ✅ 语义搜索：支持基于自然语言的问题进行模糊/语义化搜索，依赖嵌入模型（如 OpenAIEmbeddings）。\n",
    "- ✅ 与 Checkpointer 协同：Checkpointer 记录状态，MemoryStore 记录知识或记忆，两者结合可实现强上下文感知。\n",
    "\n",
    "\n",
    "##### 示例代码：使用 InMemoryStore 实现语义化记忆搜索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_core.embeddings import Embeddings\n",
    "from langgraph.store.memory import InMemoryStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可选：无网络环境下使用的假 Embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeEmbeddings(Embeddings):\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        return [[0.1] * 10 for _ in texts]  # 固定维度\n",
    "\n",
    "    def embed_query(self, text: str) -> List[float]:\n",
    "        return [0.1] * 10\n",
    "\n",
    "\n",
    "# 初始化嵌入模型\n",
    "try:\n",
    "    # 如果你有 OpenAI key，可使用真实模型\n",
    "    # embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    # dims = 1536\n",
    "    raise Exception(\"跳过真实模型，用 FakeEmbeddings 示例\")\n",
    "except Exception as e:\n",
    "    print(f\"使用 FakeEmbeddings（原因：{e}）\")\n",
    "    embeddings_model = FakeEmbeddings()\n",
    "    dims = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 创建内存存储，启用语义索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_memory_store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings_model,\n",
    "        \"dims\": dims,\n",
    "        \"fields\": [\"text_content\", \"summary\"]  # 需要嵌入的字段\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 准备用户记忆（模拟数据）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"user_001\"\n",
    "memory_namespace = (\"user_conversations\", user_id)\n",
    "memories_to_add = [\n",
    "    {\n",
    "        \"key\": \"project_1\",\n",
    "        \"value\": {\n",
    "            \"text_content\": \"User built a web application using Flask.\",\n",
    "            \"summary\": \"Flask web app project\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"project_2\",\n",
    "        \"value\": {\n",
    "            \"text_content\": \"User is learning React for frontend development.\",\n",
    "            \"summary\": \"React learning\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"key\": \"project_3\",\n",
    "        \"value\": {\n",
    "            \"text_content\": \"They deployed the project using Docker and Heroku.\",\n",
    "            \"summary\": \"Deployment details\"\n",
    "        }\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 存储这些记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mem in memories_to_add:\n",
    "    semantic_memory_store.put(memory_namespace, mem[\"key\"], mem[\"value\"])\n",
    "\n",
    "print(f\"\\n--- 已为用户 {user_id} 添加 {len(memories_to_add)} 条记忆 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 语义搜索：寻找相关项目"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"user's software development projects\"\n",
    "print(f\"\\n🔍 正在搜索与：'{query}' 相关的记忆项...\")\n",
    "results = list(semantic_memory_store.search(memory_namespace, query=query, limit=2))\n",
    "\n",
    "if results:\n",
    "    for r in results:\n",
    "        print(f\"✅ 匹配：key={r.key}, score={r.score:.4f}, 内容={r.value}\")\n",
    "else:\n",
    "    print(\"未找到相关记忆。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 复杂人机交互 (HIL) 模式\n",
    "\n",
    "LangGraph 的 HIL（Human-in-the-Loop）机制允许在图执行过程中主动暂停执行，等待人工反馈，再根据反馈动态恢复并继续执行。这对于多阶段审批、结构化修改、合规流程等场景非常关键。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心 API 说明\n",
    "| 功能                    | 描述                                     |\n",
    "| --------------------- | -------------------------------------- |\n",
    "| `interrupt(payload)`  | 在图中断执行，返回控制权等待人类输入。`payload` 是给人的提示信息。 |\n",
    "| `Command(resume=...)` | 人类输入结构化数据后，通过 `resume` 恢复执行。           |\n",
    "| `goto=\"...\"`          | 人类反馈后，根据内容跳转至下一个节点。                    |\n",
    "\n",
    "#### 示例结构设计\n",
    "\n",
    "- 起点：生成初稿（generate_draft）\n",
    "\n",
    "- 审核：人类介入（human_review）\n",
    "\n",
    "- 分支：\n",
    "\n",
    "    - 修改草稿（revise_draft）\n",
    "    \n",
    "    - 发布草稿（publish）\n",
    "    \n",
    "    - 丢弃草稿（discard）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 状态结构定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List, Dict, Literal\n",
    "\n",
    "from langchain_core.messages import SystemMessage, AIMessage, HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.types import Command, interrupt\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "# ✅ 人类审阅反馈的结构定义（结构化人工输入）\n",
    "class ReviewFeedback(BaseModel):\n",
    "    approved: bool  # 是否批准当前草稿（True 表示通过，False 表示需要修改或拒绝）\n",
    "    comments: Optional[str] = None  # 审核意见，可选，供人类解释为什么批准/拒绝\n",
    "    suggested_edits: Optional[Dict[str, str]] = None  # 结构化修改建议，例如 {\"section_2\": \"请补充伦理分析\"}\n",
    "\n",
    "# ✅ 多阶段人工审阅流程中状态的结构定义\n",
    "class MultiStageHILState(BaseModel):\n",
    "    messages: Annotated[List, add_messages]  \n",
    "    # 聊天消息历史，包含人类提问、AI 回答、系统提示等。由 LangGraph 管理（add_messages 是 reducer）\n",
    "\n",
    "    draft_content: Optional[str] = None  \n",
    "    # 当前草稿的内容，可能是 AI 生成的初稿或经过修改的版本\n",
    "\n",
    "    review_stage: int = 0  \n",
    "    # 当前审阅阶段（0=未审，1=初审，2=终审等）。每通过一次审核递增\n",
    "\n",
    "    feedback_history: List[ReviewFeedback] = []  \n",
    "    # 审核历史，记录每一轮人类提供的反馈，用于溯源和动态决策\n",
    "\n",
    "    is_finally_approved: bool = False  \n",
    "    # 标志位：草稿是否通过最终审核。流程结束时用于判断是否发布/丢弃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 节点定义\n",
    "- 初稿生成节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_draft_node(state: MultiStageHILState) -> dict:\n",
    "    # 如果有消息历史，则使用最后一条消息的内容作为生成草稿的参考；否则使用默认提示\n",
    "    draft = f\"Initial draft based on: {state.messages[-1].content if state.messages else 'initial prompt'}.\"\n",
    "\n",
    "    # 返回状态更新：\n",
    "    return {\n",
    "        \"draft_content\": draft,  # 存入生成的草稿\n",
    "        \"review_stage\": 1,       # 设置审阅阶段为 1（初审阶段）\n",
    "        \"messages\": [            # 向消息列表添加 AI 回复：草稿内容\n",
    "            AIMessage(content=f\"草稿生成：{draft}\")\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 人类审核节点（使用 interrupt）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def human_review_node(state: MultiStageHILState) -> Command[Literal[\"revise_draft_node\", \"publish_node\", \"discard_node\"]]:\n",
    "    payload = {\n",
    "        \"draft\": state.draft_content,  # 当前草稿\n",
    "        \"stage\": state.review_stage,   # 当前审阅阶段\n",
    "        \"previous_feedback\": [fb.dict() for fb in state.feedback_history],  # 所有历史反馈，序列化成字典形式\n",
    "    }\n",
    "\n",
    "\n",
    "    # 中断执行等待人类反馈\n",
    "    feedback_dict = interrupt(payload)\n",
    "\n",
    "    try:\n",
    "        feedback = ReviewFeedback(**feedback_dict)\n",
    "    except Exception as e:\n",
    "        return Command(\n",
    "            update={\"messages\": [SystemMessage(content=f\"反馈格式无效：{e}\")]},\n",
    "            goto=\"human_review_node\"\n",
    "        )\n",
    "\n",
    "    state.feedback_history.append(feedback)\n",
    "\n",
    "    if feedback.approved:\n",
    "        if state.review_stage >= 2:\n",
    "            return Command(\n",
    "                update={\"is_finally_approved\": True},\n",
    "                goto=\"publish_node\"\n",
    "            )\n",
    "        else:\n",
    "            updated_draft = state.draft_content\n",
    "            if feedback.suggested_edits:\n",
    "                updated_draft += f\" (根据建议修改: {feedback.suggested_edits})\"\n",
    "            return Command(\n",
    "                update={\n",
    "                    \"review_stage\": state.review_stage + 1,\n",
    "                    \"draft_content\": updated_draft\n",
    "                },\n",
    "                goto=\"human_review_node\"\n",
    "            )\n",
    "    else:\n",
    "        if state.review_stage >= 2:\n",
    "            return Command(goto=\"discard_node\")\n",
    "        return Command(\n",
    "            update={\"draft_content\": state.draft_content + f\" (需修改: {feedback.comments})\"},\n",
    "            goto=\"revise_draft_node\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 修改草稿节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def revise_draft_node(state: MultiStageHILState) -> dict:\n",
    "    last_comment = state.feedback_history[-1].comments if state.feedback_history else \"无\"\n",
    "    revised = f\"Revised based on comment: {last_comment}\"\n",
    "    return {\"draft_content\": revised, \"messages\": [AIMessage(content=revised)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  发布草稿节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_node(state: MultiStageHILState) -> dict:\n",
    "    return {\"messages\": [SystemMessage(content=f\"发布内容: {state.draft_content}\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 丢弃草稿节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_node(state: MultiStageHILState) -> dict:\n",
    "    return {\"messages\": [SystemMessage(content=\"草稿已丢弃\")]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 构建图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = StateGraph(MultiStageHILState)\n",
    "builder.set_entry_point(\"generate_draft\")\n",
    "builder.add_node(\"generate_draft\", generate_draft_node)\n",
    "builder.add_node(\"human_review_node\", human_review_node)\n",
    "builder.add_node(\"revise_draft_node\", revise_draft_node)\n",
    "builder.add_node(\"publish_node\", publish_node)\n",
    "builder.add_node(\"discard_node\", discard_node)\n",
    "\n",
    "builder.add_edge(\"generate_draft\", \"human_review_node\")\n",
    "builder.add_edge(\"revise_draft_node\", \"human_review_node\")\n",
    "builder.add_edge(\"publish_node\", END)\n",
    "builder.add_edge(\"discard_node\", END)\n",
    "\n",
    "graph = builder.compile(checkpointer=InMemorySaver())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 执行与模拟人工反馈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# 初始执行（生成草稿 + 中断）\n",
    "state = graph.invoke({\"messages\": [HumanMessage(content=\"写一篇关于 AI 伦理的文章\")]}, config)\n",
    "if \"__interrupt__\" in state:\n",
    "    print(\"🚨 中断，人工审核 1：\")\n",
    "    print(state[\"__interrupt__\"][0].value)\n",
    "    \n",
    "    # 人工反馈\n",
    "    feedback1 = ReviewFeedback(\n",
    "        approved=True,\n",
    "        comments=\"不错，但请补充关于偏见的问题\",\n",
    "        suggested_edits={\"section2\": \"加入 AI 偏见讨论\"}\n",
    "    )\n",
    "    state = graph.invoke(Command(resume=feedback1.dict()), config)\n",
    "\n",
    "# 第二次中断（复审）\n",
    "if \"__interrupt__\" in state:\n",
    "    print(\"🚨 中断，人工审核 2：\")\n",
    "    print(state[\"__interrupt__\"][0].value)\n",
    "    \n",
    "    feedback2 = ReviewFeedback(\n",
    "        approved=True,\n",
    "        comments=\"很好，可以发布\"\n",
    "    )\n",
    "    state = graph.invoke(Command(resume=feedback2.dict()), config)\n",
    "\n",
    "print(\"✅ 最终状态：\")\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应用场景举例\n",
    "\n",
    "- 内容生成：AI 撰写报告 → 人工审阅 → 修订 → 发布\n",
    "\n",
    "- 法律审核：AI 初步总结 → 法律人员修改 → 审批后签发\n",
    "\n",
    "- 合规流程：自动流程中间等待决策人批准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 子图 (Subgraphs) 的高级应用\n",
    "\n",
    "LangGraph 支持将一个编译好的图作为「节点」嵌入到另一个图中，即 **子图嵌套**，这允许我们：\n",
    "\n",
    "- 把复杂任务分成子流程并重用\n",
    "\n",
    "- 管理不同层级状态和流程逻辑\n",
    "\n",
    "- 让子图处理完后，将控制权和数据返回父图（支持跨图跳转）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 场景：父图调用子图完成一个研究任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. 父图状态：MainWorkflowState**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MainWorkflowState(TypedDict):\n",
    "    main_query: str  # 用户请求的问题\n",
    "    sub_analysis_report: Annotated[Optional[str], operator.add]  # 子图更新此报告\n",
    "    overall_status: str  # 当前进度状态\n",
    "    \n",
    "    research_depth: str\n",
    "    topic_for_research: str\n",
    "    intermediate_result: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. 子图状态：ResearchSubtaskState**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResearchSubtaskState(TypedDict):\n",
    "    topic_for_research: str\n",
    "    research_depth: int\n",
    "    sub_log: Annotated[List[str], operator.add]  # 子图内部日志\n",
    "\n",
    "    intermediate_result: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ** 3. 子图定义：研究任务（两步）**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def research_step_1(state: ResearchSubtaskState):\n",
    "    # 记录当前研究主题和深度的日志信息\n",
    "    log = f\"Sub: Researching '{state['topic_for_research']}' at depth {state['research_depth']}.\"\n",
    "    print(log)\n",
    "\n",
    "    # 返回更新日志（追加模式）和一个模拟的中间研究结果\n",
    "    return {\n",
    "        \"sub_log\": [log],  # 子图内部日志，支持多次追加\n",
    "        \"intermediate_result\": f\"Findings for {state['topic_for_research']}\"  # 子图内部状态，供后续节点使用\n",
    "    }\n",
    "\n",
    "\n",
    "def research_step_2_and_decide(state: ResearchSubtaskState):\n",
    "    # 生成针对当前研究主题的详细报告，包含前一步的中间结果\n",
    "    report = f\"Detailed Report on {state['topic_for_research']}: {state['intermediate_result']}\"\n",
    "\n",
    "    # 使用 Command.PARENT 将控制权交还给父图，\n",
    "    # 同时通过 update 参数更新父图状态中的 sub_analysis_report 字段\n",
    "    # 并指定跳转到父图中的 final_report_node 节点继续执行\n",
    "    return Command(\n",
    "        update={\"sub_analysis_report\": report},  # 更新父图的共享状态\n",
    "        goto=\"final_report_node\",  # 父图中的目标节点\n",
    "        graph=Command.PARENT  # 指示跳转回父图\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编译子图\n",
    "subgraph_research_builder = StateGraph(ResearchSubtaskState)\n",
    "subgraph_research_builder.add_node(\"res_step_1\", research_step_1)\n",
    "subgraph_research_builder.add_node(\"res_step_2_decide\", research_step_2_and_decide)\n",
    "subgraph_research_builder.add_edge(START, \"res_step_1\")\n",
    "subgraph_research_builder.add_edge(\"res_step_1\", \"res_step_2_decide\")\n",
    "compiled_subgraph = subgraph_research_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  **4. 父图构建与嵌入子图**\n",
    "- 父图节点：任务分派 → 运行子图 → 接收报告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_research_task_node(state: MainWorkflowState) -> dict:\n",
    "    # 打印当前父图状态中的主查询，用于调试或日志记录\n",
    "    print(f\"Parent: Assigning research for '{state['main_query']}'\")\n",
    "\n",
    "    # 返回一个新的状态更新字典，将任务分配给子图：\n",
    "    # - 更新 overall_status 表示研究任务已分配\n",
    "    # - 将父图的 main_query 传递给子图的 topic_for_research 作为研究主题\n",
    "    # - 设定研究深度 research_depth 为 2（示例参数）\n",
    "    # - 初始化子图内部日志 sub_log 为空列表\n",
    "    # 这些字段用于启动子图的研究流程\n",
    "    return {\n",
    "        \"overall_status\": \"Research_Assigned\",\n",
    "        \"topic_for_research\": state[\"main_query\"],\n",
    "        \"research_depth\": 2,\n",
    "        \"sub_log\": []\n",
    "    }\n",
    "\n",
    "def report_to_parent_node(state: MainWorkflowState) -> dict:\n",
    "    # 打印从子图收到的研究报告内容，用于日志或调试\n",
    "    print(f\"Parent: Received report: '{state['sub_analysis_report']}'\")\n",
    "    \n",
    "    # 返回状态更新，表示研究已完成，并将最终结果存储到父图状态中\n",
    "    return {\n",
    "        \"overall_status\": \"Research_Completed\",\n",
    "        \"final_outcome\": state[\"sub_analysis_report\"]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建父图并嵌入子图为节点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_builder = StateGraph(MainWorkflowState)\n",
    "parent_builder.add_node(\"assign_task\", assign_research_task_node)\n",
    "parent_builder.add_node(\"research_subgraph_node\", compiled_subgraph)  # 子图直接嵌入\n",
    "parent_builder.add_node(\"final_report_node\", report_to_parent_node)\n",
    "\n",
    "parent_builder.set_entry_point(\"assign_task\")\n",
    "parent_builder.add_edge(\"assign_task\", \"research_subgraph_node\")\n",
    "# 无需给子图配置出口边，因为 Command.PARENT 已跳到 final_report_node\n",
    "parent_builder.add_edge(\"final_report_node\", END)\n",
    "\n",
    "compiled_parent_graph = parent_builder.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 执行父图任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled_parent_graph.invoke(\n",
    "    {\n",
    "        \"main_query\": \"AI in healthcare\",\n",
    "        \"sub_analysis_report\": \"\",\n",
    "        \"overall_status\": \"Started\"\n",
    "    },\n",
    "    config={\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
    ")\n",
    "\n",
    "print(\"\\n🎯 最终父图结果：\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流式处理 (Streaming) 深入\n",
    "LangGraph 支持多种流式处理模式，允许在图执行过程中实时获取状态更新、消息、调试信息以及自定义事件。流式能力特别适合实时展示模型生成的结果、日志、状态变化等，帮助构建交互式和监控型系统。\n",
    "\n",
    "#### 常用流式模式\n",
    "\n",
    "- \"values\"：每个步骤后流式传输完整状态\n",
    "\n",
    "- \"updates\"：每个步骤后流式传输状态的更新部分\n",
    "\n",
    "- \"messages\"：逐个 token 流式传输 LLM 消息\n",
    "\n",
    "- \"custom\"：通过 StreamWriter 发射的自定义事件流\n",
    "\n",
    "- \"debug\"：尽可能详细的调试信息\n",
    "\n",
    "可以传入模式列表，例如 `stream_mode=[\"updates\", \"custom\"]`，每条流式输出是 (mode, chunk) 形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 子图流式传输\n",
    "父图调用子图时，可以在 .stream() 方法里传入 subgraphs=True，实时获取父子图所有流事件。返回的流事件带有 namespace 标识，表示事件来源子图路径，方便区分。\n",
    "\n",
    "####  代码示例：自定义流 + 子图流式输出\n",
    "\n",
    "下面示范一个子图如何使用 `StreamWriter` 发送自定义流事件，父图如何开启对子图流事件的监听并实时打印。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, TypedDict\n",
    "\n",
    "from langgraph.config import get_stream_writer\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "\n",
    "# --- 子图状态定义 ---\n",
    "class SubStreamState(TypedDict):\n",
    "    sub_input: str\n",
    "    sub_output: Optional[str]\n",
    "\n",
    "\n",
    "# --- 子图节点，使用 StreamWriter 发射自定义流式数据 ---\n",
    "def sub_stream_node(state: SubStreamState):\n",
    "    writer = get_stream_writer()  # 获取流写入器\n",
    "\n",
    "    # 模拟分多步流式发射数据\n",
    "    for i in range(3):\n",
    "        chunk = f\"子图输出块 #{i + 1}，基于输入 '{state['sub_input']}'\"\n",
    "        writer(chunk)  # 发射到 \"custom\" 流\n",
    "        print(f\"[子图] 流式发送: {chunk}\")\n",
    "\n",
    "    # 结束时返回结果更新状态\n",
    "    return {\"sub_output\": f\"处理完成: {state['sub_input']}\"}\n",
    "\n",
    "\n",
    "# --- 子图构建 ---\n",
    "subgraph = StateGraph(SubStreamState)\n",
    "subgraph.add_node(\"sub_stream_node\", sub_stream_node)\n",
    "subgraph.set_entry_point(\"sub_stream_node\")\n",
    "compiled_subgraph = subgraph.compile()\n",
    "\n",
    "\n",
    "# --- 父图状态定义 ---\n",
    "class ParentState(TypedDict):\n",
    "    main_input: str\n",
    "    sub_input: str\n",
    "    sub_output: Optional[str]\n",
    "\n",
    "\n",
    "# --- 父图节点，准备子图输入 ---\n",
    "def parent_node(state: ParentState):\n",
    "    print(f\"[父图] 接收输入: {state['main_input']}\")\n",
    "    return {\n",
    "        \"sub_input\": state[\"main_input\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 父图构建 ---\n",
    "parent_graph = StateGraph(ParentState)\n",
    "parent_graph.add_node(\"parent_node\", parent_node)\n",
    "parent_graph.add_node(\"subgraph_node\", compiled_subgraph)  # 直接加入子图节点\n",
    "parent_graph.set_entry_point(\"parent_node\")\n",
    "parent_graph.add_edge(\"parent_node\", \"subgraph_node\")\n",
    "\n",
    "compiled_parent_graph = parent_graph.compile()\n",
    "\n",
    "# --- 启动带子图流式输出的执行 ---\n",
    "for chunk in compiled_parent_graph.stream(\n",
    "        {\"main_input\": \"测试流式数据\"},\n",
    "        stream_mode=[\"custom\"],  # 监听自定义流\n",
    "        subgraphs=True  # 启用子图流式传输\n",
    "):\n",
    "    print(f\"流式数据: {chunk}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十部分：深入LLM Agent设计模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Agent 设计模式概要\n",
    "| **设计模式 (Pattern)**            | **核心思想**                                      | **LangGraph 主要实现组件**                            | **主要优势**                                |\n",
    "| ----------------------------- | --------------------------------------------- | ----------------------------------------------- | --------------------------------------- |\n",
    "| ReAct（推理与行动）                  | LLM 在思考（推理）和执行动作（如调用工具）之间交替进行，形成迭代循环。         | StateGraph, AIMessage (含 tool\\_calls), ToolNode | 增强 LLM 的问题解决能力，使其能与外部世界交互并根据观察调整行为。     |\n",
    "| 规划 (Planning)                 | 将复杂任务分解为一系列更小、更易于管理和执行的子任务或步骤。                | 状态（存储计划），规划器节点，执行器节点                            | 降低 LLM 处理复杂任务的认知负荷，提高推理准确性，减少错误。        |\n",
    "| “计划-执行” (Plan-and-Execute)    | 首先制定完整计划，然后逐步执行，并在必要时根据执行结果进行重新规划。            | 状态（计划、已执行步骤），规划器、执行器、重规划器节点                     | 实现明确的长期规划，适应任务执行过程中的变化，允许使用不同模型进行规划与执行。 |\n",
    "| 编排器-工作器 (Orchestrator-Worker) | 中心编排器将任务分解为 DAG，分配给工作器并行或串行执行，最后汇总结果。         | 状态（任务 DAG、结果），编排器节点，工作器节点 (Send API)            | 通过并行处理提升效率，模块化任务处理。                     |\n",
    "| 反思与自我批判 (Reflection)          | Agent 生成初步输出后，对其进行评估和批判，并利用反馈进行迭代式改进。         | 状态（输出、批判历史），生成器节点，反思器/批判器节点                     | 提升输出质量，纠正错误，使 Agent 能够从自身行为中学习。         |\n",
    "| 工具增强 (Tool-Augmented)         | 赋予 LLM 调用外部工具（API、函数、数据库等）的能力，以获取实时信息或执行特定操作。 | bind\\_tools, ToolNode                           | 克服 LLM 知识截至的限制，扩展其与现实世界交互和执行具体任务的能力。    |\n",
    "| 多智能体协作 (Multi-Agent)          | 多个 Agent 协同工作，每个 Agent 可能拥有专门的技能或角色，共同完成复杂目标。 | 主管节点，专员 Agent 节点 (子图)，Command (Handoffs)        | 模块化解决复杂问题，利用不同 Agent 的专长，提高系统的鲁棒性和可扩展性。 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReAct（推理与行动）模式\n",
    "\n",
    "#### 什么是 ReAct\n",
    "\n",
    "ReAct (Reasoning and Acting) 模式是一种基础且强大的 Agent 设计范式，它使 LLM 能够在“思考”（推理）和“行动”（执行具体步骤，如调用工具或 API）之间进行迭代循环 。其核心思想是，LLM 不仅仅是生成最终答案，而是生成一个“思考”过程，判断下一步需要什么信息或操作，然后“行动”以获取该信息或执行该操作。行动的结果（观察）会反馈给 LLM，LLM 再进行下一轮“思考”，如此往复，直至任务完成 。\n",
    "\n",
    "**核心思想**：**通过“思考（Thought）- 行动（Action）- 观察（Observation）”的循环，使 LLM 能够进行工具使用、信息检索和动态决策。**\n",
    "\n",
    "**典型工作流程**：\n",
    "\n",
    "    1. LLM 接收用户查询和当前状态。\n",
    "    2. LLM 生成“思考”，分析任务并决定是否需要使用工具。\n",
    "    3. 如果需要工具，LLM 生成“行动”，指定要调用的工具及其参数。\n",
    "    4. 系统执行工具调用。\n",
    "    5. LLM 接收工具返回的“观察”结果。\n",
    "    6. LLM 结合观察结果进行下一轮“思考”，或生成最终答案。\n",
    "**用例**：需要与外部环境交互的任务，如问答系统（使用搜索引擎）、任务自动化（调用 API）、代码生成与执行等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 代码示例 \n",
    "\n",
    "- 手动定义 llm_node_func + ToolNode 实现 ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt.chat_agent_executor import AgentState\n",
    "\n",
    "\n",
    "# 1. 定义工具 (示例)\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"获取指定城市的天气信息。\"\"\"\n",
    "    if city == \"北京\":\n",
    "        return \"北京今天晴朗，25摄氏度。\"\n",
    "    return f\"无法获取{city}的天气信息。\"\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature=0.0\n",
    ").bind_tools(tools)\n",
    "\n",
    "\n",
    "# 2. 定义 Agent 状态 (已在上方定义 AgentState)\n",
    "\n",
    "# 3. 定义节点函数\n",
    "def llm_node_func(state: AgentState):\n",
    "    \"\"\"调用 LLM 进行推理或决定工具调用。\"\"\"\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "tool_node_instance = ToolNode(tools)  # ToolNode 负责执行工具调用\n",
    "\n",
    "\n",
    "# 4. 定义条件边逻辑\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"根据 LLM 的输出决定下一步走向。\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"  # 转到工具节点\n",
    "    return \"end\"  # 结束\n",
    "\n",
    "\n",
    "# 5. 构建图\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"llm\", llm_node_func)\n",
    "workflow.add_node(\"tools\", tool_node_instance)\n",
    "\n",
    "workflow.set_entry_point(\"llm\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"tools\", \"llm\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "\n",
    "# 6. 运行图\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "# 构造初始对话历史\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"今天天气怎么样？请告诉我北京的。\")\n",
    "    ]\n",
    "}\n",
    "\n",
    "# 执行图，直到 END 节点\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "# 打印最终消息\n",
    "print(\"最终输出：\")\n",
    "for msg in final_state[\"messages\"]:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 示例：用 create_agent + ChatOpenAI + 自定义工具构建并运行一个 LLM Agent 图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义工具\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"获取指定城市的天气信息。\"\"\"\n",
    "    if city == \"北京\":\n",
    "        return \"北京今天晴朗，25摄氏度。\"\n",
    "    return f\"无法获取{city}的天气信息。\"\n",
    "\n",
    "\n",
    "tools = [get_weather]\n",
    "\n",
    "# 创建 Agent 节点（封装了 LLM 推理与工具调用意图识别）\n",
    "agent_node = create_react_agent(model, tools)\n",
    "\n",
    "# 构建 LangGraph 流程图\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", agent_node)\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_edge(\"agent\", END)\n",
    "\n",
    "# 编译\n",
    "graph = workflow.compile()\n",
    "\n",
    "# 运行：传入初始对话消息\n",
    "initial_state = {\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"北京今天的天气怎么样？\")\n",
    "    ]\n",
    "}\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "# 输出结果\n",
    "print(\"\\n✅ 最终对话：\")\n",
    "for msg in result[\"messages\"]:\n",
    "    print(f\"{msg.type}: {msg.content}\")\n",
    "\n",
    "\n",
    "# agent_executor = create_react_agent(model, tools)\n",
    "\n",
    "# # 初始消息\n",
    "# initial_state = {\n",
    "#     \"messages\": [\n",
    "#         HumanMessage(content=\"北京今天的天气怎么样？\")\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # 直接运行\n",
    "# result = agent_executor.invoke(initial_state)\n",
    "\n",
    "# # 输出结果\n",
    "# for msg in result[\"messages\"]:\n",
    "#     print(f\"{msg.type}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 规划模式\n",
    "\n",
    "#### 什么是规划模式\n",
    "规划模式是 Agentic AI 的核心特征之一，它使 Agent 能够为实现复杂目标制定一个全面的策略 。与 ReAct 模式侧重于单步的“思考-行动”不同，规划模式更强调在行动之前进行多步骤的“深思熟虑”。   \n",
    "\n",
    "#### 1. 任务分解与基础规划\n",
    "\n",
    "- **核心思想**：此模式的核心在于**任务分解 (task decomposition)**，即将一个宏大、复杂的目标或问题分解为一系列更小、更易于管理和执行的子任务或步骤 。LLM 通常被用来执行这种分解，生成一个有序的行动计划。这种方法能够有效降低 LLM 在处理每个独立步骤时的认知负荷，从而提高推理的准确性，并最大限度地减少幻觉和其他不准确性的产生 。   \n",
    "- **典型工作流程**：用户提供一个目标。一个“规划器 (Planner)” LLM 接收此目标，并将其分解为一个有序的步骤列表（即计划）。随后，每个步骤按顺序被执行，执行者可能是另一个 LLM、一个专门的工具，或者一个子 Agent。\n",
    "- **用例**：复杂查询应答（例如，需要多步信息检索和整合的问题）、多阶段报告生成、项目初步规划、编写代码的初步大纲等 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. “计划-执行”模式（含重新规划）\n",
    "\n",
    "##### **核心思想**\n",
    "\n",
    "Agent 首先创建一个全面的初始计划。然后，它按顺序执行计划中的每个步骤。关键在于，在每个步骤执行完毕后（或在执行一系列步骤后），Agent 会进行反思，评估已取得的成果和遇到的问题，并根据这些观察结果调整剩余的计划。\n",
    "\n",
    "这个“**计划 → 执行 → 观察 →（可能）重新规划 → 执行下一步**”的循环是此模式的核心。\n",
    "\n",
    "\n",
    "##### **典型工作流程**\n",
    "\n",
    "1. **规划 (Plan)**  \n",
    "   基于用户目标，生成初始的多步骤计划。\n",
    "\n",
    "2. **执行 (Execute Step)**  \n",
    "   执行计划中的当前步骤。\n",
    "\n",
    "3. **观察 (Observe)**  \n",
    "   获取执行步骤的结果或反馈。\n",
    "\n",
    "4. **重新规划 (Re-plan)**  \n",
    "   评估观察结果：\n",
    "   - 如果原始计划仍然适用且有效，则继续。\n",
    "   - 如果遇到问题、出现新的信息或发现更优路径，则修改或完全重制计划。\n",
    "\n",
    "5. **循环回到“执行”**  \n",
    "   直到所有（可能是调整后的）计划步骤完成。\n",
    "\n",
    "6. **最终响应 (Final Response)**  \n",
    "   生成并返回最终结果。\n",
    "\n",
    "> 💡 一个相关的图示可以在 LangGraph 官方文档中找到。LangChain 也提及了一个使用 Tavily 搜索来回答复杂问题的“计划-执行”LangGraph 机器人。\n",
    "\n",
    "##### 用例\n",
    "\n",
    "- 复杂的调研任务  \n",
    "- 代码生成与调试项目  \n",
    "- 长期项目管理  \n",
    "- 任何初始假设可能在执行过程中被证明是错误的或需要调整的长期任务\n",
    "\n",
    "\n",
    "##### 优势\n",
    "\n",
    "- ✅ **明确的长期规划**：  \n",
    "  有助于 LLM 处理其本身可能难以应对的复杂长期依赖。\n",
    "\n",
    "- 🧠 **模型选择的灵活性**：  \n",
    "  可以为规划/重新规划阶段选择能力更强、成本更高的模型，而为具体的执行步骤选择更轻量级、更经济的模型。\n",
    "\n",
    "- 📈 **更高的任务完成率和质量**：  \n",
    "  通过强制 LLM 明确“思考”完成整个任务所需的所有步骤，可以改善最终结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 实现策略：「计划-执行」模式\n",
    "\n",
    "##### 🌐 表LangGraph 实现“计划-执行”模式\n",
    "| 组件 (Component) | LangGraph 节点/逻辑 | 关键代码/逻辑参考 | 目的 (Purpose) |\n",
    "|------------------|---------------------|-------------------|----------------|\n",
    "| **状态 (State)** | `PlanExecute TypedDict`<br>字段包括：`input: str`, `plan: List[str]`, `past_steps: Annotated`, `response: str` | `operator.add` | 存储用户输入、当前计划、已执行步骤及其结果、最终响应 |\n",
    "| **规划器 (Planner)** | `planner_node` | 使用 `planner_prompt` 和 `model.with_structured_output(Plan)` | 接收用户 input，调用 LLM（结构化输出）生成初始计划 |\n",
    "| **执行器 (Executor)** | `executor_node` | 从 `state['plan']` 获取下一步，使用 `agent_executor.invoke()` 执行 | 执行当前步骤，将结果写入 `past_steps`，并从 `plan` 中移除当前步骤 |\n",
    "| **重新规划器 (Re-planner)** | `replanner_node` | 使用 `replanner_prompt` 和 `create_openai_fn_runnable(...)` | 根据 input、plan、past_steps 评估是否完成，更新计划或生成最终 response |\n",
    "| **控制流 (Control Flow)** | 条件边 | 例如：`workflow.add_conditional_edges(\"replan\", should_end, {\"True\": END, \"False\": \"agent\"})` | 控制执行顺序：START → planner → executor → replanner；replanner 决定结束或继续执行 |\n",
    "\n",
    "\n",
    "##### ⚙️ 节点说明（Nodes）\n",
    "\n",
    "###### 🔸 1. 状态 (`PlanExecute`)\n",
    "- 类型：`TypedDict`\n",
    "- 字段：\n",
    "  - `input`: 用户问题或请求\n",
    "  - `plan`: 当前计划（任务步骤列表）\n",
    "  - `past_steps`: 已执行的步骤及其结果\n",
    "  - `response`: 如果任务已完成，记录最终响应\n",
    "\n",
    "\n",
    "###### 🔸 2. 规划器节点 (`planner_node`)\n",
    "- 作用：生成初始任务计划\n",
    "- 提示模版强调：\n",
    "  - 简洁、明确、包含必要信息的步骤\n",
    "  - 确保最后一个步骤的执行结果是用户想要的最终答案\n",
    "- 技术实现：结构化输出模型（`with_structured_output(Plan)`）\n",
    "\n",
    "\n",
    "###### 🔸 3. 执行器节点 (`executor_node`)\n",
    "- 作用：逐步执行 `plan` 中的每个步骤\n",
    "- 每次执行后更新：\n",
    "  - `past_steps`：追加新执行结果\n",
    "  - `plan`：移除已执行步骤\n",
    "\n",
    "\n",
    "###### 🔸 4. 重新规划器节点 (`replanner_node`)\n",
    "- 作用：\n",
    "  - 根据 `input`、`plan`、`past_steps` 评估当前执行状态\n",
    "  - 决定：\n",
    "    - 是否继续执行下一步\n",
    "    - 是否重新生成计划\n",
    "    - 是否给出最终 `response`\n",
    "- 提示强调：\n",
    "  - 不应重复已完成的步骤\n",
    "  - 仅补充仍需执行的步骤\n",
    "  - 若目标已完成，生成最终答案\n",
    "\n",
    "###### 🔁 控制流结构（Control Flow）\n",
    "\n",
    "1. `START` → `planner_node`  \n",
    "2. `planner_node` → `executor_node`  \n",
    "3. `executor_node` → `replanner_node`  \n",
    "4. `replanner_node` → 条件判断：\n",
    "   - 若 `state['response']` 已生成 → `END`\n",
    "   - 否则 → 返回 `executor_node` 执行剩余 `plan`\n",
    "\n",
    "###### 示例条件边代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# workflow.add_conditional_edges(\n",
    "#     \"replan\",\n",
    "#     should_end,\n",
    "#     {\n",
    "#         \"True\": END,\n",
    "#         \"False\": \"agent\"  # agent 通常就是 executor_node\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 📌 总结\n",
    "\n",
    "“计划-执行”是 LangGraph 支持的高级 Agent 模式，适用于多步骤任务，具备以下优势：\n",
    "\n",
    "- 清晰的任务拆解和管理\n",
    "- 支持反馈驱动的自我调整\n",
    "- 提高任务完成率与正确性\n",
    "- 可插入不同模型处理规划、执行、调整三个阶段\n",
    "\n",
    "该模式特别适合：\n",
    "- 调研任务\n",
    "- 多阶段工具调用\n",
    "- 代码生成与调试\n",
    "- 长期任务管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import TypedDict, List, Tuple, Optional\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "\n",
    "# --- 模型初始化 ---\n",
    "# 初始化与 DeepSeek API 兼容的 ChatOpenAI 模型\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "\n",
    "# --- 状态定义 ---\n",
    "class PlanExecuteState(TypedDict):\n",
    "    \"\"\"定义智能体每一步的状态结构\"\"\"\n",
    "    input: str\n",
    "    plan: List[str]\n",
    "    past_steps: List[Tuple[str, str]]\n",
    "    response: str\n",
    "\n",
    "\n",
    "# --- 结构化输出模型 ---\n",
    "class Plan(BaseModel):\n",
    "    \"\"\"计划模型：用于规划器的输出\"\"\"\n",
    "    steps: List[str] = Field(description=\"为完成任务所需的分步计划列表\", default=[])\n",
    "\n",
    "\n",
    "class ReplannerDecision(BaseModel):\n",
    "    \"\"\"\n",
    "    修正者的决策模型。\n",
    "    模型将根据情况填充 response 字段（任务完成时）或 plan 字段（任务未完成时）。\n",
    "    \"\"\"\n",
    "    response: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"当任务已完成时，对用户原始任务的最终、完整的回答。\"\n",
    "    )\n",
    "    plan: Optional[List[str]] = Field(\n",
    "        default=None,\n",
    "        description=\"当任务未完成时，更新后的后续步骤列表。\"\n",
    "    )\n",
    "\n",
    "\n",
    "# --- 核心修复: 强制使用 \"function_calling\" 方法 ---\n",
    "# 明确指定获取结构化输出的方法，以兼容 DeepSeek API。\n",
    "planner_model = model.with_structured_output(Plan, method=\"function_calling\")\n",
    "executor_model = model\n",
    "replanner_model = model.with_structured_output(ReplannerDecision, method=\"function_calling\")\n",
    "\n",
    "\n",
    "# --- 节点定义 ---\n",
    "\n",
    "def planner_node(state: PlanExecuteState) -> PlanExecuteState:\n",
    "    \"\"\"规划器节点：接收用户任务，生成初始计划\"\"\"\n",
    "    print(\"--- [Planner] 规划中... ---\")\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个专业的项目规划师。请根据用户的任务，为其制定一个清晰、可执行的分步计划。\"),\n",
    "        (\"user\", \"任务：{input}\\n\\n请以步骤列表的形式返回你的计划。\")\n",
    "    ])\n",
    "    chain = prompt | planner_model\n",
    "    plan_obj = chain.invoke({\"input\": state[\"input\"]})\n",
    "    print(f\"--- [Planner] 生成计划: {plan_obj.steps} ---\")\n",
    "    return {**state, \"plan\": plan_obj.steps, \"past_steps\": []}\n",
    "\n",
    "\n",
    "def executor_node(state: PlanExecuteState) -> PlanExecuteState:\n",
    "    \"\"\"执行器节点：执行计划中的一个步骤\"\"\"\n",
    "    if not state[\"plan\"]:\n",
    "        print(\"--- [Executor] 计划已空，无需执行 ---\")\n",
    "        return state\n",
    "    step = state[\"plan\"][0]\n",
    "    print(f\"--- [Executor] 正在执行步骤: {step} ---\")\n",
    "    past_steps_formatted = \"\\n\".join(f\"步骤: {s}\\n结果: {r}\" for s, r in state[\"past_steps\"])\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"你是一个任务执行专家。请专注于执行当前给定的步骤，并简洁地报告你的执行结果。\"),\n",
    "        (\"user\", (\n",
    "            \"原始任务: {input}\\n\"\n",
    "            \"当前步骤: {step}\\n\\n\"\n",
    "            \"已完成的历史步骤和结果:\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{past_steps_formatted}\\n\"\n",
    "            \"---------------------\\n\\n\"\n",
    "            \"请执行当前步骤，并告诉我结果。\"\n",
    "        ))\n",
    "    ])\n",
    "    chain = prompt | executor_model\n",
    "    result = chain.invoke({\n",
    "        \"input\": state[\"input\"],\n",
    "        \"step\": step,\n",
    "        \"past_steps_formatted\": past_steps_formatted or \"无\"\n",
    "    })\n",
    "    result_content = result.content.strip()\n",
    "    print(f\"--- [Executor] 步骤执行结果: {result_content} ---\")\n",
    "    return {\n",
    "        **state,\n",
    "        \"plan\": state[\"plan\"][1:],\n",
    "        \"past_steps\": state[\"past_steps\"] + [(step, result_content)],\n",
    "    }\n",
    "\n",
    "\n",
    "def replanner_node(state: PlanExecuteState) -> PlanExecuteState:\n",
    "    \"\"\"修正者节点：评估当前进展，判断是继续、修正计划还是结束任务\"\"\"\n",
    "    print(\"--- [Replanner] 评估中... ---\")\n",
    "    past_steps_formatted = \"\\n\".join(f\"步骤: {s}\\n结果: {r}\" for s, r in state[\"past_steps\"])\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", (\n",
    "            \"你是一位经验丰富的项目经理。你的任务是评估当前的工作进展，并决定下一步行动。\\n\"\n",
    "            \"1. 如果你认为任务已成功解决，请在 'response' 字段中给出最终的、完整的答案。\\n\"\n",
    "            \"2. 如果你认为计划尚未完成或需要调整，请在 'plan' 字段中给出一个全新的后续计划。\\n\"\n",
    "            \"你必须且只能填充 'response' 或 'plan' 中的一个字段。\"\n",
    "        )),\n",
    "        (\"user\", (\n",
    "            \"原始任务: {input}\\n\\n\"\n",
    "            \"剩余计划: {plan}\\n\\n\"\n",
    "            \"已完成的工作历史:\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{past_steps_formatted}\\n\"\n",
    "            \"---------------------\\n\\n\"\n",
    "            \"请根据以上信息进行判断并返回你的决策。\"\n",
    "        ))\n",
    "    ])\n",
    "    chain = prompt | replanner_model\n",
    "    result = chain.invoke({\n",
    "        \"input\": state[\"input\"],\n",
    "        \"plan\": state[\"plan\"] or \"无\",\n",
    "        \"past_steps_formatted\": past_steps_formatted or \"无\"\n",
    "    })\n",
    "\n",
    "    if result.response is not None:\n",
    "        print(\"--- [Replanner] 任务完成，生成最终响应 ---\")\n",
    "        return {**state, \"response\": result.response, \"plan\": []}\n",
    "    elif result.plan is not None:\n",
    "        print(f\"--- [Replanner] 计划已更新: {result.plan} ---\")\n",
    "        return {**state, \"plan\": result.plan, \"response\": \"\"}\n",
    "    else:\n",
    "        print(\"--- [Replanner] 模型未做出明确决策，任务终止 ---\")\n",
    "        return {**state, \"response\": \"模型未能做出决策，任务提前结束。\", \"plan\": []}\n",
    "\n",
    "\n",
    "def should_end(state: PlanExecuteState) -> str:\n",
    "    \"\"\"条件边：判断是否应该结束图的执行\"\"\"\n",
    "    return \"True\" if state.get(\"response\") else \"False\"\n",
    "\n",
    "\n",
    "# --- 构建图 ---\n",
    "workflow = StateGraph(PlanExecuteState)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"replanner\", replanner_node)\n",
    "workflow.set_entry_point(\"planner\")\n",
    "workflow.add_edge(\"planner\", \"executor\")\n",
    "workflow.add_edge(\"executor\", \"replanner\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"replanner\",\n",
    "    should_end,\n",
    "    {\"True\": END, \"False\": \"executor\"}\n",
    ")\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- 执行与演示 ---\n",
    "if __name__ == \"__main__\":\n",
    "    task = \"写一份关于'人工智能在市场营销中的应用'的市场调研报告\"\n",
    "    init_state: PlanExecuteState = {\n",
    "        \"input\": task,\n",
    "        \"plan\": [],\n",
    "        \"past_steps\": [],\n",
    "        \"response\": \"\"\n",
    "    }\n",
    "\n",
    "    print(f\"开始执行任务: {task}\\n\" + \"=\" * 50)\n",
    "\n",
    "    try:\n",
    "        print(\"\\n--- 任务执行中... ---\\n\")\n",
    "        final_state = app.invoke(init_state, {\"recursion_limit\": 20})\n",
    "\n",
    "        if final_state and final_state.get('response'):\n",
    "            print(\"\\n✅ 任务完成！最终报告：\\n\")\n",
    "            print(final_state['response'])\n",
    "        else:\n",
    "            print(\"\\n❌ 任务未生成最终结果，可能是因为达到了递归次数限制或发生其他错误。\")\n",
    "            print(\"\\n最终状态快照:\", final_state)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n🔥🔥🔥 程序执行时发生错误 🔥🔥🔥\")\n",
    "        if \"401\" in str(e):\n",
    "            print(\"错误类型: 认证失败 (Authentication Error)\")\n",
    "            print(\"解决方案: 请检查你的 DEEPSEEK_API_KEY 是否正确、有效且有足够额度。\")\n",
    "        elif isinstance(e, OutputParserException):\n",
    "            print(\"错误类型: 输出解析失败 (Output Parser Exception)\")\n",
    "            print(\"根本原因: 模型返回的内容无法被正确解析为预期的格式。\")\n",
    "            print(\"错误详情:\", e)\n",
    "        else:\n",
    "            print(\"未知错误:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新规划步骤是“计划-执行”模式智能性的核心体现。它不仅仅是机械地执行一个静态计划，而是展现了适应性问题解决能力。LangGraph 通过条件边，使得在重新规划后能够基于动态状态决定是否循环回执行器，这对于实现这种适应性至关重要。如果一个初步计划在早期步骤未能产生预期结果（例如，搜索查询未返回任何信息），这将可能导致后续依赖此结果的步骤完全失效。重新规划器通过审视 `past_steps` 中的实际执行历史，能够识别这类问题。它可以生成一个全新的计划，例如，包含重新表述搜索查询或尝试不同信息源的步骤，而不是盲目地继续一个已经偏离轨道的旧计划。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编排器-工作器模式（Orchestrator-Worker Pattern）\n",
    "\n",
    "#### 模式概述\n",
    "\n",
    "**编排器-工作器（Orchestrator-Worker）模式** 是一种高级任务规划执行模型，受到如 [LLMCompiler](https://arxiv.org/abs/2307.04743) 等研究启发，适用于将任务拆解为可并行执行的多个子任务（组成一个 DAG，有向无环图）的场景。\n",
    "\n",
    "\n",
    "#### 核心思想\n",
    "\n",
    "该模式中存在两个核心角色：\n",
    "\n",
    "- **编排器（Orchestrator）**：由 LLM 充当，负责将用户的目标任务分解为多个子任务，形成 DAG 图结构，并定义任务之间的依赖关系。\n",
    "- **工作器（Worker）**：可以是 LLM 或专用工具，接收任务执行请求并返回结果。\n",
    "\n",
    "工作流程：\n",
    "\n",
    "1. 编排器将任务拆解为若干子任务（节点），并构建 DAG。\n",
    "2. 每个子任务包含：\n",
    "   - 子任务内容（描述）\n",
    "   - 所需工具或调用的 Agent\n",
    "   - 前置依赖任务列表\n",
    "3. 一个**任务调度器（Task Fetching Unit）**监控 DAG 和执行状态：\n",
    "   - 任何一个任务如果其所有依赖都已完成，就会被调度执行。\n",
    "   - 多个可同时执行的任务将被并行派发。\n",
    "4. 所有任务完成后，编排器或一个专门的 **连接器（Joiner）** 汇总所有任务结果，生成最终输出。\n",
    "5. 如果结果不理想，连接器还可以触发 **重新规划**（Replan）。\n",
    "\n",
    "\n",
    "#### 典型工作流程\n",
    "\n",
    "```text\n",
    "用户输入 → 编排器生成 DAG → 任务调度器监控 DAG → 满足依赖的任务被派发至工作器执行 → 执行结果返回 → 连接器汇总并生成最终结果\n",
    "```\n",
    "\n",
    "#### 代表用例\n",
    "- 复杂的市场调研报告（不同章节可并行撰写）\n",
    "\n",
    "- 多模块软件开发（并行实现多个功能模块）\n",
    "\n",
    "- 多人协作式智能助手任务拆解执行\n",
    "\n",
    "- 任意子任务间具有依赖但可并行的任务流\n",
    "\n",
    "---\n",
    "\n",
    "LangGraph 的 Send API 是实现编排器-工作器模式中动态并行性的关键。它允许图在运行时根据计划的结构“派生”出工作，而无需在设计时静态地定义所有可能的并行分支。传统的并行化方法可能涉及预先定义固定数量的并行路径（如  的基础并行化示例）。然而，编排器生成的计划可能包含可变数量的子任务。Send 原语使得编排器节点（或其后的分发节点）能够为每个子任务动态地创建对 worker_node 的调用，有效地将工作分散出去。然后，结果被收集回共享状态，从而实现了动态的“扇出”和“扇入”。   \n",
    "\n",
    "在 LangGraph 的状态中有效管理 DAG 的依赖关系需要精心设计。状态不仅要存储任务定义，还要存储它们之间的相互依赖关系和完成状态，以便“任务获取单元”的逻辑（无论是节点还是条件边）能够正确地调度任务。这意味着状态中的 task_dag 理想情况下应明确表示依赖关系（例如，每个任务对象都有一个 depends_on: List[str] 字段）。然后，worker_scheduler_node 就变成了一个有状态的调度器，持续检查是否有已准备好运行的任务。这比简单的线性计划要复杂得多，但提供了显著的灵活性和效率优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Literal, TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "import time\n",
    "\n",
    "class Task(TypedDict):\n",
    "    id: str\n",
    "    description: str\n",
    "    tool_to_use: str\n",
    "    dependencies: List[str]\n",
    "    input_args: Dict[str, Any]\n",
    "\n",
    "class OrchestratorState(TypedDict):\n",
    "    input: str\n",
    "    task_dag: List[Task]\n",
    "    task_status: Dict[str, Literal[\"pending\", \"ready\", \"in_progress\", \"completed\", \"failed\"]]\n",
    "    task_results: Dict[str, Any]\n",
    "    active_workers: int\n",
    "    final_report: str\n",
    "\n",
    "# 模拟模型\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# 工具执行器（worker）\n",
    "def worker_node(state: OrchestratorState) -> OrchestratorState:\n",
    "\n",
    "    new_status = state[\"task_status\"].copy()\n",
    "    new_results = state[\"task_results\"].copy()\n",
    "    new_active = state[\"active_workers\"]\n",
    "\n",
    "    for task in state[\"task_dag\"]:\n",
    "        print(f\"正在调度任务：{task['id']}, 当前状态：{state['task_status'][task['id']]}\")\n",
    "        tid = task[\"id\"]\n",
    "        if state[\"task_status\"].get(tid) != \"ready\":\n",
    "            continue\n",
    "\n",
    "        # 模拟执行：合成 prompt\n",
    "        args = task[\"input_args\"].copy()\n",
    "        for dep in task[\"dependencies\"]:\n",
    "            args[dep] = state[\"task_results\"].get(dep, \"\")\n",
    "\n",
    "        prompt = f\"任务描述：{task['description']}\\n输入：{args}\"\n",
    "        resp = model.invoke([HumanMessage(content=prompt)])\n",
    "        new_status[tid] = \"completed\"\n",
    "        new_results[tid] = resp.content\n",
    "        new_active -= 1\n",
    "        break  # 每次只执行一个任务，便于调度控制\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"task_status\": new_status,\n",
    "        \"task_results\": new_results,\n",
    "        \"active_workers\": new_active,\n",
    "    }\n",
    "\n",
    "# 调度器：更新状态、检查依赖\n",
    "def scheduler_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    updated_status = state[\"task_status\"].copy()\n",
    "\n",
    "    for task in state[\"task_dag\"]:\n",
    "        print(f\"正在调度任务：{task['id']}, 当前状态：{state['task_status'][task['id']]}\")\n",
    "        tid = task[\"id\"]\n",
    "        if updated_status.get(tid) != \"pending\":\n",
    "            continue\n",
    "\n",
    "        # 检查依赖是否完成\n",
    "        deps = task[\"dependencies\"]\n",
    "        if all(state[\"task_status\"].get(dep) == \"completed\" for dep in deps):\n",
    "            updated_status[tid] = \"ready\"\n",
    "\n",
    "    return {\n",
    "        **state,\n",
    "        \"task_status\": updated_status,\n",
    "        \"active_workers\": sum(1 for v in updated_status.values() if v == \"in_progress\"),\n",
    "    }\n",
    "\n",
    "# 聚合器：最终报告生成器\n",
    "def joiner_node(state: OrchestratorState) -> OrchestratorState:\n",
    "    if all(v == \"completed\" for v in state[\"task_status\"].values()):\n",
    "        summary = \"\\n\".join(\n",
    "            [f\"[{tid}]: {res}\" for tid, res in state[\"task_results\"].items()]\n",
    "        )\n",
    "        return {**state, \"final_report\": f\"任务全部完成，结果如下：\\n{summary}\"}\n",
    "    return state\n",
    "\n",
    "# 检查是否结束\n",
    "def should_terminate(state: OrchestratorState):\n",
    "    return \"True\" if state.get(\"final_report\") else \"False\"\n",
    "\n",
    "# LangGraph 编排\n",
    "workflow = StateGraph(OrchestratorState)\n",
    "\n",
    "workflow.add_node(\"schedule\", scheduler_node)\n",
    "workflow.add_node(\"execute\", worker_node)\n",
    "workflow.add_node(\"joiner\", joiner_node)\n",
    "\n",
    "workflow.set_entry_point(\"schedule\")\n",
    "workflow.add_edge(\"schedule\", \"execute\")\n",
    "workflow.add_edge(\"execute\", \"joiner\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"joiner\",\n",
    "    should_terminate,\n",
    "    {\n",
    "        \"True\": END,\n",
    "        \"False\": \"schedule\",\n",
    "    },\n",
    ")\n",
    "\n",
    "app = workflow.compile()\n",
    "init_state: OrchestratorState = {\n",
    "    \"input\": \"写一份关于AI趋势的市场调研报告\",\n",
    "    \"task_dag\": [\n",
    "        {\n",
    "            \"id\": \"collect_data\",\n",
    "            \"description\": \"收集最新AI发展趋势数据\",\n",
    "            \"tool_to_use\": \"search_tool\",\n",
    "            \"dependencies\": [],\n",
    "            \"input_args\": {}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"analyze_data\",\n",
    "            \"description\": \"分析AI趋势数据，提炼出关键趋势\",\n",
    "            \"tool_to_use\": \"llm_tool\",\n",
    "            \"dependencies\": [\"collect_data\"],\n",
    "            \"input_args\": {}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"write_report\",\n",
    "            \"description\": \"撰写市场调研报告\",\n",
    "            \"tool_to_use\": \"llm_tool\",\n",
    "            \"dependencies\": [\"analyze_data\"],\n",
    "            \"input_args\": {}\n",
    "        }\n",
    "    ],\n",
    "    \"task_status\": {\n",
    "        \"collect_data\": \"pending\",\n",
    "        \"analyze_data\": \"pending\",\n",
    "        \"write_report\": \"pending\"\n",
    "    },\n",
    "    \"task_results\": {},\n",
    "    \"active_workers\": 0,\n",
    "    \"final_report\": \"\"\n",
    "}\n",
    "\n",
    "result = app.invoke(init_state)\n",
    "print(result[\"final_report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 反思与自我批判模式\n",
    "\n",
    "通过引入“反思器”或“批判器”（反思器 ≈ 评估器 + 建议器），让 LLM 对自己的初步回答进行自我检查、自我批判、自我修正，从而生成更高质量、更可信、更完整的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 迭代式改进（生成器-反思器/批判器循环）\n",
    "\n",
    "\n",
    "##### **核心思想**\n",
    "通过引入“反思器（Critic）”，对模型生成的答案进行质量评价，并在必要时提出改写建议，形成一个“生成-反思-再生成”的闭环，从而系统性地提高输出的准确性、完整性和可信度。\n",
    "\n",
    "\n",
    "##### **典型工作流程**\n",
    "\n",
    "将其概括为三个主要组成部分：生成 (Generation)、自我反思 (Self-Reflection) 和迭代求精 (Iterative Refinement)。 展示了一个简单的循环：生成器 (Generator) -> 反思器 (Reflector) -> 生成器。\n",
    "\n",
    "##### **举个例子**\n",
    "```text\n",
    "Q: 爱因斯坦与牛顿的主要科学贡献分别是什么？\n",
    "\n",
    "🔸 初步回答：\n",
    "“牛顿提出了万有引力，爱因斯坦提出了相对论。”\n",
    "\n",
    "🔹 反思器：\n",
    "[问题] 该回答过于简略，未区分两人贡献的时代背景与物理理论层次。\n",
    "[建议] 扩展牛顿在经典力学和微积分的角色，补充爱因斯坦对量子论和光电效应的影响。\n",
    "\n",
    "🔁 重写后回答：\n",
    "“牛顿在经典力学、万有引力定律和微积分上做出奠基性贡献；爱因斯坦则以相对论、光电效应及对量子物理的推动改变了现代物理格局。”\n",
    "\n",
    "✅ 输出。\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **MessageGraph 实现文本反思**\n",
    "**场景：处理对话或文本内容的改写与优化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessageGraph, END\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from typing import List\n",
    "\n",
    "# -- 1. 自定义生成器节点 --\n",
    "def generation_node(state: List):\n",
    "    previous_message = state[-1].content if state else \"\"\n",
    "    new_output = f\"Revised draft based on: {previous_message}\"\n",
    "    return state + [AIMessage(content=new_output)]\n",
    "\n",
    "# -- 2. 自定义反思器节点 --\n",
    "def reflection_node(state: List):\n",
    "    last_output = state[-1].content\n",
    "    feedback = f\"This needs more detail. Consider expanding: '{last_output}'\"\n",
    "    return state + [AIMessage(content=feedback)]\n",
    "\n",
    "# -- 3. 构建 MessageGraph --\n",
    "builder = MessageGraph()\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.set_entry_point(\"generate\")\n",
    "\n",
    "# -- 4. 迭代终止条件 --\n",
    "def should_continue_reflection_loop(state: List):\n",
    "    if len(state) >= 6:  # 限制3轮（每轮2条消息）\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "builder.add_conditional_edges(\"generate\", should_continue_reflection_loop)\n",
    "builder.add_edge(\"reflect\", \"generate\")\n",
    "\n",
    "# -- 5. 编译图并执行 --\n",
    "graph = builder.compile()\n",
    "\n",
    "# 初始输入：来自用户的请求\n",
    "initial_state = [HumanMessage(content=\"Write an email apologizing for a delay\")]\n",
    "\n",
    "# 运行图\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "# 打印每一步的消息\n",
    "for i, msg in enumerate(final_state):\n",
    "    print(f\"[Step {i}] {msg.type}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **StateGraph 实现代码级反思**\n",
    "**场景：适用于代码生成 + 批判 + 重新生成等结构性产物优化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简要伪代码结构（适配 create_reflection_graph 的思路）\n",
    "from typing import TypedDict, Any\n",
    "\n",
    "from langgraph.constants import END\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "\n",
    "# def is_critique_positive(feedback: str) -> bool:\n",
    "#     # 如果批判器反馈中含有 \"looks good\" 这样的词，就认为可以终止\n",
    "#     return \"looks good\" in feedback.lower() or \"no more changes\" in feedback.lower()\n",
    "\n",
    "def is_critique_positive(feedback: dict) -> bool:\n",
    "    return feedback.get(\"score\", 0) >= 8  # 得分大于等于 8 说明足够好\n",
    "\n",
    "\n",
    "\n",
    "def create_reflection_graph(generator_graph, critique_graph, max_iterations):\n",
    "    class ReflectionLoopState(TypedDict):\n",
    "        input_request: Any\n",
    "        current_artifact: Any\n",
    "        critique_feedback: Any\n",
    "        iteration: int\n",
    "\n",
    "    workflow = StateGraph(ReflectionLoopState)\n",
    "\n",
    "    def generation_step(state):\n",
    "        new_artifact = generator_graph.invoke(state[\"current_artifact\"])\n",
    "        return {\"current_artifact\": new_artifact, \"iteration\": state[\"iteration\"] + 1}\n",
    "\n",
    "    def critique_step(state):\n",
    "        feedback = critique_graph.invoke(state[\"current_artifact\"])\n",
    "        return {\"critique_feedback\": feedback}\n",
    "\n",
    "    def check_condition(state):\n",
    "        if state[\"iteration\"] >= max_iterations or is_critique_positive(state[\"critique_feedback\"]):\n",
    "            return END\n",
    "        return \"generation_step\"\n",
    "\n",
    "    workflow.add_node(\"generation_step\", generation_step)\n",
    "    workflow.add_node(\"critique_step\", critique_step)\n",
    "    workflow.set_entry_point(\"generation_step\")\n",
    "\n",
    "    workflow.add_edge(\"generation_step\", \"critique_step\")\n",
    "    workflow.add_conditional_edges(\"critique_step\", check_condition)\n",
    "\n",
    "    return workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **反思质量与有效性分析**\n",
    "\n",
    "有效的反思器需满足以下条件：\n",
    "- 结构化反馈：不是“这不好”，而是“哪里不好、为什么、怎么改”。\n",
    "\n",
    "- 高操作性：建议应能被 Generator 明确执行（如“替换 X 为 Y”、“补充段落 B”）。\n",
    "\n",
    "- 精准工具辅助：例如代码反思中使用 Pyright 等静态分析器。\n",
    "\n",
    "✅ 高质量 Reflector 示例：\n",
    "- 扮演“资深编辑”，逐段或逐行点评并提出修改建议\n",
    "\n",
    "- 使用格式化提示结构，例如：\n",
    "\n",
    "```markdown\n",
    "[Issue] 第三段含糊不清\n",
    "[Suggestion] 补充“定义A”的说明，并举例\n",
    "\n",
    "```\n",
    "\n",
    "❌ 低质量 Reflector 示例：\n",
    "- “这段写得很差”\n",
    "\n",
    "- “需要改进”\n",
    "\n",
    "这样的模糊反馈对生成器几乎无帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自我反思 RAG（Self-Reflective RAG，简称 Self-RAG）\n",
    "\n",
    "自我反思检索增强生成是一种高级 RAG 技术，不仅被动利用检索文档，而是主动对检索和生成过程进行“反思”和评估，从而提升答案的准确性和质量。\n",
    "\n",
    "##### **核心思想**\n",
    "\n",
    "LLM 在 RAG 流程中扮演主动角色，不只是生成答案，还要：\n",
    "\n",
    "- 判断是否真的需要检索外部文档\n",
    "\n",
    "- 评估检索到文档的相关性\n",
    "\n",
    "- 判断生成答案是否忠实于提供的文档（防止幻觉）\n",
    "\n",
    "- 评估答案是否完整且对用户有用\n",
    "\n",
    "- 根据评估结果，决定是否进行迭代优化（重新检索、重写问题、重新生成等）\n",
    "\n",
    "\n",
    "##### **典型工作流程**\n",
    "1. **（可选）判断是否需要检索**：\n",
    "LLM 先分析问题，判断是否凭自身知识库就能回答，或者必须检索外部知识。\n",
    "\n",
    "2. **检索文档**：\n",
    "根据问题，从知识库中检索相关文档。\n",
    "\n",
    "3. **评估文档相关性**：\n",
    "使用评估器对每个文档相关性打分或分类，仅保留高度相关的文档。\n",
    "\n",
    "4. **生成答案**：\n",
    "基于筛选后的相关文档生成初步答案。\n",
    "\n",
    "5. **评估答案忠实度（防幻觉）**：\n",
    "检查生成答案是否真实基于提供的文档内容。\n",
    "\n",
    "6. **评估答案完整性和相关性**：\n",
    "判断答案是否完整且直接回应原始问题。\n",
    "\n",
    "7. **（可选）迭代/优化**：\n",
    "根据评估结果，若答案不理想，则可能重新检索、改写问题、重新生成等。\n",
    "\n",
    "\n",
    "\n",
    "##### **典型用例**\n",
    "- 高度事实准确性的问答系统\n",
    "\n",
    "- 避免幻觉的智能聊天机器人\n",
    "\n",
    "- 需要严格信息来源和输出质量控制的 RAG 应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **LangGraph 实现 Self-RAG**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义状态结构"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from typing import List, TypedDict, Any\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# SelfRAGState 是一个用于 LangGraph 状态传递的数据结构（类似上下文状态容器）\n",
    "# 它记录了整个自我增强型 RAG 流程中每个阶段的输入、输出和中间结果。\n",
    "class SelfRAGState(TypedDict):\n",
    "    # 用户输入的问题，将作为整个流程的起点\n",
    "    question: str\n",
    "\n",
    "    # 从向量数据库或其他检索系统中检索到的原始文档（未过滤）\n",
    "    documents: List[Document]\n",
    "\n",
    "    # 经过相关性评分器（grade_documents_node）过滤后的文档，仅保留与问题高度相关的部分\n",
    "    relevant_documents: List[Document]\n",
    "\n",
    "    # 基于 relevant_documents 和 question 生成的最终回答\n",
    "    generation: str\n",
    "\n",
    "    # 对检索文档相关性的评估结果，例如：\"relevant\"、\"irrelevant\"，或结构化反馈\n",
    "    document_relevance_critique: Any  # 可以是字符串、结构化对象或模型预测结果\n",
    "\n",
    "    # 对生成答案是否忠于文档事实的评估结果（即是否出现幻觉）\n",
    "    # 可能的值： \"good\"、\"hallucinated\"、结构化反馈等\n",
    "    generation_groundedness_critique: Any\n",
    "\n",
    "    # 对生成答案是否完整、是否确实回答了问题的评估结果\n",
    "    # 可能的值： \"good\"、\"incomplete\"、结构化反馈等\n",
    "    generation_completeness_critique: Any\n",
    "\n",
    "    # 标记是否需要重新生成答案，例如检测到幻觉或其他质量问题\n",
    "    should_regenerate: bool\n",
    "\n",
    "    # 标记是否需要重新进行检索（例如：没有找到相关文档，或者回答不完整）\n",
    "    should_reretrieve: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 节点定义（Nodes）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 假设以下函数均已实现：\n",
    "# retrieve_node_func, grade_documents_node_func, generate_node_func,\n",
    "# grade_hallucination_node_func, grade_answer_relevance_node_func,\n",
    "# (可选) rewrite_query_node_func\n",
    "\n",
    "workflow = StateGraph(SelfRAGState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_node_func)\n",
    "workflow.add_node(\"grade_documents\", grade_documents_node_func)\n",
    "workflow.add_node(\"generate\", generate_node_func)\n",
    "workflow.add_node(\"grade_hallucination\", grade_hallucination_node_func)\n",
    "workflow.add_node(\"grade_answer_relevance\", grade_answer_relevance_node_func)\n",
    "# 可选节点：rewrite 查询\n",
    "# workflow.add_node(\"rewrite_query\", rewrite_query_node_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. retrieve_node_func: 检索文档节点"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def retrieve_node_func(state):\n",
    "    \"\"\"\n",
    "    文档检索节点。\n",
    "    输入：用户问题 `state[\"question\"]`\n",
    "    输出：更新 `state[\"documents\"]` 字段（List[Document]）\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    # 假设调用了一个向量检索系统（如 FAISS、Weaviate、Qdrant 等）\n",
    "    documents = vector_store.similarity_search(question, k=5)\n",
    "    \n",
    "    return {**state, \"documents\": documents}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. grade_documents_node_func: 文档相关性评估节点"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def grade_documents_node_func(state):\n",
    "    \"\"\"\n",
    "    使用 LLM 或启发式方法对文档进行相关性评估。\n",
    "    输出：\n",
    "        - relevant_documents: 过滤后的文档\n",
    "        - document_relevance_critique: 字符串（\"relevant\"/\"irrelevant\"/其他标志）\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    # 使用 LLM + prompt 判断每个文档是否相关（这里简化为全部保留）\n",
    "    relevant_documents = [doc for doc in documents if is_relevant(question, doc.page_content)]\n",
    "    \n",
    "    critique = \"relevant\" if relevant_documents else \"irrelevant\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"relevant_documents\": relevant_documents,\n",
    "        \"document_relevance_critique\": critique\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. generate_node_func: 文本生成节点"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_node_func(state):\n",
    "    \"\"\"\n",
    "    使用相关文档生成答案。\n",
    "    输入：question + relevant_documents\n",
    "    输出：更新字段 \"generation\"\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    context = \"\\n\".join([doc.page_content for doc in state[\"relevant_documents\"]])\n",
    "    \n",
    "    prompt = f\"\"\"你是一个问答助手，请基于以下上下文回答用户问题：\n",
    "    上下文：\n",
    "    {context}\n",
    "\n",
    "    问题：{question}\n",
    "    答案：\"\"\"\n",
    "    \n",
    "    generation = llm.invoke(prompt)  # 这里假设你有一个 LLM 实例\n",
    "    return {**state, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. grade_hallucination_node_func: 幻觉检测节点"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def grade_hallucination_node_func(state):\n",
    "    \"\"\"\n",
    "    判断生成是否与上下文一致（防止幻觉）。\n",
    "    输出：\n",
    "        - generation_groundedness_critique: \"good\"/\"hallucinated\"\n",
    "        - should_regenerate: bool\n",
    "    \"\"\"\n",
    "    context = \"\\n\".join([doc.page_content for doc in state[\"relevant_documents\"]])\n",
    "    answer = state[\"generation\"]\n",
    "    \n",
    "    prompt = f\"\"\"请判断以下回答是否忠于提供的上下文。\n",
    "\n",
    "    上下文：\n",
    "    {context}\n",
    "\n",
    "    回答：\n",
    "    {answer}\n",
    "\n",
    "    如果回答有编造内容，请回答 \"hallucinated\"，否则回答 \"good\"。\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(prompt).strip().lower()\n",
    "    critique = \"hallucinated\" if \"hallucinated\" in result else \"good\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"generation_groundedness_critique\": critique,\n",
    "        \"should_regenerate\": (critique != \"good\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. grade_answer_relevance_node_func: 回答相关性与完整性检查节点"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def grade_answer_relevance_node_func(state):\n",
    "    \"\"\"\n",
    "    判断回答是否解决了用户问题、是否完整。\n",
    "    输出：\n",
    "        - generation_completeness_critique: \"good\"/\"incomplete\"\n",
    "        - should_reretrieve: bool\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    answer = state[\"generation\"]\n",
    "    \n",
    "    prompt = f\"\"\"请判断以下回答是否完整并且确实回答了问题。\n",
    "\n",
    "    问题：{question}\n",
    "    回答：{answer}\n",
    "\n",
    "    如果回答完整并相关，回答 \"good\"；否则回答 \"incomplete\"。\n",
    "    \"\"\"\n",
    "    \n",
    "    result = llm.invoke(prompt).strip().lower()\n",
    "    critique = \"incomplete\" if \"incomplete\" in result else \"good\"\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"generation_completeness_critique\": critique,\n",
    "        \"should_reretrieve\": (critique != \"good\")\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（可选）6. rewrite_query_node_func: 查询重写节点（提高检索效果）"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def rewrite_query_node_func(state):\n",
    "    \"\"\"\n",
    "    当原始查询无法检索到相关文档时，重写查询。\n",
    "    输出：更新 state[\"question\"]\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", [])\n",
    "    \n",
    "    prompt = f\"\"\"用户原始问题是：\n",
    "    {question}\n",
    "\n",
    "    目前检索到的文档如下：\n",
    "    {\"；\".join([doc.page_content[:200] for doc in documents])}\n",
    "\n",
    "    请根据这些内容重写用户的问题，使其更容易检索到相关文档。\n",
    "    新问题：\"\"\"\n",
    "    \n",
    "    new_question = llm.invoke(prompt).strip()\n",
    "    return {**state, \"question\": new_question}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置入口节点"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 文档评估后决策逻辑"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decide_after_grading_documents(state: SelfRAGState):\n",
    "    if state[\"document_relevance_critique\"] == \"relevant\":  # 简化判断\n",
    "        return \"generate\"\n",
    "    # elif state[\"should_reretrieve\"]:\n",
    "    #     return \"rewrite_query\"\n",
    "    return END\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\"grade_documents\", decide_after_grading_documents, {\n",
    "    \"generate\": \"generate\",\n",
    "    # \"rewrite_query\": \"rewrite_query\",\n",
    "    END: END\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回答评估路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workflow.add_edge(\"generate\", \"grade_hallucination\")\n",
    "workflow.add_edge(\"grade_hallucination\", \"grade_answer_relevance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 回答评估后决策逻辑"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def decide_after_grading_generation(state: SelfRAGState):\n",
    "    if (state[\"generation_groundedness_critique\"] == \"good\" and\n",
    "        state[\"generation_completeness_critique\"] == \"good\"):\n",
    "        return END\n",
    "    # elif state[\"should_regenerate\"]:\n",
    "    #     return \"generate\"\n",
    "    # elif state[\"should_reretrieve\"]:\n",
    "    #     return \"rewrite_query\"\n",
    "    return END\n",
    "\n",
    "workflow.add_conditional_edges(\"grade_answer_relevance\", decide_after_grading_generation, {\n",
    "    # \"generate\": \"generate\",\n",
    "    # \"rewrite_query\": \"rewrite_query\",\n",
    "    END: END\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建图"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "graph = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **总结与优势分析**\n",
    "\n",
    "Self-RAG 借助 LangGraph 的图结构将传统 RAG 流程从「线性」转变为「智能自适应」。它引入多个反思与评估节点，增强了整个系统的可靠性与自我修复能力：\n",
    "\n",
    "| 模块                            | 功能      | 优势             |\n",
    "| ----------------------------- | ------- | -------------- |\n",
    "| `grade_documents_node`        | 文档相关性判断 | 过滤无关上下文，减少干扰   |\n",
    "| `grade_hallucination_node`    | 幻觉检测    | 避免生成错误或编造的信息   |\n",
    "| `grade_answer_relevance_node` | 答案完整性评估 | 确保回复对用户有用且回答全面 |\n",
    "| 条件分支                          | 控制流程走向  | 支持重检索、重生成等智能策略 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 工具增强型智能体\n",
    "\n",
    "#### 什么是工具增强型智能体？\n",
    "\n",
    "工具增强型智能体是指 **允许大型语言模型（LLM）调用外部工具**（如 API、数据库、代码执行环境、搜索引擎等）的智能体。这种机制克服了 LLM 的静态知识限制，使其具备：\n",
    "\n",
    "- 获取实时数据的能力\n",
    "\n",
    "- 执行复杂计算的能力\n",
    "\n",
    "- 查询专有数据库的能力\n",
    "\n",
    "- 与外部服务交互的能力\n",
    "\n",
    "> Retriever-Reader (RAG) 属于 工具增强型智能体（Tool-augmented Agent） 的一种典型形式。\n",
    "#### 核心思想：ReAct 循环 + 工具调用\n",
    "\n",
    "**工作流程概览：**\n",
    "```mermaid\n",
    "graph TD\n",
    "  A[用户输入] --> B[LLM 判断是否需调用工具]\n",
    "  B -->|需要| C[生成工具调用请求]\n",
    "  C --> D[工具执行]\n",
    "  D --> E[返回观察结果]\n",
    "  E --> B\n",
    "  B -->|不再需要工具| F[最终回复]\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- LLM 接收用户消息，并分析是否需要调用工具。\n",
    "\n",
    "- 如果需要，生成带参数的“工具调用”。\n",
    "\n",
    "- 外部系统执行该调用，返回结果（ToolMessage）。\n",
    "\n",
    "- LLM 使用结果继续推理，直至完成任务。\n",
    "\n",
    "\n",
    "#### 用例示例\n",
    "| 类别        | 场景               |\n",
    "| --------- | ---------------- |\n",
    "| 📡 获取实时数据 | 天气、股票、新闻         |\n",
    "| 🧮 精确计算   | 数学公式、统计分析        |\n",
    "| 🔍 查询专属数据 | RAG 检索、企业数据库     |\n",
    "| 🔧 代码执行   | 编译/运行代码片段        |\n",
    "| 🌐 服务交互   | 日程管理、酒店预订、IoT 控制 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LangGraph 实现策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 状态定义（State）\n",
    "使用 TypedDict 维护消息序列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import add_messages\n",
    "\n",
    "\n",
    "class ToolAgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 工具定义（Tools）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_city_current_temperature(city: str) -> str:\n",
    "    \"\"\"获取指定城市的当前气温。\"\"\"\n",
    "    temps = {\"上海\": \"22°C\", \"伦敦\": \"15°C\"}\n",
    "    return f\"{city}目前气温为 {temps.get(city, '未知')}。\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化模型并绑定工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_city_current_temperature]\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    temperature=0.0\n",
    ").bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义 Agent 节点（agent_node）\n",
    "\n",
    "让 LLM 决策并生成 tool_calls 或直接回复："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node_func(state: ToolAgentState) -> dict:\n",
    "    result = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [result]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 定义工具执行节点（ToolNode）\n",
    "\n",
    "使用 LangGraph 的预构建节点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "tool_node = ToolNode([get_city_current_temperature])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建 Agent 流程图（StateGraph）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "workflow = StateGraph(ToolAgentState)\n",
    "\n",
    "# 添加节点\n",
    "workflow.add_node(\"agent\", agent_node_func)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# 设置入口\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# 条件边：是否调用工具\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,\n",
    "    {\n",
    "        \"tools\": \"tools\",\n",
    "        END: END\n",
    "    }\n",
    ")\n",
    "\n",
    "# 工具执行后返回给 agent\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "\n",
    "# 编译为可执行应用\n",
    "graph_app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 示例运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "initial_messages = [HumanMessage(content=\"伦敦现在多少度？\")]\n",
    "\n",
    "for event in graph_app.stream({\"messages\": initial_messages}):\n",
    "    for key, value in event.items():\n",
    "        if key == \"__end__\":\n",
    "            continue\n",
    "        last_msg = value[\"messages\"][-1]\n",
    "        if isinstance(last_msg, AIMessage) and last_msg.tool_calls:\n",
    "            print(\"🔧 调用工具:\", last_msg.tool_calls)\n",
    "        else:\n",
    "            print(\"🤖 LLM 回复:\", last_msg.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多智能体协作\n",
    "\n",
    "当单个 Agent 不足以应对任务的复杂性或广度时，多智能体协作 (Multi-Agent Collaboration) 模式便应运而生。它涉及多个 Agent 协同工作，每个 Agent 可能拥有特定的专长、角色或工具集，共同致力于达成一个宏大的目标 。   \n",
    "\n",
    "#### 分层系统（例如，主管-专员模式）\n",
    "\n",
    "分层系统，特别是主管-专员 (Supervisor-Specialist) 模式，是一种常见且有效的多 Agent 协作架构。\n",
    "\n",
    "##### **核心思想**\n",
    "核心思想：一个“主管 (Supervisor)”（或称为编排器/管理者）Agent 负责协调一组“专员 (Specialist)”（或称为工作器/专家）Agent 的工作。\n",
    "\n",
    "主管 Agent 的职责是理解用户的高级请求，将其分解为更小的、可操作的子任务，并将这些子任务委派给最合适的专员 Agent 去执行。专员 Agent 完成其子任务后，将结果返回给主管，主管再根据当前进展决定下一步行动，可能包括将新的子任务（可能利用了前一个专员的输出）分配给另一个专员，或者在所有子任务完成后综合所有专员的成果，形成最终的答复 。   \n",
    "\n",
    "##### **典型工作流程 **\n",
    "1. 用户向主管 Agent 提交一个复杂请求（例如，“帮我规划一次到巴黎的五日游，包括推荐景点、预订航班和酒店”）。\n",
    "2. 主管 Agent 分析请求，将其分解。例如，它可能首先决定需要一个“目的地推荐专员”来确定巴黎的热门景点。\n",
    "3. 主管 Agent 将“推荐巴黎景点”的任务分配给目的地推荐专员。\n",
    "4. 目的地推荐专员执行任务（可能使用其专门的工具，如旅游数据库API），并将景点列表返回给主管。\n",
    "5. 主管 Agent 收到景点列表后，可能会接着将“搜索飞往巴黎的航班”的任务分配给“航班搜索专员”，并将用户提供的日期和上一步得到的目的地信息一并传递。\n",
    "6. 航班搜索专员返回航班选项。\n",
    "7. 类似地，主管 Agent 可能再调用“酒店预订专员”。\n",
    "8. 所有必要的子任务完成后，主管 Agent 负责整合所有专员的输出（景点、航班、酒店信息），形成一个完整的旅行计划，并将其呈现给用户。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import HumanMessage\n",
    "# 假设这是您本地的或特定版本的库\n",
    "# from langgraph_supervisor import create_supervisor\n",
    "# 如果您使用的是标准库，导入方式如下：\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.tools import tool\n",
    "from langgraph_supervisor import create_supervisor\n",
    "\n",
    "# --- (可选) 设置环境变量 ---\n",
    "# 请确保在这里填入你的有效 API Key\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "\n",
    "# ========== 1. 构建工具 ==========\n",
    "# 工具函数保持不变\n",
    "@tool\n",
    "def recommend_paris_sites() -> str:\n",
    "    \"\"\"推荐巴黎的著名景点。\"\"\"\n",
    "    print(\"--- 调用工具: recommend_paris_sites ---\")\n",
    "    return \"埃菲尔铁塔、卢浮宫、凡尔赛宫、巴黎圣母院、香榭丽舍大街\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_flights(date: str, destination: str) -> str:\n",
    "    \"\"\"根据确切的日期和目的地搜索航班。\"\"\"\n",
    "    print(f\"--- 调用工具: search_flights (日期: {date}, 目的地: {destination}) ---\")\n",
    "    return f\"查询到 {date} 飞往 {destination} 的航班有：CX260, CA934, AF123。\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def search_hotels(destination: str, dates: str) -> str:\n",
    "    \"\"\"根据目的地和入住/退房日期搜索酒店。\"\"\"\n",
    "    print(f\"--- 调用工具: search_hotels (目的地: {destination}, 日期: {dates}) ---\")\n",
    "    return f\"在 {destination} 根据日期 '{dates}' 为您找到推荐酒店：巴黎香格里拉大酒店，巴黎丽兹酒店。\"\n",
    "\n",
    "\n",
    "# ========== 2. 创建专员 Agent ==========\n",
    "\n",
    "# --- 2.1 模型初始化 ---\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# --- 2.2 定义健壮的 Agent Prompts ---\n",
    "# 这是最终的、最严格的指令版本，旨在强制模型遵守规则。\n",
    "recommendation_prompt = \"你是景点推荐专员。你的唯一任务是调用 `recommend_paris_sites` 工具。直接调用，不要添加任何额外文本。\"\n",
    "\n",
    "flight_prompt = (\n",
    "    \"你是一个航班搜索专员。你的唯一任务是调用 `search_flights` 工具。\\n\"\n",
    "    \"## 规则:\\n\"\n",
    "    \"1. **搜索参数**: 你必须首先在完整的对话历史中搜索 `date` 和 `destination` 这两个参数。\\n\"\n",
    "    \"2. **调用工具**: 在找到参数后，立即调用 `search_flights` 工具。\\n\"\n",
    "    \"3. **禁止提问**: 绝对禁止向用户提问已经提供过的日期或地点信息。\\n\"\n",
    "    \"4. **最后手段**: 只有在彻底搜索后仍然找不到信息时，才能向用户提问。\\n\"\n",
    "    \"## 示例:\\n\"\n",
    "    \"如果用户说：'...去巴黎，时间是2025年10月22日...'\\n\"\n",
    "    \"你就必须用这些信息调用工具，而不是提问。\"\n",
    ")\n",
    "\n",
    "hotel_prompt = (\n",
    "    \"你是一个酒店预订专员。你的唯一任务是调用 `search_hotels` 工具。\\n\"\n",
    "    \"## 规则:\\n\"\n",
    "    \"1. **搜索参数**: 你必须首先在完整的对话历史中搜索 `destination` 和 `dates` 这两个参数。\\n\"\n",
    "    \"2. **调用工具**: 在找到参数后，立即调用 `search_hotels` 工具。\\n\"\n",
    "    \"3. **禁止提问**: 绝对禁止向用户提问已经提供过的日期或地点信息。\\n\"\n",
    "    \"4. **最后手段**: 只有在彻底搜索后仍然找不到信息时，才能向用户提问。\\n\"\n",
    "    \"## 示例:\\n\"\n",
    "    \"如果用户说：'...去巴黎，时间是2025年10月22日...'\\n\"\n",
    "    \"你就必须用这些信息调用工具，而不是提问。\"\n",
    ")\n",
    "\n",
    "# --- 2.3 创建 Agent ---\n",
    "# 注意：标准的 create_react_agent 使用 `messages_modifier` 参数来传递 prompt。\n",
    "recommendation_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[recommend_paris_sites],\n",
    "    name=\"recommendation_agent\",\n",
    "    prompt=recommendation_prompt\n",
    ")\n",
    "\n",
    "flight_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[search_flights],\n",
    "    name=\"flight_agent\",\n",
    "    prompt=flight_prompt\n",
    ")\n",
    "\n",
    "hotel_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[search_hotels],\n",
    "    name=\"hotel_agent\",\n",
    "    prompt=hotel_prompt\n",
    ")\n",
    "\n",
    "# ========== 3. 创建主管 Agent ==========\n",
    "# 使用最严格、最程序化的指令来约束 Supervisor\n",
    "supervisor_prompt = (\n",
    "    \"你是一个路由选择器。你的任务是分析对话历史，然后严格按照下面的规则输出一个单词，作为下一步的指令。你绝对不能输出任何其他内容。\\n\\n\"\n",
    "    \"## 规则 ##\\n\"\n",
    "    \"1. 读取对话历史。\\n\"\n",
    "    \"2. 检查 ToolMessage 中是否包含 `recommend_paris_sites` 的调用结果。如果没有，你的输出必须是 `recommendation_agent`。\\n\"\n",
    "    \"3. 如果上一步完成，检查 ToolMessage 中是否包含 `search_flights` 的调用结果。如果没有，你的输出必须是 `flight_agent`。\\n\"\n",
    "    \"4. 如果上一步完成，检查 ToolMessage 中是否包含 `search_hotels` 的调用结果。如果没有，你的输出必须是 `hotel_agent`。\\n\"\n",
    "    \"5. 如果所有工具都已调用，你的输出必须是 `FINISH`。\\n\\n\"\n",
    "    \"## 输出格式 ##\\n\"\n",
    "    \"你的输出必须严格为以下单词之一，不包含任何修饰、代码块或解释：\\n\"\n",
    "    \"- recommendation_agent\\n\"\n",
    "    \"- flight_agent\\n\"\n",
    "    \"- hotel_agent\\n\"\n",
    "    \"- FINISH\"\n",
    ")\n",
    "\n",
    "supervisor = create_supervisor(\n",
    "    agents=[recommendation_agent, flight_agent, hotel_agent],\n",
    "    model=model,\n",
    "    prompt=supervisor_prompt,\n",
    ").compile()\n",
    "\n",
    "# ========== 4. 测试运行 ==========\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 使用包含所有信息的初始输入来测试\n",
    "    initial_input = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"你好，请帮我规划一次到巴黎的五日游，时间是2025年10月22日-2025年10月27日，我想知道推荐去哪些景点，以及航班和酒店信息。\")\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 使用 stream 模式来观察每一步的执行过程\n",
    "    for event in supervisor.stream(initial_input, {\"recursion_limit\": 15}):\n",
    "        for key, value in event.items():\n",
    "            print(f\"--- Event: Node '{key}' ---\")\n",
    "            if 'messages' in value:\n",
    "                 # 为了更清晰的展示，只打印 message 的内容\n",
    "                 for msg in value['messages']:\n",
    "                      print(f\"    {type(msg).__name__}: {getattr(msg, 'content', 'No content')}\")\n",
    "            else:\n",
    "                  print(value)\n",
    "        print(\"\\n========================================\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 注意提示词很重要，没用好，直接g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 去中心化/集群架构\n",
    "与分层结构相对的是更为灵活的去中心化或集群 (Swarm) 架构。\n",
    "\n",
    "\n",
    "**核心思想**：在此类架构中，Agent 根据其专业能力和当前任务上下文动态地将控制权相互传递，不一定存在一个中心化的主管来统一调度。系统可能会记录最后一个活动的 Agent，以便在后续交互中从该点恢复对话 。这种模式允许更具流动性和涌现性的协作方式。   \n",
    "\n",
    "**典型工作流程**：Agent A 处理一个请求。根据其处理结果，它判断 Agent B 更适合执行下一步。于是，Agent A 将控制权（以及相关数据）直接“交接”给 Agent B。Agent B 完成其任务后，可能会将控制权交接给 Agent C，或者交还给 Agent A，或者直接结束任务。整个流程的路径并非预先固定，而是由各个 Agent 在执行过程中动态决定的。\n",
    "\n",
    "**用例**：\n",
    "- 控制流不确定性较高，高度依赖于中间结果的场景。\n",
    "- 专业知识分布在多个对等 Agent 中的协作式问题解决。\n",
    "- 需要高度自适应和容错性的系统，其中一个 Agent 的失败可以由其他 Agent 接管。\n",
    "\n",
    "详见[群体模式 (Swarm Pattern)](#群体模式 (Swarm Pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, Any\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "\n",
    "# --- 1. 设置模型 ---\n",
    "# 请确保您已经设置了 API 密钥\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# 如果没有有效的key，以下代码会报错\n",
    "# 创建一个模型实例\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.0 # 温度设为0，让模型输出更稳定\n",
    ")\n",
    "\n",
    "# --- 2. 定义智能体的专属工具 ---\n",
    "def solve_technical_issue(issue: str) -> str:\n",
    "    \"\"\"用于解决具体的技术问题。\"\"\"\n",
    "    print(f\"--- 正在调用【技术专家工具】：解决 '{issue}' ---\")\n",
    "    if \"卡顿\" in issue or \"慢\" in issue:\n",
    "        return \"技术诊断结果：检测到网络波动，建议您重启路由器后再次尝试。\"\n",
    "    else:\n",
    "        return \"技术诊断结果：这是一个未知问题，请提供更多细节。\"\n",
    "\n",
    "# --- 3. 创建两个不同角色的智能体（Agent） ---\n",
    "\n",
    "# 智能体 1: 客服小李 (普通客服)\n",
    "# 他没有技术工具，但知道可以将技术问题“转接”给王工\n",
    "li_agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[\n",
    "        create_handoff_tool(\n",
    "            agent_name=\"tech_expert_wang\", # 已修改: 使用英文名称\n",
    "            description=\"当遇到任何技术或网络问题时，将对话转接给'技术王工'处理。\"\n",
    "        )\n",
    "    ],\n",
    "    prompt=\"你是客服小李，一个彬彬有礼的客服代表。你的任务是回答用户的常规问题。如果遇到无法解决的技术问题，就转接给技术王工。\",\n",
    "    name=\"customer_service_li\", # 已修改: 使用符合规范的英文名称\n",
    ")\n",
    "\n",
    "# 智能体 2: 技术王工 (技术专家)\n",
    "# 他拥有解决技术问题的专属工具，也可以把非技术问题“转接”回给小李\n",
    "wang_agent = create_react_agent(\n",
    "    model,\n",
    "    tools=[\n",
    "        solve_technical_issue, # 王工独有的技术工具\n",
    "        create_handoff_tool(\n",
    "            agent_name=\"customer_service_li\", # 注意使用英文名称\n",
    "            description=\"当用户提出的问题与技术无关时（例如问候、常规咨询等），将对话转接回'客服小李'。\"\n",
    "        )\n",
    "    ],\n",
    "    prompt=\"你是技术王工，一位专业的网络工程师。你的回答必须简洁、专业，只专注于解决技术问题。\",\n",
    "    name=\"tech_expert_wang\", # 已修改: 使用符合规范的英文名称\n",
    ")\n",
    "\n",
    "# --- 4. 创建智能体群组（Swarm）---\n",
    "# Swarm 会像一个路由器，根据对话内容，自动将任务分配给最合适的智能体\n",
    "workflow = create_swarm(\n",
    "    agents=[li_agent, wang_agent],\n",
    "    default_active_agent=\"customer_service_li\" # 注意使用英文名称\n",
    ")\n",
    "\n",
    "# 编译成一个可执行的应用\n",
    "checkpointer = InMemorySaver() # 使用内存记录对话状态\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "# --- 5. 开始与智能客服团队对话 ---\n",
    "# 我们需要为每一次独立的对话指定一个唯一的 thread_id\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation_1\"}}\n",
    "\n",
    "print(\"--- 对话开始 ---\")\n",
    "\n",
    "# 第一个问题：一个技术问题\n",
    "print(\"\\n[用户]: 你好，我的网络很卡顿，看视频总是转圈圈。\")\n",
    "response_1 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"你好，我的网络很卡顿，看视频总是转圈圈。\"}]},\n",
    "    config,\n",
    ")\n",
    "print(f\"\\n[智能客服团队]: {response_1}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "\n",
    "# 第二个问题：一个常规问题\n",
    "print(\"\\n[用户]: 谢谢！问题解决了。顺便问一下，你们的上班时间是？\")\n",
    "response_2 = app.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"谢谢！问题解决了。顺便问一下，你们的上班时间是？\"}]},\n",
    "    config,\n",
    ")\n",
    "print(f\"\\n[智能客服团队]: {response_2}\")\n",
    "print(\"--- 对话结束 ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Human-in-the-Loop (HITL)\t\n",
    "\n",
    "#### 什么是“人在回路”？\n",
    "“人在回路”是在 Agent 的自动化流程中设置“暂停点”的一种机制。当 Agent 的执行到达这些预设的节点时，流程会暂停，将控制权移交给人类用户。这允许用户进行以下操作：\n",
    "\n",
    "- **审查与批准**：确认关键操作，例如执行会产生费用或修改数据库的工具。\n",
    "\n",
    "- **修正与编辑**：修改 Agent 生成的内容，例如一封重要的邮件草稿。\n",
    "\n",
    "- **提供帮助**：当 Agent 遇到无法独立解决的模糊问题时，为其提供必要的澄清或额外信息。\n",
    "\n",
    "集成 HIL 是构建更可靠、更安全、更智能 Agent 的关键一步。\n",
    "\n",
    "#### 核心能力\n",
    "LangGraph 通过其独特的设计，为实现强大的 HIL 功能提供了坚实的基础：\n",
    "\n",
    "- **持久化的执行状态 (Persistent Execution State)**：LangGraph 的检查点（Checkpoint）机制会在每一步之后保存图（Graph）的当前状态。这使得图的执行可以在任何被标记为 HIL 断点的节点无限期地暂停。这种能力支持异步的人工审查或输入，不受实时交互的时间限制。\n",
    "\n",
    "- **灵活的集成点 (Flexible Integration Points)**：HIL 逻辑可以被引入到工作流中的任何位置。这允许开发者在最需要人工参与的地方进行有针对性的人工干预，例如在执行敏感的 API 调用前进行审批，修正 LLM 生成的内容，或者让 LLM 在遇到困惑时明确请求人工澄清。\n",
    "\n",
    "\n",
    "#### 典型用例\n",
    "- **审查工具调用 (Reviewing Tool Calls)**：在 LLM 请求执行某个工具（尤其是可能产生副作用或成本的工具）之前，由人工审查、编辑或批准该工具调用。\n",
    "\n",
    "- **验证 LLM 输出 (Validating LLM Outputs)**：由人工审查、编辑或批准 LLM 生成的内容，例如重要的报告、邮件草稿或关键决策建议。\n",
    "\n",
    "- **提供上下文或澄清 (Providing Context)**：允许 LLM 主动请求人工输入以获取澄清、补充缺失信息，或支持更自然的多轮对话。\n",
    "\n",
    "#### 实现机制与工作流程\n",
    "\n",
    "LangGraph 主要通过 interrupt 函数和状态更新来实现 HIL。\n",
    "\n",
    "1. 核心函数：interrupt()\n",
    "\n",
    "这是实现 HIL 的关键。在一个工具函数或图节点内部调用 interrupt() 会导致图的执行在该点暂停。此时，图的当前状态会被完整保存。\n",
    "\n",
    "2. 恢复执行\n",
    "\n",
    "当人工审查完成并准备好提供输入以恢复执行时，可以通过向图的状态（State）中添加 HumanMessage 或更新特定字段，然后再次调用图来恢复执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工作流程示例：\n",
    "\n",
    "1. Agent 执行到一个被设计为 HIL 的节点（例如，调用了一个内部使用了 interrupt 的工具）。\n",
    "\n",
    "2. 图的执行暂停，当前状态被持久化。\n",
    "\n",
    "3. 外部系统（例如一个用户界面）向人类用户展示需要审查的内容（例如，需要审批的工具调用参数，或 LLM 请求澄清的问题）。\n",
    "\n",
    "4. 人类用户进行审查，并提供必要的输入（例如，批准/拒绝，修改后的参数，或对问题的回答）。\n",
    "\n",
    "5. 应用程序将人类的输入包装成消息，然后再次调用图的 stream 或 invoke 方法来恢复执行。\n",
    "\n",
    "#### 代码示例：需要人工审批的搜索工具\n",
    "下面的 Python 代码演示了如何创建一个简单的 Agent，该 Agent 在执行搜索前，如果问题模糊，会暂停并请求人工澄清。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# -- 核心依赖 --\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# -- LangGraph 相关导入 --\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# 1. 定义工具\n",
    "# 这次我们只需要一个实际工作的工具，不再需要特殊的 HIL 工具。\n",
    "@tool\n",
    "def search_tool(query: str):\n",
    "    \"\"\"一个模拟的搜索工具，用于查找信息。\"\"\"\n",
    "    print(f\"\\n--- ✅ 工具被执行: search_tool, 查询: '{query}' ---\")\n",
    "    if \"human-in-the-loop\" in query.lower() or \"人在回路\" in query:\n",
    "        return \"LangGraph 使用 `interrupt_before` 参数在执行节点前设置断点，以实现“人在回路”审批流程。\"\n",
    "    return \"未找到相关信息。\"\n",
    "\n",
    "\n",
    "tools = [search_tool]\n",
    "tool_node = ToolNode(tools)\n",
    "\n",
    "# 2. 设置模型\n",
    "# 我们将模型与工具绑定，以便它能生成工具调用。\n",
    "# -- 使用 DeepSeek 模型 --\n",
    "# 请确保您已在环境中设置了 DEEPSEEK_API_KEY，或者直接提供密钥\n",
    "# os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-...\"\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.0\n",
    ")\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "\n",
    "\n",
    "# 3. 定义图的节点和逻辑\n",
    "# `call_model` 节点: 调用 LLM 生成回应或工具调用。\n",
    "def call_model(state: MessagesState):\n",
    "    \"\"\"调用 LLM。它的输出会被添加到消息列表中。\"\"\"\n",
    "    print(\"--- 🗣️ 调用 LLM ---\")\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# `should_continue` 决定下一步走向: 结束还是调用工具。\n",
    "def should_continue(state: MessagesState):\n",
    "    \"\"\"检查 LLM 的最后一条消息是否包含工具调用。\"\"\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "# 4. 构建图并设置断点\n",
    "# 我们将手动构建一个 ReAct Agent 流程。\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# 添加节点\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "\n",
    "# 定义边的逻辑\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"action\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "\n",
    "# 5. 编译图并添加检查点和断点\n",
    "# 这是关键步骤！\n",
    "# - `checkpointer`: 用于保存图的状态，使得中断和恢复成为可能。\n",
    "# - `interrupt_before=[\"action\"]`: 在每次执行 'action' 节点（即工具调用）之前暂停。\n",
    "memory = MemorySaver()\n",
    "graph = workflow.compile(\n",
    "    checkpointer=memory,\n",
    "    interrupt_before=[\"action\"]  # 在调用工具前中断\n",
    ")\n",
    "\n",
    "\n",
    "# 可视化图的结构\n",
    "# display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "\n",
    "# 6. 运行带有审批流程的 Agent\n",
    "def run_interactive_agent():\n",
    "    # 为每次运行创建一个独立的线程\n",
    "    config = {\"configurable\": {\"thread_id\": \"hil-thread-2\"}}  # 使用新的线程ID以避免状态冲突\n",
    "    user_input = \"给我找找 LangGraph 中关于“人在回路”的资料\"\n",
    "\n",
    "    print(f\"--- 🚀 开始执行, 用户输入: '{user_input}' ---\")\n",
    "\n",
    "    # 首次运行，图会在调用工具前停止\n",
    "    events = graph.stream(\n",
    "        {\"messages\": [HumanMessage(content=user_input)]}, config=config\n",
    "    )\n",
    "\n",
    "    for event in events:\n",
    "        if \"messages\" in event:\n",
    "            event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "    # 此时图已暂停，等待人工干预\n",
    "    print(\"\\n--- ⏸️ Agent 已暂停 ---\")\n",
    "    last_state = graph.get_state(config)\n",
    "\n",
    "    # 检查是否存在工具调用\n",
    "    if not last_state.values['messages'][-1].tool_calls:\n",
    "        print(\"Agent 没有请求调用工具，执行结束。\")\n",
    "        return\n",
    "\n",
    "    pending_tool_call = last_state.values['messages'][-1].tool_calls[0]\n",
    "    print(f\"Agent 准备执行以下工具: {pending_tool_call['name']}\")\n",
    "    print(f\"参数: {pending_tool_call['args']}\")\n",
    "\n",
    "    # 获取用户批准\n",
    "    try:\n",
    "        user_approval = input(\"\\n是否批准执行此操作? (yes/no): \")\n",
    "    except EOFError:\n",
    "        user_approval = \"yes\"  # 在非交互式环境中默认为 'yes'\n",
    "\n",
    "    if user_approval.lower() == \"yes\":\n",
    "        print(\"\\n--- ▶️ 用户已批准, 继续执行 ---\")\n",
    "        # 传入 None 来从中断处恢复执行\n",
    "        resumed_events = graph.stream(None, config=config)\n",
    "        for event in resumed_events:\n",
    "            if \"messages\" in event:\n",
    "                event[\"messages\"][-1].pretty_print()\n",
    "    else:\n",
    "        print(\"\\n--- ❌ 操作已被用户取消 ---\")\n",
    "\n",
    "\n",
    "# 执行交互式流程\n",
    "run_interactive_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HIL 与检查点：实现真正的异步人工干预\n",
    "LangGraph 的 HIL 功能与检查点机制的结合，使得真正意义上的异步人工干预成为可能。这与那些依赖实时人工在线的解决方案（例如基于 WebSocket 的 HIL）相比，是一个显著的优势。\n",
    "\n",
    "许多关键的业务流程都包含非即时发生的人工审批环节。例如，一位经理可能需要批准 Agent 提出的某项财务交易，但他可能每天只在固定时间检查审批队列。LangGraph 能够无限期暂停执行（因为状态已被持久化），并在人工最终提供输入时无缝恢复，这使其非常适合这类现实世界中的异步工作流。Agent 并非在一个活动进程中“卡住等待”，而是处于一种可以随时被“重新激活”的暂停状态。\n",
    "\n",
    "#### 超越批准：作为协作工具的 HIL\n",
    "在 LangGraph 中，HIL 不仅仅是作为最终输出的批准门槛，它更可以成为人与 Agent 之间进行协作式问题解决的强大工具，尤其是在复杂的模式如“计划-执行”或“反思”循环中。\n",
    "\n",
    "- **在“计划-执行”循环中**：可以在 replanner_node（重新计划节点）执行之后加入一个 HIL 步骤，允许人工在下一轮执行开始前审查并可能修改 LLM 更新后的计划。人类用户可能会发现 LLM 忽略的缺陷，或提出一个更优的执行路径。\n",
    "\n",
    "- **在“反思”循环中**：人工可以在 reflector_node（反思节点）产生的批判意见被反馈给 generator_node（生成节点）之前，对其进行审查和编辑，以提高批判的清晰度和有效性。\n",
    "\n",
    "这种设计使得 HIL 能够在 Agent 推理周期的多个关键杠杆点注入人类的专业知识和判断力，而不仅仅是充当一个最终的“守门员”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting / Debate Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 什么是“投票/辩论 Agent”？\n",
    "\n",
    "“投票/辩论 Agent”是一种多 Agent 协作模式。它不是依赖单个 Agent 来解决问题，而是让多个“工作 Agent”（Worker Agents）并行地对同一个问题进行分析和回答。然后，通过一个“聚合器”（Aggregator）或“裁判 Agent”（Resolver Agent）来处理这些并行的输出，最终形成一个更全面、更可靠的结论。\n",
    "\n",
    "这个过程可以像：\n",
    "\n",
    "- **投票 (Voting)**：聚合器选择出现次数最多或置信度最高的答案。\n",
    "\n",
    "- **辩论 (Debate)**：每个工作 Agent 被赋予一个特定的角色或立场（例如，“支持方”和“反对方”）。“裁判 Agent”则负责分析它们的论点，并给出一个综合了各方观点的、更加平衡和深入的最终答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么这种模式很有用？\n",
    "- **提升鲁棒性**：通过综合多个来源的观点，可以有效减少单个模型可能出现的“幻觉”或片面性。\n",
    "\n",
    "- **激发深度思考**：模拟辩论过程可以迫使模型从多个对立的角度深入探讨一个复杂问题，从而产生更有深度的见解。\n",
    "\n",
    "- **增强创造力**：可以让不同的 Agent 专注于一个创意的不同方面（如情节、角色、世界观），然后将它们融合在一起。\n",
    "\n",
    "\n",
    "#### 实现思路\n",
    "1. **定义工作 Agent**：创建多个具有不同职责或系统提示（System Prompt）的 Agent。在我们的辩论示例中，就是一个“支持方 Agent”和一个“反对方 Agent”。\n",
    "\n",
    "2. **并行执行**：LangGraph 的一个核心优势是能够轻松地将任务分发给多个节点并行处理。我们会将同一个初始问题分发给所有工作 Agent。\n",
    "\n",
    "3. **聚合结果**：创建一个最终的“裁判”节点。这个节点会等待所有工作 Agent 完成，收集它们的全部输出。\n",
    "\n",
    "4. **最终裁决**：“裁判”节点（通常也是一个 LLM）会分析收到的所有观点，并根据预设的指令（例如，“综合以下正反两方观点，给出一个中立的总结”）生成最终的、高质量的答案。\n",
    "\n",
    "接下来，我们将通过一个具体的代码示例来构建一个“辩论 Agent”，它将就“人工智能对就业的未来影响”这一主题展开辩论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "import operator\n",
    "\n",
    "# --- 模型设置 ---\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.7  # 稍微提高温度以增加辩论的多样性\n",
    ")\n",
    "\n",
    "\n",
    "# 1. 定义图的状态 (State)\n",
    "class DebateState(TypedDict):\n",
    "    # 初始辩题\n",
    "    topic: str\n",
    "    # 跟踪所有消息，包括辩论过程和最终总结\n",
    "    messages: Annotated[list, operator.add]\n",
    "    # 每个辩手（worker）的论点\n",
    "    # 修正: 使用 Annotated 来告诉 LangGraph 如何合并并行节点的输出\n",
    "    arguments: Annotated[list, operator.add]\n",
    "\n",
    "\n",
    "# 2. 定义工作 Agent (辩手)\n",
    "# 这是一个可以被不同角色（系统提示）复用的函数\n",
    "def debate_worker(state: DebateState, role_prompt: str):\n",
    "    \"\"\"一个辩手，根据指定的角色对主题进行论证。\"\"\"\n",
    "\n",
    "    # 构造向 LLM 提问的消息\n",
    "    messages = [\n",
    "        SystemMessage(content=role_prompt),\n",
    "        HumanMessage(content=f\"辩题是: '{state['topic']}'. 请陈述你的论点。\")\n",
    "    ]\n",
    "\n",
    "    # 调用 LLM\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    # 在终端打印每个辩手的论点，方便观察\n",
    "    print(f\"--- {role_prompt.split('：')[0]} ---\")\n",
    "    print(response.content)\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 返回的列表会被自动追加到 State 的 arguments 字段中\n",
    "    return {\"arguments\": [response.content]}\n",
    "\n",
    "\n",
    "# 3. 定义聚合器 (裁判)\n",
    "def resolver(state: DebateState):\n",
    "    \"\"\"裁判，负责总结所有辩手的论点。\"\"\"\n",
    "\n",
    "    # 将所有收集到的论点格式化\n",
    "    all_arguments = \"\\n\\n\".join(\n",
    "        [f\"第 {i + 1} 方论点:\\n{arg}\" for i, arg in enumerate(state[\"arguments\"])]\n",
    "    )\n",
    "\n",
    "    # 构造裁判的提示\n",
    "    resolver_prompt = f\"\"\"你是一位公正的裁判。你的任务是基于以下几方的论点，对辩题“{state['topic']}”给出一个全面、中立且有深度的总结。请不要偏袒任何一方，而是客观地分析并综合所有观点。\n",
    "\n",
    "以下是各方论点:\n",
    "{all_arguments}\n",
    "\n",
    "请开始你的总结:\n",
    "\"\"\"\n",
    "\n",
    "    # 调用 LLM\n",
    "    messages = [SystemMessage(content=resolver_prompt)]\n",
    "    final_response = model.invoke(messages)\n",
    "\n",
    "    return {\"messages\": [final_response.content]}\n",
    "\n",
    "\n",
    "# 4. 构建图\n",
    "# 定义不同的角色\n",
    "PRO_PROMPT = \"你是支持方辩手：你的任务是清晰、有力地论证“人工智能（AI）将主要为就业市场带来积极影响”。请从创造新岗位、提高生产力、改善工作质量等方面展开。\"\n",
    "CON_PROMPT = \"你是反对方辩手：你的任务是清晰、有力地论证“人工智能（AI）将主要为就业市场带来负面影响”。请从岗位替代、技能要求提升、加剧不平等等方面展开。\"\n",
    "\n",
    "# 使用 StateGraph\n",
    "builder = StateGraph(DebateState)\n",
    "\n",
    "# 为每个辩手创建节点\n",
    "# 我们使用 .with_config() 来为同一个函数传入不同的角色配置\n",
    "builder.add_node(\"pro_debater\", lambda state: debate_worker(state, PRO_PROMPT))\n",
    "builder.add_node(\"con_debater\", lambda state: debate_worker(state, CON_PROMPT))\n",
    "# 创建裁判节点\n",
    "builder.add_node(\"resolver\", resolver)\n",
    "\n",
    "# 定义图的流程\n",
    "# 从起点并行地将任务分发给两个辩手\n",
    "builder.add_edge(\"pro_debater\", \"resolver\")\n",
    "builder.add_edge(\"con_debater\", \"resolver\")\n",
    "\n",
    "# 设置入口点\n",
    "# langgraph 会自动将 \"pro_debater\" 和 \"con_debater\" 作为并行执行的起点\n",
    "builder.set_entry_point(\"pro_debater\")\n",
    "builder.set_entry_point(\"con_debater\")\n",
    "\n",
    "# 裁判节点是辩论的终点\n",
    "builder.add_edge(\"resolver\", END)\n",
    "\n",
    "# 编译图\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. 运行图\n",
    "topic = \"人工智能对未来就业的总体影响是积极的还是消极的？\"\n",
    "initial_state = {\"topic\": topic, \"messages\": [], \"arguments\": []}\n",
    "\n",
    "# 运行并获取最终状态\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "# 打印最终的裁判总结\n",
    "print(\"\\n--- ⚖️ 裁判最终总结 ⚖️ ---\")\n",
    "print(final_state[\"messages\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory-Augmented\n",
    "\n",
    "#### 什么是“记忆增强型 Agent”？\n",
    "\n",
    "想象一下你在写一篇长篇报告。你不会一口气从头写到尾，而是会写一段，然后停下来，回顾一下已经写了什么，思考一下接下来要写什么，确保上下文衔接流畅。\n",
    "\n",
    "“记忆增强型 Agent”就是模仿这个过程。它在标准 Agent 的“思考-行动”循环中，增加了一个关键的 “反思” (Reflection) 环节。\n",
    "\n",
    "1. **生成 (Generation)**：Agent 根据当前任务和上下文，生成一部分内容或执行一个操作。\n",
    "\n",
    "2. **反思 (Reflection)**：一个专门的“反思者” (Reflector) 会审视刚刚生成的内容和整个历史记录，然后提炼出一个简短的“记忆”或“反思”。这个反思可能包括：“目前的情节发展到了高潮，下一段需要开始揭示谜底了”或者“我们已经收集了 A 和 B 两方面的资料，现在需要寻找 C 方面的证据来形成闭环”。\n",
    "\n",
    "3. **再次生成**：这个新生成的“反思”会作为关键的上下文信息，被送回给主 Agent，指导它进行下一步的生成。\n",
    "\n",
    "这个“**生成 -> 反思 -> 生成**”的循环，就是记忆增强模式的核心。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 为什么这种模式如此强大？\n",
    "- **提升连贯性**：通过不断回顾和总结，Agent 能够始终保持对任务全局目标的认知，避免在多步骤执行中“迷失方向”。\n",
    "\n",
    "- **实现自我修正**：如果反思者发现之前的路径有误，它可以在“记忆”中提出修正建议，引导 Agent 在下一步纠正错误。\n",
    "\n",
    "- **解决复杂问题**：对于需要逐步推理、层层递进的复杂任务（如代码生成、科学推演、小说创作），这种模式几乎是必需的。它为 Agent 提供了一个“工作记忆区”来组织思路。\n",
    "\n",
    "#### 实现思路\n",
    "\n",
    "1. **定义状态 (State)**：在图的状态中，除了常规的 messages 列表，我们还需要一个专门的字段来存放“反思”的结果，例如 reflection: str。\n",
    "\n",
    "2. **创建生成节点**：这个节点负责执行核心任务（如写一段文字）。它接收的输入不仅包括历史消息，还包括来自上一步的“反思”。\n",
    "\n",
    "3. **创建反思节点**：这个节点接收整个执行历史，并调用 LLM 来生成一段新的“反思”，然后更新状态中的 reflection 字段。\n",
    "\n",
    "4. **构建循环**：将图的边连接成一个循环：生成节点 -> 反思节点 -> 生成节点。\n",
    "\n",
    "5. **设置终止条件**：在循环中设置一个条件，判断任务是否已经完成（例如，循环达到一定次数，或生成内容中包含特定结束语），然后将流程导向 END。\n",
    "\n",
    "\n",
    "接下来，我们将通过一个代码示例，构建一个“记忆增强型写作 Agent”，让它一步步地创作一个简短的科幻故事。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# --- 模型设置 ---\n",
    "# 我们继续使用 DeepSeek 模型\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.8  # 写作需要一些创造性，稍微提高温度\n",
    ")\n",
    "\n",
    "\n",
    "# 1. 定义图的状态 (State)\n",
    "# 我们需要跟踪故事的片段、当前的反思以及迭代次数\n",
    "class WritingState(TypedDict):\n",
    "    # 初始写作主题\n",
    "    topic: str\n",
    "    # 故事的片段列表\n",
    "    story_fragments: Annotated[list, operator.add]\n",
    "    # 当前的反思\n",
    "    reflection: str\n",
    "    # 迭代次数\n",
    "    iteration: int\n",
    "\n",
    "\n",
    "# 2. 定义图的节点\n",
    "\n",
    "# --- 生成节点 (Writer) ---\n",
    "def writer_node(state: WritingState):\n",
    "    \"\"\"根据当前反思和故事历史，生成下一个故事片段。\"\"\"\n",
    "    print(f\"\\n--- ✍️ 迭代 {state['iteration']}: 写作中... ---\")\n",
    "\n",
    "    # 构建作者的提示\n",
    "    # 关键点：将上一步的“反思”作为重要的上下文输入\n",
    "    prompt = f\"\"\"你是一位科幻小说家。你正在根据以下主题创作一个短篇故事。\n",
    "主题: {state['topic']}\n",
    "\n",
    "目前的故事情节:\n",
    "{''.join(state['story_fragments'])}\n",
    "\n",
    "---\n",
    "来自你的文学编辑（反思者）的最新指导:\n",
    "{state['reflection']}\n",
    "---\n",
    "\n",
    "请根据编辑的指导，写出故事的下一个段落。请只输出新的段落内容，不要重复之前的情节。\n",
    "\"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    print(\"新的段落:\", response.content)\n",
    "\n",
    "    # 更新状态\n",
    "    return {\n",
    "        \"story_fragments\": [response.content],\n",
    "        \"iteration\": state[\"iteration\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 反思节点 (Reflector) ---\n",
    "def reflector_node(state: WritingState):\n",
    "    \"\"\"阅读整个故事，并为下一步的写作提供指导。\"\"\"\n",
    "    print(f\"--- 🤔 迭代 {state['iteration']}: 反思中... ---\")\n",
    "\n",
    "    # 构建反思者的提示\n",
    "    prompt = f\"\"\"你是一位经验丰富的文学编辑。你正在审阅一部关于“{state['topic']}”的短篇科幻小说。\n",
    "目前的完整草稿如下:\n",
    "---\n",
    "{''.join(state['story_fragments'])}\n",
    "---\n",
    "\n",
    "请基于当前内容，为作者提供清晰、简洁的指导，告诉他下一段应该写什么。\n",
    "你的指导应该包括：\n",
    "1. 对当前情节的简要总结。\n",
    "2. 对下一段情节发展的具体建议（例如，引入一个新角色，制造一个冲突，或者揭示一个秘密）。\n",
    "请直接输出指导意见。\n",
    "\"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    response = model.invoke(messages)\n",
    "\n",
    "    print(\"新的反思:\", response.content)\n",
    "\n",
    "    return {\"reflection\": response.content}\n",
    "\n",
    "\n",
    "# 3. 定义边的逻辑 (终止条件)\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "\n",
    "def should_continue(state: WritingState):\n",
    "    \"\"\"判断是否继续循环。\"\"\"\n",
    "    if state[\"iteration\"] >= MAX_ITERATIONS:\n",
    "        return \"end\"\n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "# 4. 构建图\n",
    "builder = StateGraph(WritingState)\n",
    "\n",
    "# 添加节点\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_node(\"reflector\", reflector_node)\n",
    "\n",
    "# 定义图的流程\n",
    "builder.set_entry_point(\"writer\")\n",
    "\n",
    "# 添加循环边\n",
    "builder.add_edge(\"writer\", \"reflector\")\n",
    "builder.add_conditional_edges(\n",
    "    \"reflector\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"writer\",\n",
    "        \"end\": END,\n",
    "    },\n",
    ")\n",
    "\n",
    "# 编译图\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. 运行图\n",
    "topic = \"一个孤独的灯塔看守员，发现他的灯塔其实是一艘伪装成灯塔的古老外星飞船的控制室。\"\n",
    "# 初始状态，给出一个初始的反思来启动流程\n",
    "initial_state = {\n",
    "    \"topic\": topic,\n",
    "    \"story_fragments\": [],\n",
    "    \"reflection\": \"故事刚刚开始。请先描写主角的日常生活和他对灯塔的感情，为后续的惊人发现做铺垫。\",\n",
    "    \"iteration\": 0,\n",
    "}\n",
    "\n",
    "# 运行并打印最终结果\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- 📖 故事最终完成 📖 ---\")\n",
    "print(''.join(final_state['story_fragments']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进一步加强，增加批评，是否决定重写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "# --- 模型设置 ---\n",
    "\n",
    "# 修正: 创建两个独立的模型实例以解决 API 错误\n",
    "# 1. 用于写作的模型，输出纯文本\n",
    "writer_model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.8,\n",
    ")\n",
    "# 2. 用于批判的模型，强制输出 JSON\n",
    "critic_model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.0,  # 批判时需要更稳定的输出\n",
    "    # 要求模型输出 JSON\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "\n",
    "# 1. 定义图的状态 (State)\n",
    "# 我们新增了一个 'decision' 字段来存储评论家的决定\n",
    "class WritingState(TypedDict):\n",
    "    topic: str\n",
    "    # 我们将手动管理列表，不再使用 operator.add，以便于删除不满意的片段\n",
    "    story_fragments: list\n",
    "    feedback: str\n",
    "    iteration: int\n",
    "    # 评论家的决定: 'accept' 或 'revise'\n",
    "    decision: str\n",
    "\n",
    "\n",
    "# 2. 定义图的节点\n",
    "\n",
    "# --- 生成节点 (Writer) ---\n",
    "def writer_node(state: WritingState):\n",
    "    \"\"\"根据反馈，生成或重写故事片段。\"\"\"\n",
    "    print(f\"\\n--- ✍️ 迭代 {state['iteration']}: 写作中... ---\")\n",
    "\n",
    "    # 修改提示，使其能同时处理“新段落指导”和“重写要求”\n",
    "    prompt = f\"\"\"你是一位科幻小说家。你正在根据以下主题创作一个短篇故事。\n",
    "主题: {state['topic']}\n",
    "\n",
    "目前的故事情节:\n",
    "{''.join(state['story_fragments'])}\n",
    "\n",
    "---\n",
    "来自你的文学评论家（反思者）的最新反馈:\n",
    "{state['feedback']}\n",
    "---\n",
    "\n",
    "请根据评论家的反馈，写出故事的下一个段落（或重写不满意的段落）。请只输出新的段落内容，不要包含任何额外的前缀或 JSON 格式。\n",
    "\"\"\"\n",
    "\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    # 修正: 使用专门的 writer_model\n",
    "    response = writer_model.invoke(messages)\n",
    "\n",
    "    # 修正: 不再需要解析 JSON\n",
    "    new_content = response.content\n",
    "\n",
    "    print(\"新生成的段落:\", new_content)\n",
    "\n",
    "    # 手动更新 story_fragments 列表\n",
    "    new_fragments = state.get(\"story_fragments\", []) + [new_content]\n",
    "    return {\"story_fragments\": new_fragments}\n",
    "\n",
    "\n",
    "# --- 批判节点 (Critic) ---\n",
    "def critic_node(state: WritingState):\n",
    "    \"\"\"\n",
    "    审阅最新的故事片段，并决定是接受还是要求重写。\n",
    "    这是新增的核心逻辑。\n",
    "    \"\"\"\n",
    "    print(f\"--- 🧐 迭代 {state['iteration']}: 批判性审查中... ---\")\n",
    "\n",
    "    prompt = f\"\"\"你是一位严厉的文学评论家。你正在审阅一部关于“{state['topic']}”的短篇科幻小说。\n",
    "目前的完整草稿如下:\n",
    "---\n",
    "{''.join(state['story_fragments'])}\n",
    "---\n",
    "你的任务是：\n",
    "1.  **评估最新添加的段落**：它是否符合逻辑？是否有趣？是否推动了情节发展？\n",
    "2.  **做出决定**：如果段落质量高，决定 \"accept\"；如果段落有问题（如逻辑不通、文笔差、情节停滞），决定 \"revise\"。\n",
    "3.  **提供反馈**：\n",
    "    -   如果决定是 \"revise\"，请提供清晰、具体的修改意见，告诉作者为什么不好以及如何修改。\n",
    "    -   如果决定是 \"accept\"，请简要表扬，并为作者的**下一段**写作提供指导。\n",
    "\n",
    "请严格以 JSON 格式输出你的评判，包含 \"decision\" ('accept' 或 'revise') 和 \"feedback\" (你的评语) 两个键。\n",
    "例如: {{ \"decision\": \"revise\", \"feedback\": \"这段描写过于平淡，没有突出主角的孤独感。请重写，聚焦于他与灯塔的互动细节。\" }}\n",
    "\"\"\"\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    # 修正: 使用专门的 critic_model\n",
    "    response = critic_model.invoke(messages)\n",
    "\n",
    "    try:\n",
    "        critique = json.loads(response.content)\n",
    "        print(\"评论家决定:\", critique.get('decision'))\n",
    "        print(\"评论家反馈:\", critique.get('feedback'))\n",
    "        return critique\n",
    "    except (json.JSONDecodeError, AttributeError) as e:\n",
    "        print(f\"评论家返回格式错误: {e}\")\n",
    "        # 如果解析失败，默认接受并继续\n",
    "        return {\n",
    "            \"decision\": \"accept\",\n",
    "            \"feedback\": \"继续写下一段。\"\n",
    "        }\n",
    "\n",
    "\n",
    "# --- 重写准备节点 (Reviser) ---\n",
    "def reviser_node(state: WritingState):\n",
    "    \"\"\"\n",
    "    如果评论家要求重写，此节点负责从故事中移除最后一个（不满意的）片段。\n",
    "    \"\"\"\n",
    "    print(\"--- 🔄 上一段落被拒绝，准备重写... ---\")\n",
    "    # 移除最后一个元素\n",
    "    revised_fragments = state.get(\"story_fragments\", [])[:-1]\n",
    "    return {\"story_fragments\": revised_fragments}\n",
    "\n",
    "\n",
    "# 3. 定义边的逻辑\n",
    "MAX_ITERATIONS = 3\n",
    "\n",
    "\n",
    "def route_critique(state: WritingState):\n",
    "    \"\"\"根据评论家的决定，决定下一步走向。\"\"\"\n",
    "    if state.get(\"decision\") == \"revise\":\n",
    "        return \"reviser\"\n",
    "\n",
    "    # 如果接受，则检查是否已达到最大迭代次数\n",
    "    if state[\"iteration\"] >= MAX_ITERATIONS:\n",
    "        return END\n",
    "\n",
    "    # 如果接受且未完成，则进入下一次写作迭代\n",
    "    return \"writer\"\n",
    "\n",
    "\n",
    "def increment_iteration(state: WritingState):\n",
    "    \"\"\"只有在接受了上一步之后，才增加迭代次数。\"\"\"\n",
    "    return {\"iteration\": state.get(\"iteration\", 0) + 1}\n",
    "\n",
    "\n",
    "# 4. 构建图\n",
    "builder = StateGraph(WritingState)\n",
    "\n",
    "# 添加节点\n",
    "builder.add_node(\"writer\", writer_node)\n",
    "builder.add_node(\"critic\", critic_node)\n",
    "builder.add_node(\"reviser\", reviser_node)\n",
    "# 新增一个专门用于增加迭代次数的节点，以确保逻辑清晰\n",
    "builder.add_node(\"incrementer\", increment_iteration)\n",
    "\n",
    "# 定义图的流程\n",
    "builder.set_entry_point(\"incrementer\")\n",
    "builder.add_edge(\"incrementer\", \"writer\")\n",
    "builder.add_edge(\"writer\", \"critic\")\n",
    "\n",
    "# 关键的条件路由\n",
    "builder.add_conditional_edges(\"critic\", route_critique, {\n",
    "    \"reviser\": \"reviser\",\n",
    "    \"writer\": \"incrementer\",  # 如果接受，就去增加迭代次数，然后写作\n",
    "    END: END\n",
    "})\n",
    "\n",
    "# 重写后，直接返回给作者进行修改\n",
    "builder.add_edge(\"reviser\", \"writer\")\n",
    "\n",
    "# 编译图\n",
    "graph = builder.compile()\n",
    "\n",
    "# 5. 运行图\n",
    "topic = \"一个孤独的灯塔看守员，发现他的灯塔其实是一艘伪装成灯塔的古老外星飞船的控制室。\"\n",
    "initial_state = {\n",
    "    \"topic\": topic,\n",
    "    \"story_fragments\": [],\n",
    "    \"feedback\": \"故事刚刚开始。请先描写主角的日常生活和他对灯塔的感情，为后续的惊人发现做铺垫。\",\n",
    "    \"iteration\": 0,\n",
    "    \"decision\": \"\"\n",
    "}\n",
    "\n",
    "# 运行并打印最终结果\n",
    "final_state = graph.invoke(initial_state)\n",
    "\n",
    "print(\"\\n--- 📖 故事最终完成 📖 ---\")\n",
    "print(''.join(final_state['story_fragments']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoGPT / Self-Looping Agent\n",
    "\n",
    "#### 什么是“AutoGPT 风格的 Agent”？\n",
    "AutoGPT 是一种开创性的自主 AI Agent 概念。它的核心思想是创建一个能够“自我驱动”的循环，工作流程如下：\n",
    "\n",
    "1. **规划 (Plan)**：Agent 首先接收一个最终目标（例如，“写一份关于人工智能最新进展的报告”）。一个“规划者” (Planner) 会将这个模糊的目标分解成一个具体的、可执行的步骤列表（例如 [\"1. 搜索'2024年AI领域突破'。\", \"2. 搜索'主要AI模型的最新论文'。\", \"3. 综合搜索结果，撰写报告。\"]）。\n",
    "\n",
    "2. **执行 (Execute)**：Agent 从计划中取出第一项任务，并选择合适的工具来执行它（例如，使用 search 工具）。\n",
    "\n",
    "3. **更新与反思 (Update & Reflect)**：执行完成后，Agent 会评估结果。一个“重新规划者” (Re-planner) 会审视这个结果，并对照最终目标，决定下一步行动：\n",
    "\n",
    "- **计划有效**，继续下一步：如果搜索结果很好，就继续执行计划中的第二项。\n",
    "\n",
    "- **计划需要修改**：如果搜索结果表明最初的计划有缺陷（例如，发现“通用人工智能”是一个更重要的子话题），它会动态地修改计划列表（例如，在列表中插入一个新的搜索任务）。\n",
    "\n",
    "- **目标已达成**：如果计划中的最后一个任务（“撰写报告”）已完成，则 Agent 结束运行。\n",
    "\n",
    "这个“**规划 -> 执行 -> 重新规划**”的循环，使得 Agent 能够在没有持续人工干预的情况下，自主地完成复杂任务。\n",
    "\n",
    "\n",
    "#### 实现思路\n",
    "\n",
    "1. **定义一个复杂的状态 (State)**：状态是 AutoGPT 的核心。它需要包含：\n",
    "\n",
    "    - goal: 最终目标。\n",
    "    \n",
    "    - plan: 一个步骤列表。\n",
    "    \n",
    "    - past_steps: 一个已完成步骤及其结果的列表，作为“记忆”。\n",
    "    \n",
    "    - response: 用于存放最终的产出。\n",
    "\n",
    "2. **创建规划节点 (Planner/Re-planner)**：这是一个关键的 LLM 节点。它接收用户的目标和 past_steps（过去的经验），然后生成或更新 plan 列表。\n",
    "\n",
    "3. **创建执行节点 (Executor)**：这是一个 ToolNode，负责执行计划中的下一步。\n",
    "\n",
    "4. **构建主循环**：\n",
    "\n",
    "    - 图的流程从**规划节点**开始，创建初始计划。\n",
    "    \n",
    "    - 进入一个条件判断节点，该节点负责读取计划并决定下一步是调用**执行节点**还是结束。\n",
    "    \n",
    "    - **执行节点**完成后，将结果存入 past_steps。\n",
    "    \n",
    "    - 流程回到**重新规划节点**，让 Agent “思考”一下新的信息是否需要修改原计划。\n",
    "    \n",
    "    - **重新规划节点**再次连接到条件判断节点，形成循环。\n",
    "\n",
    "接下来，写一个三句话的短故事，然后自己决定何时停止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "# --- 1. 定义 Agent 的记忆（状态） ---\n",
    "# 我们只用一个带有消息列表和迭代次数的状态。\n",
    "class AgentState(dict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "    iteration: int\n",
    "\n",
    "\n",
    "# --- 2. 设置模型 ---\n",
    "# Agent 的大脑\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.7,  # 给故事一点创造性\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3. 定义 Agent 节点 ---\n",
    "# 这是 Agent 唯一需要执行的动作：调用模型进行思考和写作。\n",
    "def call_model(state: AgentState):\n",
    "    \"\"\"调用 LLM 来生成故事的下一部分。\"\"\"\n",
    "    print(f\"\\n--- ✍️ 迭代 {state['iteration']}: Agent 思考中... ---\")\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    # 将模型的回复添加到记忆中\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# --- 4. 定义循环的规则（边的逻辑） ---\n",
    "# 这是整个例子的核心：Agent 如何决定是继续还是停止。\n",
    "MAX_ITERATIONS = 5  # 设置一个安全上限\n",
    "\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    \"\"\"检查 Agent 的最新回复，决定下一步走向。\"\"\"\n",
    "    # 获取 Agent 最新的一条消息\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    print(f\"Agent 最新回复: \\\"{last_message.content[:50]}...\\\"\")\n",
    "\n",
    "    # 安全检查：如果达到最大迭代次数，强制结束\n",
    "    if state[\"iteration\"] >= MAX_ITERATIONS:\n",
    "        print(\"--- 🚫 已达到最大迭代次数，强制结束。 ---\")\n",
    "        return \"end\"\n",
    "\n",
    "    # 核心逻辑：如果 Agent 在回复中说了 \"<END>\"，就结束循环\n",
    "    if \"<END>\" in last_message.content:\n",
    "        print(\"--- ✅ Agent 发出结束信号，任务完成。 ---\")\n",
    "        return \"end\"\n",
    "    else:\n",
    "        # 否则，继续循环\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# --- 5. 构建图 ---\n",
    "# 这次我们的图非常简单，只有一个节点和一条指向自己的循环边。\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# 添加唯一的“大脑”节点\n",
    "builder.add_node(\"agent\", call_model)\n",
    "builder.set_entry_point(\"agent\")\n",
    "\n",
    "# 添加条件边，实现循环\n",
    "builder.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"agent\",  # 如果继续，就再次调用自己\n",
    "        \"end\": END,  # 如果结束，就退出图\n",
    "    },\n",
    ")\n",
    "\n",
    "# 编译图\n",
    "graph = builder.compile()\n",
    "\n",
    "# --- 6. 运行 Agent ---\n",
    "# 我们给 Agent 一个初始指令，告诉它规则\n",
    "initial_prompt = HumanMessage(\n",
    "    content=\"\"\"你的任务是写一个关于一只好奇猫的、只有三句话的短篇故事。\n",
    "    规则如下：\n",
    "    1. 你一次只能写一句话。\n",
    "    2. 在写完一句话后，如果你觉得故事还没结束，就说 \"<CONTINUE>\"。\n",
    "    3. 在写完第三句话（也就是最后一句话）后，你必须说 \"<END>\" 来结束任务。\n",
    "    请从第一句话开始。\"\"\"\n",
    ")\n",
    "\n",
    "# 运行图，并查看结果\n",
    "final_state = graph.invoke(\n",
    "    {\"messages\": [initial_prompt], \"iteration\": 0},\n",
    "    # 增加每次迭代的计数器\n",
    "    {\"recursion_limit\": MAX_ITERATIONS + 5}\n",
    ")\n",
    "\n",
    "print(\"\\n\\n--- 最终故事 ---\")\n",
    "# 打印出所有非指令的、由AI生成的故事内容\n",
    "for message in final_state[\"messages\"]:\n",
    "    if not message.content.startswith(\"你的任务是\"):\n",
    "        # 清理掉控制指令，只留下故事本身\n",
    "        story_part = message.content.replace(\"<CONTINUE>\", \"\").replace(\"<END>\", \"\").strip()\n",
    "        print(story_part)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第十一部分：评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么要评估 Agent？\n",
    "\n",
    "想象一下，你构建的 AutoGPT Agent 在你的几次测试中都表现完美。但当用户输入一个稍微不同的目标时，它就陷入了死循环，或者给出了完全错误的答案。这就是为什么我们需要系统性的评估：\n",
    "\n",
    "- **确保准确性**：验证 Agent 对各种输入都能给出正确、合理的响应。\n",
    "\n",
    "- **发现边界情况**：主动寻找那些可能让 Agent“犯傻”或崩溃的边缘案例。\n",
    "\n",
    "- **防止性能衰退**：当你更新模型、工具或指令时，评估可以确保 Agent 的性能没有变差。\n",
    "\n",
    "- **量化改进**：用具体的指标（如成功率、准确度）来衡量你的每次优化是否真的有效。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估什么？\n",
    "\n",
    "对于复杂的 Agent，我们可以评估多个维度：\n",
    "\n",
    "- **任务成功率**：Agent 是否最终达成了用户的目标？\n",
    "\n",
    "- **最终答案质量**：最终的报告、总结或回答是否内容详实、逻辑清晰？\n",
    "\n",
    "- **工具使用情况**：Agent 是否正确地选择了工具？它调用工具的参数是否合理？\n",
    "\n",
    "- **幻觉率**：Agent 的回答中是否有捏造事实的成分？\n",
    "\n",
    "- **成本与延迟**：Agent 完成一次任务需要消耗多少 Token？花费多长时间？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用 LangSmith 进行评估\n",
    "\n",
    "LangSmith 是 LangChain 官方推出的用于调试、监控和评估大语言模型应用的平台。它与 LangGraph 无缝集成，是进行 Agent 评估的最佳工具。\n",
    "\n",
    "### 评估流程\n",
    "\n",
    "- 追踪 (Tracing)：首先，LangSmith 会自动捕获你的 Agent 的每一次运行轨迹。你可以像看电影回放一样，清晰地看到每一个节点的输入输出、每一次工具的调用和模型的思考过程。这是最基础、最重要的调试手段。\n",
    "\n",
    "- 创建数据集 (Dataset)：要进行系统性评估，你需要一个“考卷”，也就是数据集。这个数据集中包含了一系列你希望 Agent 能够处理的输入（inputs），以及你期望它给出的参考答案（outputs）。\n",
    "\n",
    "- 运行评估器 (Evaluator)：LangSmith 允许你将 Agent 应用于整个数据集，并使用“评估器”来为每一次运行打分。评估器可以是：\n",
    "\n",
    "- 内置评估器：例如，让一个公正的 LLM（如 GPT-4）来判断你的 Agent 的输出和参考答案是否一致。\n",
    "\n",
    "- 自定义评估器：你可以编写自己的 Python 函数来定义评分逻辑（例如，检查输出中是否包含了某个关键词）。\n",
    "\n",
    "- 分析结果：评估完成后，LangSmith 会提供一个可视化的报告，展示你的 Agent 在整个数据集上的平均分、成功率、失败案例等，让你对 Agent 的性能有一个全局的认识。\n",
    "\n",
    "接下来，我们将通过代码示例，为一个简单的 Agent 创建一个数据集，并运行一次评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate\n",
    "from langsmith.schemas import Example, Run\n",
    "\n",
    "from typing import List, TypedDict, Annotated\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END, MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import operator\n",
    "\n",
    "# --- 准备工作 ---\n",
    "# 1. 安装 LangSmith\n",
    "# pip install -U langsmith\n",
    "\n",
    "# 2. 设置 LangSmith API 密钥\n",
    "# 你需要先在 LangSmith 网站 (smith.langchain.com) 注册并创建一个API密钥\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"lsv2_pt_xxxx\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "# 评估结果将保存到这个项目中\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"pr-xxxx\"\n",
    "\n",
    "\n",
    "# --- Agent 定义 (我们使用一个简单的 ReAct Agent) ---\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"在网络上搜索信息。\"\"\"\n",
    "    # 这是一个模拟工具\n",
    "    if \"langgraph\" in query.lower():\n",
    "        return \"LangGraph 是一个用于构建有状态、多角色 Agent 的库。\"\n",
    "    return \"未找到相关信息。\"\n",
    "\n",
    "\n",
    "tools = [search]\n",
    "# 我们继续使用 DeepSeek 模型\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\", \"sk-xxxxx\")\n",
    "model = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.0,\n",
    ").bind_tools(tools)\n",
    "\n",
    "\n",
    "# 定义图的节点和逻辑\n",
    "def should_continue(state: MessagesState):\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"continue\"\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# 构建图\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", ToolNode(tools))\n",
    "workflow.set_entry_point(\"agent\")\n",
    "workflow.add_conditional_edges(\"agent\", should_continue, {\"continue\": \"action\", \"end\": END})\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "agent_graph = workflow.compile()\n",
    "\n",
    "\n",
    "# 这是我们要评估的函数\n",
    "def agent_runnable(inputs: dict):\n",
    "    return agent_graph.invoke({\"messages\": [HumanMessage(content=inputs.get(\"question\"))]})\n",
    "\n",
    "\n",
    "# --- 评估流程 ---\n",
    "\n",
    "# 1. 初始化 LangSmith 客户端\n",
    "client = Client()\n",
    "\n",
    "# 2. 创建或选择一个数据集\n",
    "# 数据集是一系列输入和可选的参考输出\n",
    "dataset_name = f\"Simple Agent Test - {uuid.uuid4().hex[:4]}\"\n",
    "if client:\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"一个用于测试简单 ReAct Agent 的数据集。\"\n",
    "    )\n",
    "    # 为数据集添加一些“考题”\n",
    "    client.create_examples(\n",
    "        inputs=[\n",
    "            {\"question\": \"什么是 LangGraph?\"},\n",
    "            {\"question\": \"北京今天的天气怎么样?\"},\n",
    "        ],\n",
    "        outputs=[\n",
    "            {\"answer\": \"LangGraph 是一个用于构建有状态、多角色 Agent 的库。\"},\n",
    "            {\"answer\": \"未找到相关信息。\"},\n",
    "        ],\n",
    "        dataset_id=dataset.id,\n",
    "    )\n",
    "    print(f\"✅ LangSmith 数据集 '{dataset_name}' 创建成功!\")\n",
    "    print(f\"   你可以在这里查看: {dataset.url}\")\n",
    "else:\n",
    "    print(\"跳过 LangSmith 数据集创建。\")\n",
    "\n",
    "\n",
    "# 3. 定义评估器\n",
    "# 我们使用一个 AI 辅助的评估器。它会调用一个 LLM（如 gpt-4）来判断我们的 Agent 的输出是否正确。\n",
    "# 'correctness' 评估器会比较预测结果（prediction）和参考答案（reference）。\n",
    "def correctness_evaluator(run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    一个简单的评估器，判断 Agent 的最终回答是否包含了参考答案中的关键信息。\n",
    "    \"\"\"\n",
    "    prediction = run.outputs.get(\"messages\", [])[-1].content\n",
    "    reference = example.outputs.get(\"answer\", \"\")\n",
    "\n",
    "    # 这是一个简单的基于关键词的检查，实际评估中可能会使用 LLM\n",
    "    is_correct = reference.lower() in prediction.lower()\n",
    "\n",
    "    return {\"key\": \"correctness\", \"score\": is_correct}\n",
    "\n",
    "\n",
    "# 4. 运行评估\n",
    "if client:\n",
    "    print(\"\\n--- 🚀 开始在 LangSmith 上运行评估 ---\")\n",
    "    # 修正: 使用 `data` 参数代替 `dataset_name`，并移除 `project_name`\n",
    "    results = evaluate(\n",
    "        agent_runnable,  # 第一个参数是我们要评估的函数\n",
    "        data=dataset_name, # 使用 `data` 关键字参数来指定数据集\n",
    "        evaluators=[correctness_evaluator],  # 直接传递评估器列表\n",
    "        metadata={\"version\": \"1.0.0\"},  # 元数据\n",
    "    )\n",
    "    print(\"\\n--- ✅ 评估完成 ---\")\n",
    "    print(\"你可以在项目主页查看详细的评估结果和每一次运行的轨迹。\")\n",
    "else:\n",
    "    print(\"\\n--- 模拟运行 ---\")\n",
    "    # 如果没有 LangSmith，我们可以本地模拟一下\n",
    "    test_input = {\"question\": \"什么是 LangGraph?\"}\n",
    "    result = agent_runnable(test_input)\n",
    "    print(\"输入:\", test_input)\n",
    "    print(\"输出:\", result['messages'][-1].pretty_print())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
