{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# LlamaIndex教程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 引言\n",
    "### LlamaIndex：LLM时代的数据框架\n",
    "\n",
    "在大型语言模型（LLMs）迅速发展的时代，其在处理通用知识和生成文本方面展现出前所未有的能力。然而，当LLMs面临特定领域或企业内部的私有数据时，它们常常会遇到“知识盲区”，导致生成不准确或“幻觉”的响应。LlamaIndex正是在这一背景下应运而生，其核心定位是作为LLM与私有或领域特定数据之间的关键桥梁 。该框架通过“上下文增强”（Context Augmentation）技术，将外部数据有效地引入LLM的推理过程，从而显著提高其响应的相关性、准确性和可靠性 。\n",
    "\n",
    "LlamaIndex最初名为“GPT Index”，于2022年开发，旨在解决开发者在构建检索增强生成（RAG）应用时面临的数据集成和索引挑战 。从最初专注于GPT模型到如今更广泛地被称为“LLM时代的数据框架”，LlamaIndex的演变和其对企业数据的持续强调，表明它不仅仅是一个简单的工具，更是解决LLM在实际企业应用中“知识盲区”的关键基础设施。这一发展轨迹反映了行业从依赖通用LLM向构建领域特定、知识密集型LLM应用的需求转变。它致力于弥合通用LLM与特定领域知识之间的鸿沟，从而解锁LLM在实际商业场景中的巨大潜力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 为什么学习LlamaIndex？核心优势与应用场景\n",
    "学习LlamaIndex对于希望在LLM应用开发中实现突破的开发者而言具有显著价值。该框架提供了一系列核心优势和广泛的应用场景，使其成为构建生产级AI知识助手和代理的强大工具。\n",
    "\n",
    "核心优势：\n",
    "\n",
    "- **高效的数据检索与管理**： LlamaIndex专为构建高效的搜索和检索应用而设计。它利用先进的算法根据语义相似性对文档进行排名，从而实现快速、准确的数据检索和管理 。在处理大量数据时，它表现出卓越的计算效率，能够有效利用GPU资源 。\n",
    "- **广泛的数据连接器（LlamaHub）**： LlamaIndex通过其开放源代码库LlamaHub，提供了数百种数据连接器（也称为Readers）。这些连接器允许开发者从各种数据源（如API、数据库、PDF、本地文件、云存储服务如Microsoft SharePoint、Box、S3，以及协作工具如Notion、Slack、Discord）直接摄取数据，总计支持超过160种数据格式和来源 。这种广泛的连接能力极大地简化了数据准备工作，降低了数据孤岛带来的集成复杂性。\n",
    "- **灵活的索引策略**： LlamaIndex支持多种索引类型，包括向量存储索引、列表索引、关键词表索引、知识图谱索引和文档摘要索引 。此外，它还支持混合索引策略，能够结合向量和SQL等多种方法，以提高搜索相关性和效率，有效处理结构化和非结构化数据 。\n",
    "- **Agentic Applications支持**： LlamaIndex是构建由LLM驱动的Agentic Applications的领先框架。这些Agent能够执行复杂任务，例如查找信息、合成洞察、生成报告，并根据文档内容采取行动 。它将RAG管道作为Agent的工具之一，使其能够根据任务需求动态地选择和利用知识库。\n",
    "- **生产就绪的解决方案**： LlamaIndex不仅仅停留在原型阶段，它提供了从概念验证到实际生产部署所需的工具链。这包括对错误处理、数据验证、可扩展性等实际复杂性的支持，并提供了详细的Jupyter Notebook示例作为可直接运行和适应的起点 。\n",
    "- **降低学习曲线与快速启动**： 相较于某些竞争对手，LlamaIndex提供较低的上手时间 。其高层API允许用户仅用5行代码即可完成数据摄取和查询，从而实现快速项目启动和原型开发 。这种易用性极大地加速了开发者的学习过程，并促进了RAG技术的快速普及和应用。\n",
    "\n",
    "LlamaIndex的“低上手时间”和“5行代码”的承诺，结合其对“企业数据”和“生产就绪”的强调，揭示了其在加速AI应用开发和实际部署中的核心价值。它不仅仅是技术上的便利，更是业务价值实现的催化剂。这种设计理念使得开发者能够快速验证想法并展示价值，这对于将AI从实验阶段推向实际部署至关重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 典型应用场景\n",
    "\n",
    "LlamaIndex的强大功能使其在多个领域和应用中展现出巨大潜力：\n",
    "\n",
    "- **问答系统 (RAG)**： 构建基于私有数据的问答系统，显著减少LLM可能出现的“幻觉”现象，提供更准确的答案 。\n",
    "- **智能聊天机器人**： 创建能够与存储数据进行动态交互的对话式AI应用，增强聊天机器人的上下文感知能力和领域特定知识 。\n",
    "- **文档理解与信息提取**： 从非结构化文本中提取结构化信息，例如合同审查、患者病例摘要、发票处理等，将简单的文档处理任务转化为业务智能工具 。\n",
    "- **自主代理**： 开发能够执行研究和采取行动的自主AI代理，例如金融文档研究、用户支持代理、问卷填写等 。\n",
    "- **多模态应用**： 支持处理文本、图像、音频等多种数据类型，实现多模态RAG，从而扩展LLM对现实世界信息的理解能力 。\n",
    "- **企业知识管理与搜索**： 统一分散的数据源，增强语义搜索能力，并实现受控访问和实时更新，解决企业数据碎片化的问题 。\n",
    "\n",
    "LlamaIndex通过提供高度抽象的API和“5行代码”的入门路径，极大地降低了LLM应用开发的门槛。这不仅加速了开发者的学习过程，也促进了RAG技术的快速普及和应用。其广泛的数据连接器和灵活的索引策略，结合对Agentic Workflows的支持，使其在企业级应用中具有显著优势，尤其是在处理复杂、多样且需要严格控制的数据环境时。这表明LlamaIndex正在填补LLM应用栈中“数据基础设施”的关键空白，而不仅仅是提供一个RAG模板。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## LlamaIndex核心概念与基础\n",
    "\n",
    "深入理解LlamaIndex的强大功能，首先需要掌握其核心概念和基础架构。这些概念构成了构建复杂LLM应用的基础。\n",
    "\n",
    "### 大型语言模型 (LLMs)、Agentic Applications与RAG\n",
    "LlamaIndex的整个框架都围绕着如何最大化LLM的效用而设计，并引入了Agentic Applications和RAG等关键概念来扩展其能力。\n",
    "\n",
    "- **LLMs (大型语言模型)**： LLM是LlamaIndex的基石，是能够理解、生成和操作自然语言的人工智能系统。它们能够根据其训练数据或在查询时提供给它们的数据来回答问题 。LlamaIndex通过提供工具和接口，使得LLM能够更有效地与外部数据交互。   \n",
    "- **Agentic Applications (代理应用)**： 当LLM在一个应用程序中被用来做出决策、采取行动和/或与外部世界互动时，它就被定义为Agentic Application。这类应用的关键特征包括：\n",
    "    - **LLM增强**： LLM通过工具（可调用函数）、记忆和/或动态提示得到增强。\n",
    "    - **提示链**： 多个LLM调用相互构建，一个LLM的输出作为下一个LLM的输入。\n",
    "    - **路由**： LLM用于将应用程序路由到下一个适当的步骤或状态。\n",
    "    - **并行性**： 应用程序可以并行执行多个步骤或操作。\n",
    "    - **编排**： 使用LLM的层次结构来编排更低级别的操作和LLM。\n",
    "    - **反思**： LLM用于反思和验证先前步骤或LLM调用的输出，从而指导应用程序进入下一个适当的步骤或状态 。 在LlamaIndex中，开发者可以通过Workflow类来编排一系列步骤和LLM，从而构建复杂的代理应用 。\n",
    "\n",
    "- **Agents (代理)**： 代理是“Agentic Application”的一个特定实例。它是一个软件组件，通过结合LLM与其他工具和记忆，在推理循环中半自主地执行任务，并决定下一步使用哪个工具（如果有的话） 。LlamaIndex提供了一个构建代理的框架，其中一个关键的能力是能够将RAG管道作为众多工具之一来完成任务 。\n",
    " \n",
    "LlamaIndex对“Agentic Applications”和“Workflows”的强调，以及将其定义为RAG之上的更高层抽象，表明其框架设计旨在支持从简单的RAG问答到复杂的、多步骤、有状态的自主AI行为 。RAG在LlamaIndex中被视为Agent的“工具”之一，而非其唯一功能。这种将RAG工具化的思想，使得Agent能够根据任务需求动态地选择和利用知识库，极大地提升了Agent的能力和灵活性。这反映了LLM应用从被动问答向主动决策和行动的范式转变，LlamaIndex正在构建一个平台，让AI能够理解上下文、维护状态、应用业务规则并采取多步骤行动。   \n",
    "\n",
    "- **RAG (检索增强生成)**： RAG是LlamaIndex中构建数据支持的LLM应用程序的核心技术。它允许LLM通过在查询时向其提供私有数据来回答有关该数据的问题，而不是通过在这些数据上训练LLM。为了避免每次都将所有数据发送给LLM，RAG会索引数据并选择性地仅发送相关部分以及查询 。这有效地解决了LLM在缺乏特定领域知识时可能出现的“幻觉”问题，确保了响应的准确性和相关性。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据生命周期：加载、索引、存储、查询与评估\n",
    "LlamaIndex提供了一套全面的工具，用于管理LLM应用中的数据生命周期，确保数据从摄取到最终响应的质量和效率。\n",
    "\n",
    "- **加载 (Loading)**： 这是数据管道的第一步，指将数据从其原生来源（无论是文本文件、PDF、网站、数据库还是API）导入到LlamaIndex的管道中。LlamaHub提供了数百个连接器，可以轻松地从各种数据源加载数据 。   \n",
    "- **索引 (Indexing)**： 数据加载后，需要创建一种数据结构，以便高效地查询数据。对于LLM而言，这几乎总是意味着创建向量嵌入——数据的语义含义的数值表示，以及其他元数据策略，以便轻松准确地找到上下文相关的数据 。LlamaIndex支持混合索引策略，可以结合向量和SQL等方法来优化索引过程 。   \n",
    "- **存储 (Storing)**： 一旦数据被索引，通常需要存储索引本身以及其他元数据，以避免重复索引过程，从而节省时间和计算资源 。向量存储是一种专门用于存储嵌入的数据库，是LlamaIndex中常用的存储方式 。   \n",
    "- **查询 (Querying)**： 对于任何给定的索引策略，LlamaIndex都提供了多种方式来利用LLM和其内部数据结构进行查询，包括子查询、多步骤查询和混合策略 。查询引擎提供了一种自然语言访问数据的方式 。   \n",
    "- **评估 (Evaluation)**： 这是任何数据管道中的关键步骤，用于检查其相对于其他策略或在进行更改后的有效性。评估提供了关于查询响应的准确性、忠实度和速度的客观衡量标准 。\n",
    "\n",
    "LlamaIndex对数据生命周期每个阶段的细致抽象和支持，特别是对“索引”和“存储”的强调，揭示了其作为“数据框架”的深层设计哲学。这种端到端的能力对于构建生产级应用至关重要，因为它确保了数据从源头到最终响应的质量和可控性。通过提供从数据摄取到评估的全面工具，LlamaIndex确保开发者能够有效管理和优化其LLM应用的数据流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 核心数据结构：文档、节点与嵌入\n",
    "LlamaIndex在内部使用特定的数据结构来高效地组织和处理信息，这些结构是其RAG系统性能优化的基础。\n",
    "\n",
    "- **文档 (Documents)**： 文档是任何数据源的容器，例如一个PDF文件、一个API的输出，或者从数据库中检索到的一段数据 。它们是原始数据的初始表示形式。   \n",
    "- **节点 (Nodes)**： 节点是LlamaIndex中的原子数据单元，代表源文档的“块”（chunk） 。当文档被摄取到索引中时，它们会被分割成更小、更易于管理的部分，这些部分就是节点。默认情况下，文档会被分割成大小为1024个token的块，并带有20个token的重叠 。这种分块策略有助于处理大型文档，并确保每个块都能适应LLM的上下文窗口限制，同时保留足够的上下文信息。   \n",
    "- **嵌入 (Embeddings)**： 嵌入是LLM生成的数据的数值表示。这些数值向量捕获了文本的语义含义。在过滤数据以进行相关性检索时，LlamaIndex会将用户查询转换为嵌入，然后向量存储会根据数值相似性找到与查询嵌入最相似的数据（即最相关的节点） 。\n",
    "\n",
    "“节点”作为原子数据单元的概念，以及对分块（chunking）策略的强调，是LlamaIndex解决LLM上下文窗口限制和提高检索精度的核心机制。它认识到简单的文档嵌入不足以实现高效和精确的检索，因此提供了更细粒度的控制，以确保LLM接收到最相关、最集中的上下文。这种设计选择对于效率（处理更少的数据）和准确性（提供更相关的上下文）都至关重要，使其成为LlamaIndex RAG能力的重要基石。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境配置与第一个RAG应用\n",
    "\n",
    "LlamaIndex是一个用Python编写的库，因此需要读者安装Python并具备基本的Python编程知识。对于JavaScript开发者，LlamaIndex也提供了TypeScript包 。许多LlamaIndex的示例都以Jupyter Notebooks的形式提供，方便开发者进行交互式学习和实验 。   \n",
    "\n",
    "LlamaIndex的设计目标之一是降低LLM应用的开发门槛。其高级API使得初学者能够以极简的方式快速启动项目，甚至在5行代码内完成数据摄取和查询 。这种快速启动能力对于开发者快速验证想法和原型开发至关重要。   \n",
    "\n",
    "以下是一个基本的LlamaIndex RAG应用伪代码示例，展示了其核心流程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index langchain openai\n",
    "# !pip install llama-index-embeddings-langchain\n",
    "# pip install langchain-community\n",
    "# pip install dashscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The author attended Stanford University from 2010 to 2014, majoring in Computer Science. During this time, they participated in hackathons, interned at Google, published two research papers on machine learning, and co-founded a student AI club.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from llama_index.llms.langchain import LangChainLLM\n",
    "from langchain_community.embeddings import DashScopeEmbeddings, OllamaEmbeddings\n",
    "\n",
    "\n",
    "# 1. 加载数据：使用SimpleDirectoryReader从指定目录加载文档\n",
    "# 假设 'data' 目录下有你的文本文件或PDF文件\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# 2. 构建向量索引：LlamaIndex会自动处理文档分块和嵌入生成\n",
    "# 这将把加载的文档转换为可查询的向量索引\n",
    "embedding_model = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\"\n",
    ") # OllamaEmbeddings\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model=embedding_model)\n",
    "\n",
    "# 3. 构建 LangChain 的 LLM（你也可以换成其他 LangChain 支持的模型）\n",
    "langchain_llm = ChatOpenAI(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=DEEPSEEK_API_KEY,\n",
    "    base_url=\"https://api.deepseek.com/v1\",\n",
    "    temperature=0.0\n",
    ")\n",
    "# 4. 将 LangChain LLM 封装给 LlamaIndex 使用\n",
    "llm = LangChainLLM(llm=langchain_llm)\n",
    "\n",
    "# 5. 创建查询引擎：通过索引创建一个查询接口\n",
    "# 查询引擎负责接收自然语言查询并利用索引检索相关信息\n",
    "query_engine = index.as_query_engine(llm)\n",
    "\n",
    "# 6. 提问并获取响应：向查询引擎发送自然语言问题\n",
    "response = query_engine.query(\"What did the author do in college?\")\n",
    "\n",
    "# 7. 打印响应：输出LLM基于检索到的信息生成的答案\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整的代码示例和详细的入门教程可以参考LlamaIndex官方文档的(https://docs.llamaindex.ai/en/stable/getting_started/starter_example/)  或IBM提供的(https://www.ibm.com/think/tutorials/llamaindex-rag) 。LlamaIndex通过提供高度抽象的API和“5行代码”的入门路径，极大地降低了LLM应用开发的门槛。这不仅加速了开发者的学习过程，也促进了RAG技术的快速普及和应用，是其大规模采用的关键驱动力。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据连接器与LlamaHub：轻松集成多源数据\n",
    "在构建LLM应用时，数据通常分散在各种系统和格式中。LlamaIndex通过其强大的数据连接器生态系统，解决了这一痛点。\n",
    "\n",
    "**LlamaHub的作用**： LlamaHub是一个开放源代码库，它汇集了数百种数据连接器（在LlamaIndex中也称为Readers） 。这些连接器的核心功能是将来自不同来源和格式的数据摄取并转换为LlamaIndex统一的Document表示形式，从而为后续的索引和查询做好准备。   \n",
    "\n",
    "**支持的[数据源](https://docs.llamaindex.org.cn/en/stable/module_guides/loading/connector/modules/)示例**： LlamaHub支持的数据源非常广泛，包括：\n",
    "\n",
    "- 本地文件： 可以轻松读取本地存储的各种文件类型，如PDF、JPG、PNG、DOCX等 。   \n",
    "- 云存储服务： 无缝集成主流云存储平台，如Microsoft SharePoint、Box和Amazon S3，并能原生处理访问控制和增量同步 。   \n",
    "- 数据库： 支持SQL、NoSQL数据库的连接和数据摄取 。   \n",
    "- API： 提供与各种API的连接器，例如Airtable、Jira和Salesforce，使得从业务应用中获取数据变得简单 。   \n",
    "- 协作工具： 支持Notion、Slack和Discord等协作平台的数据加载 。   \n",
    "- 网络爬虫： 通过Apify Actors连接器，可以执行网页爬取、抓取网页内容并提取文本信息 。\n",
    "\n",
    "总计，LlamaIndex支持超过160种不同的数据源和格式 。这种广泛的连接能力使得开发者能够轻松地将企业内部的各种异构数据整合到一个统一的知识库中，极大地简化了数据准备工作，并为构建跨平台、统一知识库的AI应用提供了坚实的基础。   \n",
    "\n",
    "以下是使用[LlamaHub连接器加载数据](https://docs.llamaindex.org.cn/en/stable/module_guides/loading/connector/modules/)的伪代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from PDF directory.\n"
     ]
    }
   ],
   "source": [
    "# 伪代码示例：使用 LlamaHub 连接器加载数据\n",
    "# from llama_index.readers.google import GoogleDocsReader # 导入Google Docs连接器\n",
    "from llama_index.core import SimpleDirectoryReader # 导入用于本地文件读取的连接器\n",
    "\n",
    "# 从本地目录加载PDF文件\n",
    "# 假设 './data/pdfs' 包含PDF文档\n",
    "documents_from_pdf = SimpleDirectoryReader(input_dir=\"./data\").load_data()\n",
    "\n",
    "# 从Google Docs加载文档\n",
    "# 需要提供Google Docs的文档ID列表\n",
    "# loader = GoogleDocsReader()\n",
    "# documents_from_gdocs = loader.load_data(document_ids=[\"your_google_doc_id_1\", \"your_google_doc_id_2\"])\n",
    "\n",
    "# 打印加载的文档数量\n",
    "print(f\"Loaded {len(documents_from_pdf)} documents from PDF directory.\")\n",
    "# print(f\"Loaded {len(documents_from_gdocs)} documents from Google Docs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaHub作为数据连接器的中心化仓库，解决了企业数据分散、格式多样化的痛点。这种全面的数据摄取能力意味着开发者无需为每个数据源构建定制的ETL管道，从而显著减少了开发时间和复杂性。它使得构建“统一格式”和“集中索引”成为可能 ，这对于构建能够从组织整个数据版图中提取洞察的整体知识助手至关重要。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基础查询引擎与聊天引擎\n",
    "LlamaIndex为LLM应用中的不同交互模式提供了专门的接口，主要区分为查询引擎和聊天引擎。\n",
    "\n",
    "- **查询引擎 (Query Engines)**： 查询引擎是LlamaIndex的核心接口，它提供了一个通用机制，允许用户以自然语言对数据进行提问，并接收到丰富的响应 。查询引擎通常（但不总是）构建在一个或多个索引之上，并通过检索器（Retriever）来获取与用户查询相关的上下文信息 。LlamaIndex还支持组合多个查询引擎以实现更高级的功能，例如处理复杂的多源查询 。   \n",
    "\n",
    "- **聊天引擎 (Chat Engines)**： 聊天引擎是LlamaIndex为多消息、来回交互的对话式应用设计的接口 。与一次性查询引擎不同，聊天引擎能够维护对话历史和上下文，从而实现更自然、连贯的多轮对话体验，非常适用于构建智能聊天机器人。\n",
    "\n",
    "LlamaIndex区分“查询引擎”和“聊天引擎”，体现了其对LLM应用交互模式的精细化理解。这种模块化设计使得开发者可以根据具体应用需求，选择最合适的交互模式，提高了框架的灵活性和适用性。查询引擎可以针对直接、高效的信息检索进行优化，而聊天引擎则可以专注于维护对话状态、记忆和轮流逻辑，这对于自然和连贯的多消息交互至关重要。\n",
    "\n",
    "以下是查询引擎和聊天引擎的伪代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Engine Response: 这份文档包含了四位不同人物的教育背景和学术经历：\n",
      "\n",
      "1. John Smith - 2010至2014年就读于斯坦福大学计算机科学专业，参加过多次黑客马拉松，曾在谷歌实习，发表过两篇机器学习研究论文，并共同创立了学生AI俱乐部。\n",
      "\n",
      "2. Sarah Lee - 哈佛大学文学专业学生，曾任校报主编，毕业论文研究方向为后现代美国小说。\n",
      "\n",
      "3. Mike Chen - 麻省理工学院机械工程专业毕业生，在校期间制作了多个机器人原型，入围过全国机器人竞赛决赛，并在校园3D打印实验室兼职工作。\n",
      "\n",
      "4. Emily Davis - 芝加哥大学经济学专业学生，曾赴德国交换学习，在当地经济智库完成研究实习，毕业课题研究城市化对小企业的影响。\n",
      "\n",
      "文档主要聚焦于这些人物在大学期间的专业选择、学术成就、课外活动和实践经历等方面的情况。\n",
      "Chatbot: 你好！有什么可以帮助你的吗？\n",
      "Chatbot: 该文档主要介绍了四位不同人物的教育背景和学术经历：\n",
      "\n",
      "1. John Smith - 2010至2014年就读于斯坦福大学计算机科学专业，参加过多次黑客马拉松，曾在谷歌实习，发表过两篇机器学习研究论文，并共同创立了学生AI俱乐部。\n",
      "\n",
      "2. Sarah Lee - 哈佛大学文学专业学生，曾担任校报主编，毕业论文研究后现代美国小说。\n",
      "\n",
      "3. Mike Chen - 麻省理工学院机械工程专业毕业生，在校期间制作了多个机器人原型，入围过全国机器人竞赛决赛，并在校园3D打印实验室兼职工作。\n",
      "\n",
      "4. Emily Davis - 芝加哥大学经济学专业学生，曾在德国交换学习，在当地经济智库完成研究实习，毕业课题研究城市化对小企业的影响。\n",
      "\n",
      "这些内容概括了四位学生在各自大学期间的主要学术成就和课外活动经历。\n",
      "Chatbot: 文档中主要描述了四位人物的教育背景和在校期间的学术活动，如专业学习、实习经历、研究项目和课外活动等。但并未提及他们毕业后的职业发展趋势或具体规划。\n"
     ]
    }
   ],
   "source": [
    "# 伪代码示例：查询引擎与聊天引擎\n",
    "# 假设 'index' 已经通过加载文档并构建索引完成初始化\n",
    "\n",
    "# 1. 使用查询引擎进行单次问答\n",
    "query_engine = index.as_query_engine(llm=llm) # 将索引转换为查询引擎\n",
    "response_query = query_engine.query(\"关于这份文档，你能告诉我什么关键信息？\")\n",
    "print(f\"Query Engine Response: {response_query}\")\n",
    "\n",
    "# 2. 使用聊天引擎进行多轮对话\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\",llm=llm) # 创建聊天引擎，可选择不同的聊天模式\n",
    "print(\"Chatbot: 你好！有什么可以帮助你的吗？\")\n",
    "chat_response_1 = chat_engine.chat(\"是的，能帮我总结一下文档的主要内容吗？\")\n",
    "print(f\"Chatbot: {chat_response_1}\")\n",
    "chat_response_2 = chat_engine.chat(\"那它有没有提到关于未来的发展趋势？\")\n",
    "print(f\"Chatbot: {chat_response_2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex高级特性与深入研究\n",
    "\n",
    "LlamaIndex的强大之处远不止于基础的RAG功能，它提供了一系列高级特性和深入定制能力，以应对复杂的数据场景和LLM应用需求。\n",
    "\n",
    "### 高级索引结构与定制化\n",
    "\n",
    "LlamaIndex提供了多种索引模型，旨在优化数据探索和分类，从而提高查询效率和准确性。这些索引类型包括："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    ListIndex,\n",
    "    KeywordTableIndex,\n",
    "    KnowledgeGraphIndex,\n",
    "    DocumentSummaryIndex,\n",
    ")\n",
    "\n",
    "# 准备示例文档\n",
    "documents = [\n",
    "    Document(text=\"产品A是一款智能手机，具有高分辨率摄像头和强大的处理器。\"),\n",
    "    Document(text=\"产品B是一款平板电脑，配备大屏幕和长续航电池。\"),\n",
    "    Document(text=\"产品C是一款智能手表，支持健康监测和消息通知。\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **VectorStoreIndex (向量存储索引)**： 这是最常用的索引类型，用于实现语义搜索。它将文本内容转换为高维向量嵌入，并存储在专门的向量数据库中，以便通过计算向量相似度来检索语义相关的文档 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorStoreIndex 查询结果： 产品B有长续航电池。\n"
     ]
    }
   ],
   "source": [
    "# 1. VectorStoreIndex: 语义搜索\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "query = \"哪款产品有长续航电池？\"\n",
    "response = vector_index.as_query_engine().query(query)\n",
    "print(\"VectorStoreIndex 查询结果：\", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **ListIndex (列表索引)**： 简单地将文档存储为一个列表。检索时通常按顺序遍历，适用于需要完整上下文或特定顺序的场景 。\n",
    "\n",
    "- 用途：简单存储文档的线性列表，查询时按照顺序遍历。\n",
    "\n",
    "- 适用场景：当文档之间有顺序关系，或需要保留完整上下文时，比如日志分析、对话历史等。\n",
    "\n",
    "- 特点：实现简单，性能相对较低，适合数据量小或对顺序敏感的场景。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ListIndex 查询结果： 这里有三款不同的智能设备：\n",
      "\n",
      "1. 一款配备高清摄像头和高效处理器的智能手机；\n",
      "2. 一款拥有大尺寸显示屏和持久续航能力的平板电脑；\n",
      "3. 一款具备健康追踪功能和消息提醒服务的智能手表。\n"
     ]
    }
   ],
   "source": [
    "# 2. ListIndex: 按顺序遍历文档\n",
    "list_index = ListIndex.from_documents(documents)\n",
    "response = list_index.as_query_engine().query(\"介绍一下所有产品\")\n",
    "print(\"ListIndex 查询结果：\", response.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **KeywordTableIndex (关键词表索引)**： 基于文档中的关键词来构建索引，支持通过关键词进行快速检索 。\n",
    "\n",
    "- 用途：基于关键词提取构建索引，支持快速的关键词匹配和检索。\n",
    "\n",
    "- 适用场景：适合需要基于关键字过滤或检索的业务场景，如标签搜索、目录导航等。\n",
    "\n",
    "- 特点：快速定位关键词相关内容，但缺少语义理解能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeywordTableIndex 查询结果： 智能手机通常具备高分辨率摄像头和强大的处理器等功能。\n"
     ]
    }
   ],
   "source": [
    "# 3. KeywordTableIndex: 关键词索引\n",
    "keyword_index = KeywordTableIndex.from_documents(documents)\n",
    "response = keyword_index.as_query_engine().query(\"智能手机的功能有哪些？\")\n",
    "print(\"KeywordTableIndex 查询结果：\", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **KnowledgeGraphIndex (知识图谱索引)**： 构建知识图谱来表示数据中的实体和关系，从而支持更复杂的结构化查询和推理 。\n",
    "\n",
    "- 用途：抽取实体及其关系，构建图结构索引，支持结构化查询和推理。\n",
    "\n",
    "- 适用场景：适合需要复杂推理、多实体关系查询的知识管理、问答系统。\n",
    "\n",
    "- 特点：支持丰富的实体关系表达和图遍历查询，能够处理更复杂的逻辑问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KnowledgeGraphIndex 查询结果： 智能手机通常具备高分辨率摄像头和强大的处理器等功能。\n"
     ]
    }
   ],
   "source": [
    "# 4. KnowledgeGraphIndex: 知识图谱索引\n",
    "kg_index = KnowledgeGraphIndex.from_documents(documents)\n",
    "response = kg_index.as_query_engine().query(\"智能手机有哪些功能？\")\n",
    "print(\"KnowledgeGraphIndex 查询结果：\", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **DocumentSummaryIndex (文档摘要索引)**： 存储文档的摘要信息，用于高层次的检索，例如快速了解文档主题或进行文档筛选 。\n",
    "\n",
    "- 用途：为每个文档生成摘要信息，索引存储的是文档的高层次概览。\n",
    "\n",
    "- 适用场景：快速筛选文档、主题聚焦、初步筛查大量文档。\n",
    "\n",
    "- 特点：查询速度快，适合用于初步过滤，后续可结合其他索引做深度检索。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current doc id: 17c6051d-e8af-4417-b1c6-35efecef4956\n",
      "current doc id: 62a23bc1-e5ba-4da3-9072-ec703f5c2c75\n",
      "current doc id: e21ccd24-4c90-493c-a159-90deac4f2070\n",
      "DocumentSummaryIndex 查询结果： 产品A是一款配备高分辨率摄像头和强大处理器的智能手机。\n"
     ]
    }
   ],
   "source": [
    "# 5. DocumentSummaryIndex: 文档摘要索引\n",
    "summary_index = DocumentSummaryIndex.from_documents(documents)\n",
    "response = summary_index.as_query_engine().query(\"快速了解产品A\")\n",
    "print(\"DocumentSummaryIndex 查询结果：\", response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex的核心设计原则之一是其高度的模块化和可定制性。几乎所有核心模块都可以被子类化和定制 。这意味着开发者可以根据特定需求定义数据如何存储、索引和查询，从而超越预打包模块的功能。例如，可以创建优先处理层次数据关系的HierarchicalIndex ，以更好地适应法律合同或组织架构等复杂文档结构。这种开放性允许开发者编写大量的自定义代码，同时仍能利用LlamaIndex的底层抽象并与其他组件无缝集成 。   \n",
    "\n",
    "以下是定制化索引的概念性伪代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 伪代码示例：定制化索引（概念性）\n",
    "from llama_index.core.indices.base import BaseIndex\n",
    "from llama_index.core.schema import Document, Node\n",
    "from typing import List\n",
    "\n",
    "class CustomHierarchicalIndex(BaseIndex):\n",
    "    \"\"\"\n",
    "    一个概念性的自定义层次索引，用于处理具有明确父子关系的文档。\n",
    "    它可能在内部使用图数据库或嵌套结构来存储节点。\n",
    "    \"\"\"\n",
    "    def __init__(self, documents: List, service_context=None, **kwargs):\n",
    "        super().__init__(service_context=service_context, **kwargs)\n",
    "        self._documents = documents\n",
    "        self._build_index() # 在初始化时构建索引\n",
    "\n",
    "    def _build_index(self):\n",
    "        # 实际实现中，这里会包含复杂的逻辑来解析文档，\n",
    "        # 识别层次关系，并将节点存储在适当的数据结构中（例如，一个图数据库）\n",
    "        print(\"Building custom hierarchical index...\")\n",
    "        self._nodes = []# 存储处理后的节点\n",
    "        for doc in self._documents:\n",
    "            # 假设有一个解析器可以将文档分解为带层次的节点\n",
    "            # 例如，一个PDF的章节可以作为父节点，段落作为子节点\n",
    "            # self._nodes.extend(self._parse_document_to_hierarchical_nodes(doc))\n",
    "            pass # 实际逻辑会在这里\n",
    "\n",
    "    def _query(self, query_bundle, **kwargs):\n",
    "        # 实际实现中，这里会包含复杂的逻辑来遍历层次索引，\n",
    "        # 根据查询相关性检索节点，并可能利用LLM进行推理\n",
    "        print(f\"Querying custom hierarchical index with: {query_bundle.query_str}\")\n",
    "        # 例如：从根节点开始，根据查询语义向下遍历层次结构\n",
    "        # relevant_nodes = self._traverse_hierarchy_and_retrieve(query_bundle)\n",
    "        # 然后使用LLM合成答案\n",
    "        return \"Response from custom hierarchical index.\"\n",
    "\n",
    "# 假设有一些文档\n",
    "# documents =\n",
    "# index = CustomHierarchicalIndex(documents)\n",
    "# query_engine = index.as_query_engine()\n",
    "# response = query_engine.query(\"查询层次结构中的信息\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex不仅提供预设的索引类型，更允许深度定制，这表明其设计理念是高度灵活和可扩展的。这对于处理复杂、非标准的企业数据结构至关重要，使得LlamaIndex能够适应广泛的垂直行业需求，并实现更精确和高效的信息检索。\n",
    "\n",
    "但是通常不直接从 BaseIndex 继承自定义索引，而是创建一个自定义的 Retriever（检索器），并将其与一个标准的 LlamaIndex 索引（例如 VectorStoreIndex 或 ListIndex）结合使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始文档转换为节点并构建层次关系完成 ---\n",
      "层次关系: {'root_dir': ['proj_a', 'proj_b'], 'proj_a': ['doc1', 'code_file', 'subdir_x'], 'proj_b': ['data_report'], 'subdir_x': ['images']}\n",
      "节点数量: 8\n",
      "\n",
      "--- 基础向量索引构建完成 ---\n",
      "\n",
      "--- 第一次查询 ---\n",
      "\n",
      "--- 正在通过 CustomHierarchicalRetriever 检索: 关于人工智能的项目有什么？ ---\n",
      "  - 阶段1: 通过基础索引进行初步检索...\n",
      "  - 初步检索到 2 个节点。\n",
      "  - 阶段2: 扩展相关节点 (父级/子级)...\n",
      "    - 扩展到父节点: '根目录' (ID: root_dir)\n",
      "    - 扩展到子节点: '文档1' (ID: doc1)\n",
      "    - 扩展到子节点: '代码文件' (ID: code_file)\n",
      "    - 扩展到子节点: '子目录X' (ID: subdir_x)\n",
      "    - 扩展到子节点: '数据报告' (ID: data_report)\n",
      "--- 最终检索到 7 个相关节点 ---\n",
      "查询结果: 关于人工智能的项目包含需求文档、核心代码以及一个名为子目录X的子目录。\n",
      "\n",
      "--- 第二次查询 ---\n",
      "\n",
      "--- 正在通过 CustomHierarchicalRetriever 检索: 给我看看项目A的文档。 ---\n",
      "  - 阶段1: 通过基础索引进行初步检索...\n",
      "  - 初步检索到 2 个节点。\n",
      "  - 阶段2: 扩展相关节点 (父级/子级)...\n",
      "    - 扩展到父节点: '项目A' (ID: proj_a)\n",
      "--- 最终检索到 3 个相关节点 ---\n",
      "查询结果: 项目A的需求文档可以在这里查看。\n",
      "\n",
      "--- 第三次查询 ---\n",
      "\n",
      "--- 正在通过 CustomHierarchicalRetriever 检索: 根目录里有什么内容？ ---\n",
      "  - 阶段1: 通过基础索引进行初步检索...\n",
      "  - 初步检索到 2 个节点。\n",
      "  - 阶段2: 扩展相关节点 (父级/子级)...\n",
      "    - 扩展到子节点: '项目A' (ID: proj_a)\n",
      "    - 扩展到子节点: '项目B' (ID: proj_b)\n",
      "    - 扩展到子节点: '图片集' (ID: images)\n",
      "--- 最终检索到 5 个相关节点 ---\n",
      "查询结果: 根目录包含两个项目：一个是关于人工智能的项目，另一个是关于数据分析的项目。\n",
      "\n",
      "--- 第四次查询 (更具体) ---\n",
      "\n",
      "--- 正在通过 CustomHierarchicalRetriever 检索: 项目A的视觉素材是什么？ ---\n",
      "  - 阶段1: 通过基础索引进行初步检索...\n",
      "  - 初步检索到 2 个节点。\n",
      "  - 阶段2: 扩展相关节点 (父级/子级)...\n",
      "    - 扩展到父节点: '项目A' (ID: proj_a)\n",
      "    - 扩展到父节点: '子目录X' (ID: subdir_x)\n",
      "--- 最终检索到 4 个相关节点 ---\n",
      "查询结果: 项目A的视觉素材是\"图片集\"，它位于\"子目录X\"中。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embedding_model\n",
    "\n",
    "from typing import List, Dict\n",
    "from llama_index.core.schema import Document, Node, TextNode, QueryBundle\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from llama_index.core.schema import Document, Node, TextNode, QueryBundle, NodeWithScore  # 导入 NodeWithScore\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode\n",
    "\n",
    "\n",
    "class CustomHierarchicalRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    一个概念性的自定义层次检索器，它利用一个基础索引进行初步检索，\n",
    "    然后根据预定义的层次关系扩展相关节点。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 base_index: VectorStoreIndex,\n",
    "                 hierarchy_map: Dict[str, List[str]],  # {parent_id: [child_id1, child_id2]}\n",
    "                 nodes_map: Dict[str, Node],  # {node_id: node_object}\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._base_index = base_index\n",
    "        self._hierarchy_map = hierarchy_map\n",
    "        self._nodes_map = nodes_map  # 存储所有节点的映射，方便通过ID查找\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:  # 返回类型改为 List[NodeWithScore]\n",
    "        \"\"\"\n",
    "        实现层次化检索逻辑。\n",
    "        1. 首先使用基础索引进行初步检索。\n",
    "        2. 然后根据层次关系扩展结果。\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- 正在通过 CustomHierarchicalRetriever 检索: {query_bundle.query_str} ---\")\n",
    "\n",
    "        # 1. 使用基础索引（例如VectorStoreIndex）进行初步检索\n",
    "        # 这将利用向量相似性找到语义相关的节点\n",
    "        print(\"  - 阶段1: 通过基础索引进行初步检索...\")\n",
    "        # LlamaIndex 0.10.x+ 版本，检索器返回的是 NodeWithScore 列表\n",
    "        preliminary_node_with_scores: List[NodeWithScore] = self._base_index.as_retriever()._retrieve(query_bundle)\n",
    "        print(f\"  - 初步检索到 {len(preliminary_node_with_scores)} 个节点。\")\n",
    "\n",
    "        # 提取初步检索到的实际 Node 对象和它们的ID\n",
    "        # 保持原始的 NodeWithScore 对象，以便保留分数信息\n",
    "        preliminary_node_ids = {nws.node.id_ for nws in preliminary_node_with_scores}\n",
    "\n",
    "        # 将初步检索到的所有 NodeWithScore 对象添加到考虑集合中\n",
    "        # 键是节点ID，值是对应的 NodeWithScore 对象\n",
    "        nodes_to_consider_with_scores: Dict[str, NodeWithScore] = {\n",
    "            nws.node.id_: nws for nws in preliminary_node_with_scores\n",
    "        }\n",
    "\n",
    "        # 2. 扩展相关节点：添加父节点和子节点\n",
    "        print(\"  - 阶段2: 扩展相关节点 (父级/子级)...\")\n",
    "        for node_id in preliminary_node_ids:  # 遍历初步检索到的节点ID\n",
    "            # 查找父节点\n",
    "            for parent_id, children_ids in self._hierarchy_map.items():\n",
    "                if node_id in children_ids:\n",
    "                    if parent_id not in nodes_to_consider_with_scores and parent_id in self._nodes_map:\n",
    "                        # 如果父节点尚未在考虑列表中，则添加它，并给一个默认分数（例如 0.5）\n",
    "                        parent_node = self._nodes_map[parent_id]\n",
    "                        nodes_to_consider_with_scores[parent_id] = NodeWithScore(node=parent_node, score=0.5)\n",
    "                        print(f\"    - 扩展到父节点: '{parent_node.metadata.get('name', parent_id)}' (ID: {parent_id})\")\n",
    "                    break\n",
    "\n",
    "            # 查找子节点\n",
    "            if node_id in self._hierarchy_map:\n",
    "                for child_id in self._hierarchy_map[node_id]:\n",
    "                    if child_id not in nodes_to_consider_with_scores and child_id in self._nodes_map:\n",
    "                        # 如果子节点尚未在考虑列表中，则添加它，并给一个默认分数（例如 0.5）\n",
    "                        child_node = self._nodes_map[child_id]\n",
    "                        nodes_to_consider_with_scores[child_id] = NodeWithScore(node=child_node, score=0.5)\n",
    "                        print(f\"    - 扩展到子节点: '{child_node.metadata.get('name', child_id)}' (ID: {child_id})\")\n",
    "\n",
    "        # 将最终考虑的 NodeWithScore 对象转换为列表\n",
    "        final_relevant_node_with_scores: List[NodeWithScore] = list(nodes_to_consider_with_scores.values())\n",
    "\n",
    "        print(f\"--- 最终检索到 {len(final_relevant_node_with_scores)} 个相关节点 ---\")\n",
    "        return final_relevant_node_with_scores  # 返回 NodeWithScore 列表\n",
    "\n",
    "# --- 示例用法 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 假设有一些文档，模拟文件系统结构\n",
    "    # Documents模拟文件和文件夹内容，metadata模拟父子关系\n",
    "    documents_raw = [\n",
    "        Document(text=\"根目录: 这是主目录。\", id_=\"root_dir\"),\n",
    "        Document(text=\"项目A: 这是一个关于人工智能的项目。\", id_=\"proj_a\", metadata={\"parent_id\": \"root_dir\"}),\n",
    "        Document(text=\"文档1: 项目A的需求文档。\", id_=\"doc1\", metadata={\"parent_id\": \"proj_a\"}),\n",
    "        Document(text=\"代码文件: 这是项目A的核心代码。\", id_=\"code_file\", metadata={\"parent_id\": \"proj_a\"}),\n",
    "        Document(text=\"项目B: 这是一个关于数据分析的项目。\", id_=\"proj_b\", metadata={\"parent_id\": \"root_dir\"}),\n",
    "        Document(text=\"数据报告: 项目B的最终数据分析报告。\", id_=\"data_report\", metadata={\"parent_id\": \"proj_b\"}),\n",
    "        Document(text=\"子目录X: 这是一个子目录。\", id_=\"subdir_x\", metadata={\"parent_id\": \"proj_a\"}),\n",
    "        Document(text=\"图片集: 项目A的视觉素材。\", id_=\"images\", metadata={\"parent_id\": \"subdir_x\"})\n",
    "    ]\n",
    "\n",
    "    # 构建层次关系和节点映射 (在实际应用中，这可能是从外部数据源加载的)\n",
    "    hierarchy_map: Dict[str, List[str]] = {}\n",
    "    nodes_map: Dict[str, Node] = {}\n",
    "\n",
    "    processed_nodes: List[TextNode] = []\n",
    "    for doc in documents_raw:\n",
    "        parts = doc.text.split(\":\", 1)\n",
    "        name = parts[0].strip() if len(parts) > 1 else doc.text.strip()\n",
    "        content = parts[1].strip() if len(parts) > 1 else doc.text.strip()\n",
    "\n",
    "        # 创建 TextNode\n",
    "        node = TextNode(text=content, id_=doc.id_, metadata={\"name\": name, \"parent_id\": doc.metadata.get(\"parent_id\")})\n",
    "        processed_nodes.append(node)\n",
    "        nodes_map[node.id_] = node # 存储所有节点的映射\n",
    "\n",
    "        # 构建层次关系\n",
    "        parent_id = doc.metadata.get(\"parent_id\")\n",
    "        if parent_id:\n",
    "            if parent_id not in hierarchy_map:\n",
    "                hierarchy_map[parent_id] = []\n",
    "            hierarchy_map[parent_id].append(doc.id_)\n",
    "\n",
    "    print(\"--- 原始文档转换为节点并构建层次关系完成 ---\")\n",
    "    print(\"层次关系:\", hierarchy_map)\n",
    "    print(\"节点数量:\", len(nodes_map))\n",
    "\n",
    "    # 1. 创建一个标准的基础索引 (例如 VectorStoreIndex)\n",
    "    # LlamaIndex 会自动对这些节点进行嵌入并存储\n",
    "    base_index = VectorStoreIndex(nodes=processed_nodes)\n",
    "    print(\"\\n--- 基础向量索引构建完成 ---\")\n",
    "\n",
    "    # 2. 初始化自定义层次检索器\n",
    "    custom_retriever = CustomHierarchicalRetriever(\n",
    "        base_index=base_index,\n",
    "        hierarchy_map=hierarchy_map,\n",
    "        nodes_map=nodes_map\n",
    "    )\n",
    "\n",
    "    # 3. 将自定义检索器连接到 QueryEngine\n",
    "    # 响应合成器：负责将检索到的节点和查询一起传给LLM，生成最终答案\n",
    "    response_synthesizer = get_response_synthesizer(\n",
    "        response_mode=ResponseMode.COMPACT # 修正：使用枚举类型\n",
    "    )\n",
    "\n",
    "    # 创建查询引擎\n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        retriever=custom_retriever,\n",
    "        response_synthesizer=response_synthesizer\n",
    "    )\n",
    "\n",
    "    # 4. 执行查询\n",
    "    print(\"\\n--- 第一次查询 ---\")\n",
    "    response = query_engine.query(\"关于人工智能的项目有什么？\")\n",
    "    print(f\"查询结果: {response}\")\n",
    "\n",
    "    print(\"\\n--- 第二次查询 ---\")\n",
    "    response = query_engine.query(\"给我看看项目A的文档。\")\n",
    "    print(f\"查询结果: {response}\")\n",
    "\n",
    "    print(\"\\n--- 第三次查询 ---\")\n",
    "    response = query_engine.query(\"根目录里有什么内容？\")\n",
    "    print(f\"查询结果: {response}\")\n",
    "\n",
    "    print(\"\\n--- 第四次查询 (更具体) ---\")\n",
    "    response = query_engine.query(\"项目A的视觉素材是什么？\")\n",
    "    print(f\"查询结果: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "复用 了 VectorStoreIndex 的向量存储和检索能力，然后通过 定制 Retriever 来添加层次感知的检索逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG性能优化策略\n",
    "\n",
    "  \n",
    "构建一个功能完善的RAG系统只是第一步，要使其在生产环境中表现出色，还需要进行精细的性能优化。LlamaIndex提供了多种策略来提升RAG的性能和准确性。  \n",
    "\n",
    "1. Prompt Engineering (提示工程)：  \n",
    "提示工程是解决LLM输出中出现“幻觉”或格式不佳问题的首要且关键的方法。这包括：  \n",
    "- **检查和定制提示**：开发者可以检查并根据具体需求调整RAG工作流中使用的提示，例如问答提示，确保其清晰、明确，并引导LLM生成高质量的响应。  \n",
    "- **添加提示函数**：对于更高级的控制，可以添加提示函数，允许动态注入少量示例（few-shot examples）或对注入的输入进行预处理，从而进一步优化LLM的行为。  \n",
    "\n",
    "2. Embeddings (嵌入模型选择)：  \n",
    "选择合适的嵌入模型对RAG系统的整体性能具有决定性影响。  \n",
    "- **模型选择**：开发者可以考虑使用除OpenAI默认的text-embedding-ada-002模型之外的其他选项，特别是当需要扩展到本地服务器或需要针对特定语言优化的模型时。LlamaIndex支持多种嵌入API、本地嵌入模型执行和自托管服务器。  \n",
    "- **重要注意事项**：如果更换嵌入模型，必须重新索引所有数据，并且在索引和查询时应确保使用相同的嵌入模型，以保证语义一致性。  \n",
    "\n",
    "3. Chunk Sizes (分块大小)：  \n",
    "根据数据类型和检索结果调整文档分块的大小和重叠度，对于优化检索效果至关重要。  \n",
    "- **默认设置与影响**：当文档被摄取到索引中时，它们会被分割成带有一定重叠的块。LlamaIndex的默认分块大小为1024个token，重叠为20个token。改变这些参数会影响生成的嵌入。  \n",
    "- **大小选择**：较小的分块通常提供更精确的嵌入，有助于回答非常具体的问题；而较大的分块可能更通用，但可能会在检索细粒度信息时遗漏细节。LlamaIndex提供了关于分块大小的评估研究，以帮助开发者找到最佳平衡。  \n",
    "- **similarity_top_k参数**：在调整分块大小时，通常建议相应调整similarity_top_k参数，以检索到足够具有代表性的数据量。例如，如果分块大小减半，similarity_top_k可能需要从2增加到4。  \n",
    "\n",
    "LlamaIndex对RAG优化策略的详细指导，特别是对分块大小和嵌入模型选择的强调，表明其不仅提供功能，更提供“如何用好”的专业知识。这反映了RAG系统性能对这些底层配置的敏感性，并指导开发者进行精细化调优，以应对现实世界中的性能挑战。  \n",
    "\n",
    "4. 混合搜索与元数据过滤 (Hybrid Search and Metadata Filtering)：  \n",
    "这些是高级检索策略，旨在提升RAG系统的准确性和相关性。  \n",
    "- **混合搜索 (Hybrid Search)**：混合搜索结合了语义搜索（基于嵌入相似性）和关键词搜索，以解决纯语义搜索可能无法返回匹配关键词文本块的问题。这种方法可以提高检索的召回率和精确度。  \n",
    "  - **实现方式**：可以通过使用原生支持混合搜索的向量数据库来实现，LlamaIndex提供了支持此功能的向量存储列表。  \n",
    "  - **本地BM25**：另一种方法是设置本地的BM25（Best Match 25）机制，这是一种基于词频的关键词搜索算法，与向量搜索结合使用。  \n",
    "\n",
    "- **元数据过滤 (Metadata Filters)**：在索引文档之前，为其附加结构化元数据非常有用。这些元数据不仅可以用于跟踪答案的来源，更重要的是，可以在查询时用于过滤数据，从而显著提高检索的相关性。  \n",
    "  - **自动过滤**：对于GPT-4等高级LLM，如果向量数据库支持过滤，LlamaIndex可以利用AutoVectorRetriever让LLM在查询时自动生成过滤器，进一步简化和优化检索过程。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 基础向量索引构建完成 ---\n",
      "--- 混合检索器 (QueryFusionRetriever) 创建完成 ---\n",
      "--- 查询引擎 (带元数据后处理) 创建完成 ---\n",
      "\n",
      "--- 示例查询 1: IBM 2023年收入 ---\n",
      "Generated queries:\n",
      "1. IBM 2023年全年财报收入数据\n",
      "2. IBM 2023年第四季度及年度财务报告\n",
      "3. IBM 2023年营收总额及业务部门细分\n",
      "查询结果: IBM在2023年的收入实现了显著增长，但具体数额未在提供的信息中提及。\n",
      "\n",
      "--- 示例查询 2: 2023年发布文档的主要话题 ---\n",
      "Generated queries:\n",
      "1. 2023年全球重要政策文件发布的主要议题有哪些？\n",
      "2. 2023年企业或机构发布的年度报告重点关注了哪些领域？\n",
      "3. 2023年学术研究或白皮书中高频出现的关键词和话题是什么？\n",
      "查询结果: 2023年发布文档的主要话题包括数据隐私保护和人工智能的最新进展。\n",
      "\n",
      "--- 示例查询 3: 关于人工智能和LLM的信息 ---\n",
      "Generated queries:\n",
      "1. 人工智能和大型语言模型（LLM）的最新研究进展有哪些？\n",
      "2. 近年来人工智能领域在LLM技术上的重大突破是什么？\n",
      "3. 人工智能和LLM的发展趋势及未来应用方向有哪些？\n",
      "查询结果: 近年来，人工智能领域取得了显著进展，特别是在大型语言模型（LLM）方面。这些模型的应用范围正在不断扩大，为科技发展带来了新的动力。同时，2023年见证了人工智能技术的重要突破，这些进展正在推动整个科技领域的创新。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "from llama_index.core.response_synthesizers.type import ResponseMode  # 导入 ResponseMode\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "# --- 1. 准备数据和构建基础索引 ---\n",
    "documents = [\n",
    "    Document(text=\"IBM在2023年的收入达到了显著增长。\", metadata={\"year\": 2023, \"company\": \"IBM\"}),\n",
    "    Document(text=\"人工智能的最新进展正在推动科技发展。\", metadata={\"topic\": \"AI\", \"published_year\": 2023}),\n",
    "    Document(text=\"大型语言模型（LLM）的应用越来越广泛。\", metadata={\"topic\": \"LLM\", \"author\": \"John Doe\"}),\n",
    "    Document(text=\"LlamaIndex是一个强大的RAG框架。\", metadata={\"category\": \"RAG\", \"release_year\": 2022}),\n",
    "    Document(text=\"数据隐私保护是当前技术领域的重要议题。\", metadata={\"topic\": \"Privacy\", \"status\": \"ongoing\"})\n",
    "]\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"--- 基础向量索引构建完成 ---\")\n",
    "\n",
    "# --- 2. 创建混合检索器 (QueryFusionRetriever) ---\n",
    "vector_retriever = index.as_retriever(similarity_top_k=2)\n",
    "bm25_retriever = BM25Retriever.from_defaults(docstore=index.docstore, similarity_top_k=2)\n",
    "\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=4,\n",
    "    num_queries=4,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=False,\n",
    "    verbose=True\n",
    ")\n",
    "print(\"--- 混合检索器 (QueryFusionRetriever) 创建完成 ---\")\n",
    "\n",
    "# --- 3. 创建带有元数据后处理的查询引擎 ---\n",
    "metadata_postprocessor = MetadataReplacementPostProcessor(\n",
    "    target_metadata_key=\"text\"  # 将元数据的值替换到节点文本中，以供LLM阅读\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=ResponseMode.COMPACT,  # 使用 ResponseMode.COMPACT\n",
    "    llm=Settings.llm  # 确保 response_synthesizer 使用全局设置的LLM\n",
    ")\n",
    "\n",
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,  # 使用我们定制的混合检索器\n",
    "    node_postprocessors=[metadata_postprocessor],  # 应用元数据后处理器\n",
    "    response_synthesizer=response_synthesizer  # 传入手动创建的 response_synthesizer\n",
    ")\n",
    "print(\"--- 查询引擎 (带元数据后处理) 创建完成 ---\")\n",
    "\n",
    "# --- 4. 示例查询 ---\n",
    "print(\"\\n--- 示例查询 1: IBM 2023年收入 ---\")\n",
    "response_1 = query_engine.query(\"IBM在2023年的收入是多少？\")\n",
    "print(f\"查询结果: {response_1}\")\n",
    "\n",
    "print(\"\\n--- 示例查询 2: 2023年发布文档的主要话题 ---\")\n",
    "response_2 = query_engine.query(\"2023年发布文档的主要话题是什么？\")\n",
    "print(f\"查询结果: {response_2}\")\n",
    "\n",
    "print(\"\\n--- 示例查询 3: 关于人工智能和LLM的信息 ---\")\n",
    "response_3 = query_engine.query(\"给我说说关于人工智能和LLM的进展。\")\n",
    "print(f\"查询结果: {response_3}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex对混合搜索和元数据过滤的强调，反映了RAG技术从“简单相似性匹配”向“复杂语义理解与结构化过滤”的演进。这对于处理企业级复杂数据集至关重要，因为实际数据往往包含丰富的结构化元数据，而不仅仅是纯文本。这些高级策略使得RAG系统能够提供更精确、更符合业务逻辑的答案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 高级查询引擎与检索策略\n",
    "LlamaIndex在查询引擎和检索策略方面的深度和广度，使其能够处理远超简单问答的复杂信息需求，实现更复杂的推理和信息整合。\n",
    "\n",
    "#### 高级查询引擎\n",
    "LlamaIndex提供了多种高级查询引擎模块，用于处理复杂查询，这些引擎可以相互组合以实现更强大的功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 多个底层查询引擎准备完成 ---\n",
      "--- 查询引擎工具包装完成 ---\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from llama_index.core import VectorStoreIndex, Settings, Document\n",
    "from llama_index.core.query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "# pip install llama-index-llms-deepseek\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "\n",
    "# --- 0. 配置 LlamaIndex 全局设置 ---\n",
    "Settings.llm = DeepSeek(model=\"deepseek-chat\")\n",
    "Settings.embed_model = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\"\n",
    ")\n",
    "\n",
    "# --- 1. 准备多个数据源及对应的查询引擎 ---\n",
    "# 数据源 1: 销售数据文档\n",
    "# 为了演示，我们直接在代码中定义文档内容，实际情况会从文件加载\n",
    "sales_docs = [\n",
    "    Document(text=\"2023年第一季度，公司A的销售额增长了15%。\"),\n",
    "    Document(text=\"公司B在北美市场的销售表现强劲，尤其是产品X。\")\n",
    "]\n",
    "sales_index = VectorStoreIndex.from_documents(sales_docs)\n",
    "sales_query_engine = sales_index.as_query_engine()\n",
    "\n",
    "# 数据源 2: 产品技术文档\n",
    "product_docs = [\n",
    "    Document(text=\"产品P拥有先进的AI算法，支持实时图像识别功能。\"),\n",
    "    Document(text=\"产品Q的用户手册详细说明了安装步骤和故障排除。\")\n",
    "]\n",
    "product_index = VectorStoreIndex.from_documents(product_docs)\n",
    "product_query_engine = product_index.as_query_engine()\n",
    "\n",
    "# 数据源 3: 人力资源政策文档\n",
    "hr_docs = [\n",
    "    Document(text=\"公司休假政策规定，员工每年享有15天带薪年假。\"),\n",
    "    Document(text=\"新员工入职培训涵盖了公司文化和核心价值观。\")\n",
    "]\n",
    "hr_index = VectorStoreIndex.from_documents(hr_docs)\n",
    "hr_query_engine = hr_index.as_query_engine()\n",
    "\n",
    "print(\"--- 多个底层查询引擎准备完成 ---\")\n",
    "\n",
    "\n",
    "# --- 2. 将查询引擎包装成工具，并提供详细的描述 ---\n",
    "# 路由查询引擎的 LLM 会根据这些描述来选择最合适的工具\n",
    "# --- 2. 将查询引擎包装成工具，并提供详细的描述 ---\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=sales_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"sales_data\",\n",
    "            description=(\n",
    "                \"这个工具提供了**公司销售额、市场增长、季度业绩、收入数据、销售表现、客户增长**等信息。\"\n",
    "                \"当你需要分析**财务指标、市场份额、销售趋势**时，请选择此工具。\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=product_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"product_info\",\n",
    "            description=(\n",
    "                \"这个工具提供了**产品功能、技术规格、用户手册、安装指南、故障排除、产品特性、版本信息、研发进展**等信息。\"\n",
    "                \"当你需要了解**某个具体产品的功能细节、如何使用产品或解决产品问题**时，请选择此工具。\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=hr_query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"hr_policies\",\n",
    "            description=(\n",
    "                \"这个工具提供了**员工福利、休假政策、入职流程、公司规章制度、人力资源、招聘信息、培训计划**等信息。\"\n",
    "                \"当你需要查询**与员工权益、公司内部管理或人力相关**的问题时，请选择此工具。\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "print(\"--- 查询引擎工具包装完成 ---\")\n",
    "\n",
    "\n",
    "def exec(router_query_engine):\n",
    "    # --- 4. 执行查询 ---\n",
    "    print(\"\\n--- 查询示例 1: 关于销售额 ---\")\n",
    "    query_1 = \"2023年公司A的销售额表现如何？\"\n",
    "    response_1 = router_query_engine.query(query_1)\n",
    "    print(f\"原始查询: {query_1}\")\n",
    "    # 移除尝试从 response.metadata 中获取 tool_name 的代码，转而观察 verbose 输出\n",
    "    print(f\"查询结果: {response_1}\")\n",
    "    \n",
    "    print(\"\\n--- 查询示例 2: 关于产品功能 ---\")\n",
    "    query_2 = \"产品P支持哪些AI功能？\"\n",
    "    response_2 = router_query_engine.query(query_2)\n",
    "    print(f\"原始查询: {query_2}\")\n",
    "    print(f\"查询结果: {response_2}\")\n",
    "    \n",
    "    print(\"\\n--- 查询示例 3: 关于员工年假 ---\")\n",
    "    query_3 = \"公司年假政策是什么？\"\n",
    "    response_3 = router_query_engine.query(query_3)\n",
    "    print(f\"原始查询: {query_3}\")\n",
    "    print(f\"查询结果: {response_3}\")\n",
    "    \n",
    "    print(\"\\n--- 查询示例 4: 跨领域或模糊查询 ---\")\n",
    "    query_4 = \"告诉我一些公司的最新进展。\"\n",
    "    response_4 = router_query_engine.query(query_4)\n",
    "    print(f\"原始查询: {query_4}\")\n",
    "    print(f\"查询结果: {response_4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Router Query Engine (路由查询引擎)**：能够根据查询内容智能地将查询路由到不同的底层查询引擎或数据源。这对于处理涉及多个领域或数据类型的复杂问题非常有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 路由查询引擎创建完成 ---\n",
      "\n",
      "--- 查询示例 1: 关于销售额 ---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks about '2023年公司A的销售额表现如何？', which translates to 'How was Company A's sales performance in 2023?'. This directly relates to sales performance and financial metrics, which are covered in choice (1) that includes '公司销售额' (company sales), '销售表现' (sales performance), and '财务指标' (financial indicators)..\n",
      "\u001b[0m原始查询: 2023年公司A的销售额表现如何？\n",
      "查询结果: 2023年第一季度，公司A的销售额实现了15%的增长。\n",
      "\n",
      "--- 查询示例 2: 关于产品功能 ---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks about the AI functions supported by a product, which falls under the category of 'product functionality and technical specifications' as described in choice 2..\n",
      "\u001b[0m原始查询: 产品P支持哪些AI功能？\n",
      "查询结果: 产品P支持实时图像识别功能。\n",
      "\n",
      "--- 查询示例 3: 关于员工年假 ---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 2: The question '公司年假政策是什么？' is related to employee benefits and company policies, which are covered in choice (3) that includes '员工福利、休假政策、公司规章制度'..\n",
      "\u001b[0m原始查询: 公司年假政策是什么？\n",
      "查询结果: 员工每年享有15天带薪年假。\n",
      "\n",
      "--- 查询示例 4: 跨领域或模糊查询 ---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 0: The question asks for 'company's latest progress,' which likely refers to financial or sales performance. Choice 1 provides information on sales, market growth, and quarterly performance, which aligns with the query..\n",
      "\u001b[0m原始查询: 告诉我一些公司的最新进展。\n",
      "查询结果: 公司A在第一季度实现了15%的销售额增长。同时，公司B在北美市场表现突出，特别是其产品X的销售情况非常强劲。\n"
     ]
    }
   ],
   "source": [
    "# --- 创建路由查询引擎 ---\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(llm=Settings.llm),\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    verbose=True # 打印路由过程的详细信息\n",
    ")\n",
    "print(\"--- 路由查询引擎创建完成 ---\")\n",
    "exec(query_engine )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Sub Question Query Engine (子问题查询引擎)**：擅长将一个复杂的、多方面的问题分解成一系列更小、更易于管理的子问题。每个子问题可以独立地查询相关数据源，然后将中间答案组合起来形成最终的综合回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 查询示例 1: 关于销售额 ---\n",
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[sales_data] Q: 2023年公司A的总销售额是多少？\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[sales_data] Q: 2023年公司A的季度销售额数据是什么？\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[sales_data] Q: 2023年公司A的销售额同比增长率是多少？\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[sales_data] A: 根据提供的信息，无法确定2023年公司A的总销售额具体是多少。\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[sales_data] A: 2023年第一季度，公司A的销售额同比增长了15%。\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[sales_data] A: 2023年第一季度，公司A的销售额实现了15%的增长。\n",
      "\u001b[0m原始查询: 2023年公司A的销售额表现如何？\n",
      "查询结果: 2023年第一季度，公司A的销售额实现了15%的同比增长。目前没有关于全年或其他季度的具体销售数据。\n",
      "\n",
      "--- 查询示例 2: 关于产品功能 ---\n",
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[product_info] Q: 产品P的技术规格中提到的AI功能有哪些？\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[product_info] Q: 产品P的用户手册中是否详细描述了其AI功能？\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[product_info] Q: 产品P的研发进展中是否提到了新增的AI功能？\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[product_info] A: 产品P的技术规格中提到其AI功能包括实时图像识别。\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[product_info] A: 产品P已经具备先进的AI算法，并支持实时图像识别功能。\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[product_info] A: 没有信息表明产品P的用户手册中详细描述了其AI功能。\n",
      "\u001b[0m原始查询: 产品P支持哪些AI功能？\n",
      "查询结果: 产品P支持实时图像识别功能。\n",
      "\n",
      "--- 查询示例 3: 关于员工年假 ---\n",
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[hr_policies] Q: 公司的年假政策具体包括哪些内容？\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[hr_policies] Q: 员工年假的天数是如何计算的？\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[hr_policies] Q: 年假是否可以累积或延期使用？\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[hr_policies] Q: 年假申请的具体流程是什么？\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[hr_policies] A: 目前没有提供关于年假申请具体流程的相关信息。建议咨询人力资源部门或查阅公司内部员工手册以获取详细指导。\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[hr_policies] A: 员工每年享有15天带薪年假。\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[hr_policies] A: 员工每年享有15天的带薪年假。\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[hr_policies] A: 员工每年享有15天带薪年假。关于年假是否可以累积或延期使用，当前信息中未提及具体规定。\n",
      "\u001b[0m原始查询: 公司年假政策是什么？\n",
      "查询结果: 员工每年享有15天带薪年假。关于年假的累积、延期使用以及申请流程等具体规定，建议咨询人力资源部门或查阅公司内部员工手册获取详细信息。\n",
      "\n",
      "--- 查询示例 4: 跨领域或模糊查询 ---\n",
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[sales_data] Q: 公司最近的销售额和市场增长情况如何？\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[product_info] Q: 公司最近发布了哪些新产品或更新了哪些产品功能？\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[hr_policies] Q: 公司最近在人力资源方面有哪些新的政策或福利？\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[sales_data] A: 公司最近的销售额在2023年第一季度实现了15%的增长。在北美市场，某款产品的销售表现尤为强劲。\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[product_info] A: 公司最近推出了具备实时图像识别能力的AI产品P，并更新了产品Q的用户手册，其中包含了详细的安装指南和故障解决方案。\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[hr_policies] A: 新员工入职时会接受关于公司文化和核心价值观的培训。此外，员工每年可以享受15天的带薪年假。\n",
      "\u001b[0m原始查询: 告诉我一些公司的最新进展。\n",
      "查询结果: 公司在2023年第一季度实现了15%的销售额增长，尤其在北美市场有产品表现突出。近期推出了具备实时图像识别功能的新AI产品P，并对产品Q的用户手册进行了更新，增加了安装指南和故障解决方案等内容。在人力资源方面，新员工将接受公司文化和核心价值观培训，所有员工每年可享受15天带薪年假。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools.query_engine import QueryEngineTool\n",
    "\n",
    "query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    verbose=True\n",
    ")\n",
    "exec(query_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Multi-Step Query Engine (多步骤查询引擎)**：处理需要多步或多次迭代才能解决的查询。它能够维护查询过程中的状态，并根据前一步的输出调整后续的查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023年，该公司的收入实现了显著增长，但具体的销售额和市场增长数据没有详细披露。如需更准确的信息，建议参考该公司发布的官方财务报告或行业分析数据。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.query.query_transform import DecomposeQueryTransform\n",
    "from llama_index.core.query_engine import MultiStepQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# 1. 创建 query_transform，用于问题拆解\n",
    "query_transform = DecomposeQueryTransform(llm=Settings.llm)\n",
    "\n",
    "# 3. 构造多步骤查询引擎\n",
    "query_engine = MultiStepQueryEngine(\n",
    "    query_engine=hr_query_engine,\n",
    "    query_transform=query_transform\n",
    ")\n",
    "\n",
    "# 4. 执行查询\n",
    "response = query_engine.query(\"公司最近的销售额和市场增长情况如何？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **SQL Router Query Engine (SQL路由查询引擎) / Text-to-SQL**：允许用户使用自然语言查询存储在SQL数据库中的结构化数据，LlamaIndex能够将自然语言查询转换为相应的SQL语句。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-readers-wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据查询结果，目前数据库中没有关于城市人口的数据记录，因此无法列出人口最多的5个城市。如果您有其他相关信息或需要查询不同的数据，请提供更多细节。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SQLDatabase\n",
    "from sqlalchemy import create_engine\n",
    "from llama_index.core.query_engine import NLSQLTableQueryEngine\n",
    "\n",
    "# 1. 创建数据库引擎（例如 SQLite）\n",
    "engine = create_engine(\"sqlite:///example.db\")\n",
    "\n",
    "# from sqlalchemy import Table, Column, Integer, String, MetaData\n",
    "\n",
    "# metadata = MetaData()\n",
    "\n",
    "# city_stats = Table(\n",
    "#     \"city_stats\",\n",
    "#     metadata,\n",
    "#     Column(\"id\", Integer, primary_key=True),\n",
    "#     Column(\"city_name\", String),\n",
    "#     Column(\"population\", Integer),\n",
    "# )\n",
    "\n",
    "# # 创建表\n",
    "# metadata.create_all(engine)\n",
    "\n",
    "# 2. 创建 LlamaIndex 的 SQLDatabase 封装对象\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "\n",
    "# 3. 构建自然语言 SQL 查询引擎\n",
    "query_engine = NLSQLTableQueryEngine(\n",
    "    sql_database=sql_database,\n",
    "    tables=[\"city_stats\"],  # 可以指定具体表\n",
    ")\n",
    "\n",
    "# 4. 执行自然语言查询\n",
    "response = query_engine.query(\"列出人口最多的5个城市\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **CitationQueryEngine (引用查询引擎)**：在生成响应的同时，提供所引用信息的来源和具体引用，这对于需要验证信息来源的场景（如法律、医疗）至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "答案： None of the provided sources contain information about the core business of any company. The sources only describe the educational backgrounds and activities of individuals (John Smith, Sarah Lee, Mike Chen, and Emily Davis) during their time at university.\n",
      "引用： > Source (Doc id: 51b017db-9bcf-4b0b-9c5e-d45be54e8fe7): Source 1:\n",
      "John Smith attended Stanford University from 2010 to 2014, where he majored in Computer...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.query_engine import CitationQueryEngine\n",
    "\n",
    "# 1. 加载文档\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# 2. 创建向量索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 3. 从索引获取底层 query engine\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# 4. 创建 CitationQueryEngine，传入基础 query_engine\n",
    "citation_engine = CitationQueryEngine.from_args(query_engine=query_engine, index=index)\n",
    "\n",
    "# 5. 查询问题\n",
    "response = citation_engine.query(\"公司的核心业务是什么？\")\n",
    "\n",
    "# 6. 输出答案和引用\n",
    "print(\"答案：\", response.response)\n",
    "print(\"引用：\", response.get_formatted_sources())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Recursive Retriever + Query Engine (递归检索器+查询引擎)**：这是一种强大的组合，用于递归地检索信息，特别适用于处理具有嵌套或层次结构的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: 请告诉我产品A的具体功能和限制？\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: 产品A的限制是电池续航时间较短，且不支持5G网络。\n",
      "\u001b[0m产品A的功能在现有信息中未具体说明。其限制包括电池续航时间较短，并且不支持5G网络连接。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 1. 加载文档并创建索引\n",
    "documents = [\n",
    "    Document(text=\"产品A是一款智能手机，具有高分辨率摄像头和强大的处理器。\"),\n",
    "    Document(text=\"产品A的功能包括拍照、视频录制和高速上网。\"),\n",
    "    Document(text=\"产品A的限制是电池续航时间较短，且不支持5G网络。\"),\n",
    "]\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 2. 从索引中获取所有 TextNode\n",
    "all_nodes = []\n",
    "for doc in documents:\n",
    "    nodes = index.docstore.get_nodes(doc.id_)\n",
    "    all_nodes.extend(nodes)\n",
    "\n",
    "# 3. 构建 node_dict（必须是 node_id -> node）\n",
    "node_dict = {node.node_id: node for node in all_nodes}\n",
    "\n",
    "# 4. 构建递归检索器\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    root_id=\"default\",\n",
    "    retriever_dict={\"default\": index.as_retriever()},\n",
    "    node_dict=node_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5. 构建查询引擎\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=recursive_retriever)\n",
    "\n",
    "# 6. 查询\n",
    "response = query_engine.query(\"产品A的功能和限制分别是什么？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 高级检索策略\n",
    "除了查询引擎，LlamaIndex还提供了多种高级检索策略，以优化从知识库中获取相关信息的过程。\n",
    "\n",
    "> LlamaIndex在查询引擎和检索策略上的“高级”和“可组合”特性，反映了其对复杂信息需求的深刻理解。这使得LLM应用能够处理多源、多格式、多层次的数据，并进行复杂的推理，远超简单的关键词匹配。这种能力对于构建真正智能的知识助手和自主代理至关重要，因为它能够分解问题、路由查询、递归检索和融合结果，从而处理高度复杂、多方面的分析任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **递归检索 (Recursive Retrieval)**：能够检索小块信息，同时传递相关联的上下文。例如，在处理PDF文档中的表格时，可以递归地检索表格内容及其周围的文本上下文，或者在知识图谱中递归遍历节点。\n",
    "\n",
    "适用场景：\n",
    "- 多层级嵌套信息（如知识图谱、结构化 PDF、章节文档）\n",
    "- 你希望能从一个“锚点”出发逐步扩展获取更多上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mRetrieving with query id None: 产品A的功能与限制分别有哪些？\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: 产品A的限制是电池续航时间较短，且不支持5G网络。\n",
      "\u001b[0m\u001b[1;3;38;5;200mRetrieving text node: 产品A的功能包括拍照、视频录制和高速上网。\n",
      "\u001b[0m产品A的功能包括拍照、视频录制和高速上网。其限制是电池续航时间较短，且不支持5G网络。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.retrievers import RecursiveRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 1. 构造文档和索引\n",
    "documents = [\n",
    "    Document(text=\"产品A是一款智能手机，具有高分辨率摄像头和强大的处理器。\"),\n",
    "    Document(text=\"产品A的功能包括拍照、视频录制和高速上网。\"),\n",
    "    Document(text=\"产品A的限制是电池续航时间较短，且不支持5G网络。\"),\n",
    "]\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 2. 从索引中获取 retriever\n",
    "base_retriever = index.as_retriever()\n",
    "\n",
    "# 正确获取所有 TextNode\n",
    "docstore = index.storage_context.docstore\n",
    "node_ids = list(docstore.docs.keys())\n",
    "nodes = [docstore.get_node(node_id) for node_id in node_ids]\n",
    "node_dict = {node.node_id: node for node in nodes}\n",
    "\n",
    "# 3. 构造递归检索器\n",
    "recursive_retriever = RecursiveRetriever(\n",
    "    root_id=\"default\",\n",
    "    retriever_dict={\"default\": base_retriever},\n",
    "    node_dict=node_dict,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 4. 构造 QueryEngine\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=recursive_retriever)\n",
    "\n",
    "# 5. 执行查询\n",
    "response = query_engine.query(\"产品A的功能与限制分别有哪些？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **自动检索 (Auto-Retrieval)**：通过结合语义搜索和结构化过滤来执行半结构化查询。LLM可以根据查询内容自动生成过滤器，从而在向量搜索之前对数据进行初步筛选。\n",
    "\n",
    "适用场景：\n",
    "- 数据有结构化字段，如 \"category\": \"finance\"、\"region\": \"Asia\"\n",
    "- 想结合语义检索和结构过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产品A的主要功能包括高分辨率摄像头和强大的处理器。目前没有提到具体的功能限制。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.indices.vector_store import VectorIndexAutoRetriever\n",
    "from llama_index.core.vector_stores import VectorStoreInfo, MetadataInfo\n",
    "\n",
    "# 使用 Document 对象\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"产品A是一款智能手机，具有高分辨率摄像头和强大的处理器。\",\n",
    "        metadata={\"category\": \"产品\", \"region\": \"华南\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 构建索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 创建 VectorIndexAutoRetriever（自动解析 query + metadata）\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    index=index,\n",
    "    vector_store_info=VectorStoreInfo(\n",
    "        content_info=\"产品说明\",\n",
    "        metadata_info=[\n",
    "            MetadataInfo(name=\"category\", type=\"str\", description=\"产品类别\"),\n",
    "            MetadataInfo(name=\"region\", type=\"str\", description=\"地区\"),\n",
    "        ],\n",
    "    )\n",
    ")\n",
    "\n",
    "# 构造 QueryEngine 并查询\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
    "response = query_engine.query(\"产品A的功能与限制分别有哪些？\")\n",
    "\n",
    "# 输出结果\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **查询转换 (Query Transformations) / Query Fusion**：例如Reciprocal Rerank Fusion，它通过生成多个搜索查询（例如，原始查询的变体或子查询）并融合/重新排序这些查询的结果，从而提高整体检索质量和鲁棒性。\n",
    "\n",
    "适用场景：\n",
    "- 一个复杂问题可以被分成多个子问题（或多个问法）\n",
    "- 提高召回率，改善排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我无法提供关于公司过去三年利润增长趋势的信息，因为给定的背景资料中并未包含相关财务数据。背景信息仅介绍了产品A是一款具有高分辨率摄像头和强大处理器的智能手机，位于华南地区。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "query_engine = index.as_query_engine()\n",
    "query_transform = HyDEQueryTransform(llm=Settings.llm)\n",
    "\n",
    "query_engine = TransformQueryEngine(\n",
    "    query_engine=base_engine,\n",
    "    query_transform=query_transform,\n",
    ")\n",
    "response = query_engine.query(\"请问公司过去三年的利润增长趋势如何？\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **自动合并检索（Auto Merging Retriever）**：包括自动合并检索（Auto Merging Retriever），它能够动态地将小的检索到的块合并成更大的、更具上下文的块，以提供更丰富的LLM输入；\n",
    "\n",
    "适用场景：\n",
    "- 文档被切割太碎，导致上下文不足\n",
    "\n",
    "- 需要“合并块”来提供更大上下文\n",
    "\n",
    "原理：基于检索结果上下文自动决定合并范围（window size）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的信息，无法回答公司在环保方面的举措。当前上下文仅介绍了华南地区一款智能手机产品的硬件配置，没有提及任何与环保相关的措施或政策。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "retriever = AutoMergingRetriever(vector_retriever=index.as_retriever(), storage_context=index.storage_context)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever=retriever)\n",
    "\n",
    "response = query_engine.query(\"公司在环保方面的主要举措有哪些？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **元数据替换（Metadata Replacement）**\n",
    "\n",
    "适用场景：\n",
    "- 想让 LLM 在检索结果中用更丰富/人类可读的 metadata 替换原始 chunk\n",
    "\n",
    "- 提升 LLM 的理解力（比如 ID -> Title）\n",
    "\n",
    "用法：在构建节点（TextNode）时指定替代元数据，然后在 query_engine 配置中设置 `use_metadata=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最适合办公室使用的产品是环保空调。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# 1. 构造带替代 metadata 的 TextNode\n",
    "nodes = [\n",
    "    TextNode(\n",
    "        text=\"产品A是一款智能手机，具有高分辨率摄像头和强大的处理器。\",\n",
    "        metadata={\"id\": \"p001\", \"title\": \"产品A - 智能手机\"},\n",
    "        # 👇 设置替代 metadata，LLM 更易理解\n",
    "        metadata_seo={\"title\": \"产品A - 智能手机\"}\n",
    "    ),\n",
    "    TextNode(\n",
    "        text=\"产品B是一款环保空调，低能耗，适合办公室使用。\",\n",
    "        metadata={\"id\": \"p002\", \"title\": \"产品B - 环保空调\"},\n",
    "        metadata_seo={\"title\": \"产品B - 环保空调\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 2. 创建索引\n",
    "index = VectorStoreIndex(nodes)\n",
    "\n",
    "# 3. 创建查询引擎，并启用 use_metadata\n",
    "query_engine = index.as_query_engine(use_metadata=True)\n",
    "\n",
    "# 4. 查询\n",
    "response = query_engine.query(\"哪款产品最适合办公室使用？\")\n",
    "\n",
    "# 5. 输出答案\n",
    "print(response.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **可组合检索器（Composable Retrievers）**\n",
    "\n",
    "适用场景：多种检索方式组合（向量、关键词、结构化、递归……）\n",
    "\n",
    "自定义检索策略/流程控制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "产品A的主要特点包括高分辨率摄像头和强大的处理器。这款智能手机适合注重拍照质量和追求高性能体验的用户，特别是对手机摄影和流畅操作有较高要求的消费者。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleKeywordTableIndex,\n",
    "    SummaryIndex, Document,\n",
    ")\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.schema import IndexNode\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"产品A是一款智能手机，具有高分辨率摄像头和强大的处理器。\",\n",
    "        metadata={\"category\": \"产品\", \"region\": \"华南\"}\n",
    "    ),\n",
    "]\n",
    "# 1. 构建你的两个索引\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "keyword_index = SimpleKeywordTableIndex.from_documents(documents)\n",
    "\n",
    "# 2. 创建 retrievers\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=1) # 不能超过文档数量\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.storage_context.docstore,\n",
    "    similarity_top_k=1\n",
    ")\n",
    "\n",
    "# 3. 定义可组合节点\n",
    "vector_node = IndexNode(index_id=\"vector\", obj=vector_retriever, text=\"Vector Retriever\")\n",
    "bm25_node = IndexNode(index_id=\"bm25\", obj=bm25_retriever, text=\"BM25 Retriever\")\n",
    "\n",
    "# 4. 使用 SummaryIndex 聚合这两个 retriever 的结果\n",
    "summary_index = SummaryIndex(objects=[vector_node, bm25_node])\n",
    "\n",
    "# 5. 创建 query engine\n",
    "query_engine = summary_index.as_query_engine()\n",
    "\n",
    "# 6. 查询\n",
    "response = query_engine.query(\"产品A有哪些特点？适合哪些用户？\")\n",
    "\n",
    "# 7. 输出答案\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LlamaIndex在查询引擎和检索策略上的“高级”和“可组合”特性，反映了其对复杂信息需求的深刻理解。这使得LLM应用能够处理多源、多格式、多层次的数据，并进行复杂的推理，远超简单的关键词匹配。这种能力对于构建真正智能的知识助手和自主代理至关重要，因为它能够分解问题、路由查询、递归检索和融合结果，从而处理高度复杂、多方面的分析任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentic Document Workflows (ADW)与多模态应用\n",
    "\n",
    "LlamaIndex正积极推动LLM应用从简单的信息检索向更广泛、更具自主性的业务流程自动化发展，Agentic Document Workflows（ADW）和多模态能力是其在这方面的两大支柱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agentic Document Workflows (ADW)\n",
    "\n",
    "**ADW代表了一种新的架构，它超越了传统的智能文档处理（IDP）和RAG范式 。传统方法通常专注于孤立的提取步骤（IDP）或问答（RAG），而ADW则将文档视为更广泛业务流程的组成部分。一个ADW系统能够跨多个步骤维护状态、应用业务规则、协调不同组件，并根据文档内容采取行动，而不仅仅是分析它**。   \n",
    "\n",
    "ADW的核心是文档代理（document agent），它负责编排整个流程。这些代理能够：\n",
    "\n",
    "- 使用LlamaParse从输入文档中提取和结构化信息 。LlamaParse以其在处理复杂文档（如嵌套表格、复杂空间布局和图像提取）方面的卓越能力而闻名 。   \n",
    "- 维护关于文档上下文和当前流程阶段的状态 。   \n",
    "- 从知识库（如LlamaCloud）检索和分析相关的参考材料 。   \n",
    "- 根据预定义的业务规则生成可操作的建议 。\n",
    "\n",
    "通过在整个流程中维护状态，这些代理能够处理复杂的、多步骤的工作流，超越简单的信息提取或匹配，从而在处理文档时建立深层上下文，并协调不同系统组件之间的交互 。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ADW的实际用例\n",
    "LlamaIndex提供了多个生产级的ADW用例，并以详细的Jupyter Notebooks形式实现，可供用户运行和改编 。   \n",
    "\n",
    "- **合同审查：智能合规分析**： 代理能够解析复杂的合同结构，识别关键条款，并根据存储在LlamaCloud中的法规要求知识库进行交叉引用。这有助于识别潜在的合规问题，并提供结构化建议供人工审查，例如非标准条款、缺失条款或与法规冲突的条款 。   \n",
    "- **患者病例摘要：上下文理解**： 针对医疗文档量大的挑战，该代理能够从医疗记录中提取信息，并将相关病症、治疗和结果进行分组，从而辅助诊断和治疗。它能够解析复杂的医疗文档，包括实验室结果和临床笔记，同时保持患者病史的关键上下文 。   \n",
    "- **发票处理：优化业务操作**： 该工作流展示了智能代理如何为日常任务增加业务智能。代理超越了基本数据提取，能够根据供应商协议和早期付款折扣来优化付款时间。通过LlamaParse准确提取行项目和付款条款，结合LlamaCloud的检索能力，代理可以根据合同费率验证定价，并建议最佳付款策略，从而将简单的文档处理任务转化为营运资金优化的工具 。   \n",
    "- **汽车保险理赔处理：结构化分析支持**： 代理通过组织和结构化来自多个文档的信息来协助理赔处理人员：解析传入的理赔表单，匹配策略文档的相关部分，并以清晰的格式呈现关键细节。该系-统旨在增强人类专业知识，而非做出最终决策，从而在敏感领域保持适当的人工监督 。\n",
    "\n",
    "ADW的引入标志着LlamaIndex从“数据检索”向“业务流程自动化”的重大飞跃。它将LLM的能力从简单的问答扩展到能够理解、推理并执行复杂业务操作的智能代理。这表明LlamaIndex正在积极推动AI在企业核心业务流程中的深度融合，而不仅仅是提供信息查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多模态应用 (Multi-modal Applications)\n",
    "\n",
    "LlamaIndex还支持处理文本、图像、音频等多种数据类型，从而实现多模态RAG 。核心RAG概念（索引、检索、合成）可以扩展到图像设置，这意味着：   \n",
    "\n",
    "- 输入可以是文本或图像。\n",
    "- 存储的知识库可以包含文本或图像。\n",
    "- 用于生成响应的输入可以是文本或图像。\n",
    "- 最终响应可以是文本或图像 。\n",
    "\n",
    "例如，LlamaIndex支持多模态检索，如使用CLIP进行图像到图像或文本到图像的检索 。多模态能力的加入进一步拓宽了其处理现实世界复杂数据的边界，使得LLM能够从更丰富的数据形式中获取信息。   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义模块与框架扩展性\n",
    "\n",
    "LlamaIndex的强大和灵活性在于其高度可定制的架构。\n",
    "\n",
    "核心设计原则： LlamaIndex的核心设计原则是几乎所有核心模块都可以被子类化和定制 。这一设计理念使得用户能够将LlamaIndex用于任何高级LLM用例，超越其预打包模块所提供的功能 。开发者可以编写大量的自定义代码，同时仍能利用LlamaIndex的底层抽象，并将其自定义模块与其他现有组件无缝结合 。LlamaIndex还提供了便捷的指导方法来子类化其模块，让开发者无需担心定义所有样板代码（例如回调函数） 。\n",
    "\n",
    "\n",
    "**可定制模块示例**： LlamaIndex提供了广泛的可定制模块，包括但不限于：\n",
    "\n",
    "- **自定义LLM**： 允许集成或定制特定的大型语言模型 。   \n",
    "- **自定义输出解析器**： 用于处理LLM输出的格式和结构 。   \n",
    "- **自定义转换器**： 例如自定义属性图提取器，用于数据预处理和结构化 。   \n",
    "- **自定义检索器**： 实现特定的检索逻辑，例如自定义属性图检索器 。   \n",
    "- **自定义后处理器/重排器**： 用于对检索到的节点进行后处理或重新排序，以优化LLM的输入 。   \n",
    "- **自定义查询引擎**： 构建满足特定查询需求的定制化查询接口 。   \n",
    "- **自定义Agent**： 开发具有特定行为和工具集的定制化AI代理，例如自定义函数调用Agent和ReAct Agent 。   \n",
    "- **自定义查询组件**： 用于查询管道中的特定功能 。   \n",
    "\n",
    "**其他定制方式**： 某些模块可以通过参数或暴露的函数进行大量定制，而不是通过子类化。这包括定制文档、节点以及高级模块中的提示 。   \n",
    "\n",
    "LlamaIndex的高度模块化和可扩展性是其作为“框架”而非“工具”的关键体现。它认识到LLM应用的多样性和快速演进，因此提供了底层的抽象和灵活的接口，允许开发者根据特定需求进行深度定制和创新。这种开放性鼓励了社区贡献和生态系统发展，确保了框架的长期适应性和生命力。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex与LangGraph对比分析\n",
    "在LLM应用开发领域，LlamaIndex和LangGraph（作为LangChain的扩展）是两个备受关注的框架。虽然它们都旨在简化LLM应用的构建，但其核心设计理念和优势侧重点有所不同，理解这些差异对于选择合适的工具或进行协同工作至关重要。\n",
    "\n",
    "### LangChain与LangGraph：复杂工作流编排的利器\n",
    "LangChain是一个模块化框架，用于构建由LLM驱动的应用程序，以其灵活性和模块化而闻名 。它擅长处理多步骤语言处理工作流，例如内容生成和语言翻译 。LangChain提供链（chains）、代理（agents）和记忆（memory）等核心组件，并支持与外部数据源（如API、数据库和文件）的集成 。对于新项目，LangChain提供了一个更直观的起点和庞大的开发者社区，使其易于查找示例和解决方案 。   \n",
    "\n",
    "LangGraph是构建在LangChain之上的一个框架，它专注于有状态的编排，通过图结构实现多代理工作流和复杂决策 。与LangChain的线性或简单任务链不同，LangGraph的图基方法使其能够处理更复杂的对话流和数据管道，支持循环图和动态决策 。\n",
    "\n",
    "### LangGraph核心优势\n",
    "\n",
    "- **图基工作流**： LangGraph允许开发者将工作流定义为图，其中每个节点代表一个代理或特定功能，边则定义了这些节点之间的转换。这种结构支持有向无环图（DAGs）和循环图，使得管理和可视化复杂的任务依赖和交互变得更加容易 。   \n",
    "- **循环图**： 循环图是LangGraph的一个关键特性，它支持需要多次迭代或基于动态输入进行条件分支的任务。这对于需要反思、重试或持续交互的复杂流程至关重要 。   \n",
    "- **持久状态管理**： LangGraph在不同节点间维护一个持久的状态对象，这意味着应用程序可以在不丢失上下文的情况下暂停和恢复任务。这对于长时间运行的流程或需要人机交互的场景特别有用 。   \n",
    "- **多代理协作**： LangGraph支持多代理工作流，其中每个代理都可以是专门的LLM或工具。这些代理可以独立工作，也可以相互协作，共享信息以更有效地完成任务 。   \n",
    "- **可靠性与可控性**： LangGraph通过内置的持久层和对审核检查、人机循环审批的支持，增强了代理行动的可靠性和可控性。它能够为长时间运行的工作流保持上下文，确保代理按预期运行 。   \n",
    "- **低级和可扩展**： LangGraph提供了低级原语，允许开发者构建高度自定义的代理，避免了可能限制定制的僵硬抽象。这有助于设计可扩展的多代理系统，每个代理都服务于特定角色 。   \n",
    "- **一流的流式支持**： LangGraph提供token级和中间步骤的流式输出，为用户提供了代理推理和行动的实时可见性 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LlamaIndex：数据管理与知识增强的专长\n",
    "与LangGraph侧重工作流编排不同，LlamaIndex的核心专长在于高效的数据检索和管理，以及作为LLM的知识增强层。\n",
    "\n",
    "**核心关注点**： LlamaIndex主要专注于数据索引、检索和与LLM的高效交互 。它的核心价值在于将私有或领域特定数据有效地连接到LLM，从而增强其知识基础，解决通用LLM在特定领域知识上的不足 。   \n",
    "\n",
    "**优势**：\n",
    "\n",
    "- **高效的数据摄取和索引**： LlamaIndex通过LlamaHub提供了广泛的数据连接器，简化了从各种数据源（如API、PDF、SQL数据库）的数据摄取过程 。它加速了数据索引，将所有信息嵌入数值表示，实现更快的扫描和访问 。   \n",
    "- **优化的检索算法**： LlamaIndex采用先进的算法根据语义相似性对文档进行排名，从而实现高效的搜索和检索 。   \n",
    "- **上下文感知查询**： 能够运行上下文感知的查询，并将检索到的相关内容精确地注入LLM提示中，确保LLM获得最相关的上下文信息 。   \n",
    "- **与向量存储的无缝集成**： LlamaIndex提供了与40多个向量存储的无缝兼容性，以及与40多个LLM和160多个数据源的连接，确保了其在数据存储和模型选择方面的灵活性和广泛兼容性 。\n",
    "\n",
    "LlamaIndex在数据管理和知识增强方面的专业性，使其成为LLM应用中“数据层”的首选。它强调的是如何高效、准确地将“外部知识”注入LLM，这是任何高级RAG应用的基础。LlamaIndex的深度专业化在于其数据管理和检索能力，这使其成为LLM应用程序的基础数据层。这意味着对于数据质量、索引效率和精确检索至关重要的应用程序（例如企业知识库、法律文档分析），LlamaIndex提供了更优化、更简化的解决方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 协同作战：LlamaIndex与LangGraph的结合应用\n",
    "LlamaIndex和LangGraph/LangChain并非竞争关系，而是高度互补的框架。许多开发者选择协同使用这两个框架，以构建更强大、更灵活的AI解决方案 。这种混合方法能够充分利用两者的优势，实现复杂AI应用的最佳性能和功能。   \n",
    "\n",
    "**互补性**： LlamaIndex作为LLM的知识库中间层，擅长处理数据摄取、分块、索引和语义搜索，确保LLM能够高效、准确地获取和利用外部信息 。而LangChain（LangGraph是其扩展）则可以管理复杂的逻辑、工作流、代理和提示链，负责编排LLM的行为和工具的使用 。   \n",
    "\n",
    "**混合架构示例**：\n",
    "\n",
    "- **客户服务机器人**： LangChain可以用于创建和管理客户服务机器人的对话流程和多轮交互逻辑，而LlamaIndex则负责从后台知识库（如历史发票、政策文档或产品手册）中检索相关信息，并提供给LangChain进行响应生成 。   \n",
    "- **法律助手/研究机器人**： LlamaIndex可以高效地解析和索引大量的法律文件、案例法或研究论文，将其转化为可查询的知识库。LangChain/LangGraph则在此基础上构建复杂的交互逻辑，使律师或研究人员能够以自然语言提问，并获得基于专业知识库的精确答案和引用 。   \n",
    "- **招聘聊天机器人**： LlamaIndex可以用于索引大量的简历文件夹和职位描述，并进行语义搜索。LangChain/LangGraph则构建招聘聊天机器人的对话逻辑，使其能够根据用户查询（如技能、经验）从LlamaIndex检索相关简历，并进行初步筛选或回答相关问题 。\n",
    "\n",
    "LlamaIndex和LangGraph的协同作战模式代表了构建复杂、生产级LLM应用的最佳实践。通过将LlamaIndex作为强大的“数据层”和“知识源”，将LangGraph作为灵活的“编排层”和“决策引擎”，开发者可以构建出既能处理海量复杂数据，又能执行多步骤智能任务的AI系统。这种分层架构提供了最佳的模块化、可扩展性和性能，是构建强大、健壮AI解决方案的关键。\n",
    "\n",
    "### LlamaIndex与LangGraph功能对比\n",
    "\n",
    "下表详细对比了LlamaIndex和LangGraph（基于LangChain）在关键功能和设计侧重点上的差异，帮助读者更好地理解两者的定位和适用场景。\n",
    "\n",
    "| 特性/方面 (Feature/Aspect) | LlamaIndex                                                   | LangGraph (基于LangChain)                                    |\n",
    "| -------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| **主要焦点**               | 数据摄取、索引、检索、为LLM提供知识增强                      | 有状态、多代理工作流编排、复杂决策制定                       |\n",
    "| **核心抽象**               | 文档、节点、索引、查询引擎、RAG                              | StateGraph、节点、边、循环图                                 |\n",
    "| **数据处理**               | 专注于高效数据检索和管理；通过LlamaHub提供160+数据连接器；处理结构化/非结构化数据，支持多模态 | 通过工具与外部数据源/API集成                                 |\n",
    "| **工作流复杂度**           | 支持代理工作流和多步骤查询，但主要优势在于RAG的数据层        | 擅长复杂对话流、动态决策、条件逻辑和多代理协调               |\n",
    "| **状态管理**               | 集成记忆管理以增强上下文保留，尤其在聊天引擎中。ADW在文档处理步骤中维护状态 | 具有跨节点持久状态的高级状态管理，支持暂停、恢复、人机循环   |\n",
    "| **可扩展性**               | 高度可定制；几乎所有核心模块都可以子类化（LLM、解析器、检索器、代理） | 提供用于自定义代理的低级原语；支持具有特定角色的多代理系统   |\n",
    "| **典型用例**               | 文档问答、数据增强聊天机器人、知识代理、企业搜索、结构化数据提取、多模态RAG | 内容生成、客户支持聊天机器人（线性）、语言翻译（LangChain）；社交网络分析、欺诈检测、多代理协调、复杂对话代理（LangGraph） |\n",
    "| **学习曲线**               | 上手时间短，设计直观，5行代码快速启动                        | 对于复杂工作流可能更具挑战性，但提供结构化教程               |\n",
    "| **集成能力**               | 与40+向量存储、40+LLM无缝集成                                | 与LangChain生态系统和LangSmith集成以进行监控                 |\n",
    "| **协同效应**               | 通常与LangChain/LangGraph结合用于复杂应用，LlamaIndex提供数据层 | 可与LlamaIndex结合使用，在复杂代理工作流中增强数据检索       |\n",
    "\n",
    "  \n",
    "\n",
    "这张表格对于希望在LlamaIndex和LangGraph之间做出选择或考虑结合使用的开发者而言具有重要价值。它清晰地勾勒出两个框架的核心能力、设计侧重点和适用场景，帮助开发者快速识别哪个工具最符合其项目的主要需求（例如，如果数据复杂度高，则LlamaIndex更优；如果工作流复杂度高，则LangGraph更优）。此外，表格还强调了它们之间的互补性，为构建更强大、分层且专业的AI解决方案提供了架构指导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaIndex生产级应用与最佳实践\n",
    "\n",
    "将LLM应用从原型阶段推向生产环境，需要考虑性能、可靠性、安全性和可维护性等多个方面。LlamaIndex不仅提供了构建工具，还强调了一系列最佳实践，以确保其应用能够满足企业级需求。\n",
    "\n",
    "### 企业级用例与真实世界案例研究\n",
    "\n",
    "LlamaIndex在金融、制造、IT和专业服务等多个行业中，为构建高级定制知识助手提供了强大的支持 。这些真实世界的案例验证了LlamaIndex在解决复杂企业数据挑战和实现商业价值方面的能力。  \n",
    "\n",
    "\n",
    "\n",
    "**具体案例：**\n",
    "\n",
    "- KPMG (毕马威)：\n",
    "   KPMG将LlamaIndex作为其内部交付AI代理的基础层。LlamaCloud的功能在标准化企业知识助手的开发方面发挥了重要作用，它能够处理多模态数据（文本、表格、图表），并加速AI在企业内部的系统化采用 。这表明LlamaIndex能够帮助大型咨询公司在严格的合规和数据环境中部署AI解决方案。\n",
    "\n",
    "- 私募股权基金：\n",
    "   一家大型私募股权基金利用LlamaParse（LlamaIndex的核心文档解析工具）来解析复杂的非结构化文档，如10-Ks报告和收益报告。LlamaParse在处理嵌套表格、复杂空间布局和图像提取方面的卓越能力，对于维护高级RAG和代理模型开发中的数据完整性至关重要，并实现了结构化值的自动填充，用于杠杆收购代理 。\n",
    "\n",
    "- Rakuten Group (乐天集团)：\n",
    "   乐天集团利用LlamaCloud高效解析和索引其复杂的企业数据，显著提升了RAG性能。这使得乐天的工程师能够将更多精力集中在LLM应用的开发和推广上，而不是耗费大量时间在数据管道的维护上，从而加速了LLM应用的构建和采用 。\n",
    "\n",
    "**通用企业搜索用例：**\n",
    "\n",
    "- 统一分散数据源： 企业数据通常分散在多个系统中（如数据库、云存储、协作工具和内部API）。LlamaIndex能够将这些碎片化的数据源索引成统一的格式，从而实现跨平台搜索，无需为每个系统编写自定义连接器  。\n",
    "\n",
    "- 增强语义搜索：传统的关键词搜索难以处理歧义或上下文相关的查询。LlamaIndex通过LLM生成向量嵌入来捕捉文本含义，从而实现语义搜索，使搜索结果能够匹配查询意图，而不仅仅是关键词 。\n",
    "\n",
    "- 受控访问与实时更新：企业数据往往需要严格的访问控制。LlamaIndex可以与权限系统（如Active Directory）集成，索引用户角色或文档权限等元数据，确保搜索结果符合策略。此外，它支持实时索引更新，这对于客户支持票据或库存数据库等动态数据至关重要，避免了提供过时信息。\n",
    "\n",
    "LlamaIndex的实际企业案例，特别是KPMG、私募股权和Rakuten的成功故事，验证了其在解决复杂企业数据挑战和实现商业价值方面的能力。这些案例不仅展示了技术可行性，更强调了LlamaIndex如何通过提高生产力、降低运营成本和改善客户互动来提供实际的投资回报率 。这对于企业决策者和寻求生产级解决方案的开发者来说是强有力的证明。  \n",
    "\n",
    "### 生产部署、监控与调试\n",
    "\n",
    "将LlamaIndex应用部署到生产环境需要遵循一套全面的最佳实践，以确保其性能、可靠性、可维护性和安全性。\n",
    "\n",
    "**最佳实践：**\n",
    "\n",
    "- 高效数据索引： 生产级应用需要结构化的数据摄取管道，以处理多样化的数据格式并随数据集增长而扩展。利用LlamaIndex内置的数据连接器简化数据导入。对文本进行预处理，将其分割成适当大小的块（节点），以平衡检索准确性和计算成本，并应在开发过程中测试不同的分块策略以找到最佳平衡。\n",
    "\n",
    "- 查询优化：选择合适的索引策略（例如，向量、关键词或混合索引）来优化查询性能。对于语义搜索，向量索引效果良好，但如果数据包含结构化元数据，可以结合向量索引和SQL索引以高效过滤结果。此外，应缓存频繁访问的索引以减少延迟，避免重新计算嵌入 。 \n",
    "\n",
    "- 系统可靠性： 持续监控和更新索引以整合新数据，例如为新闻聚合器应用安排夜间重建。实施日志记录以跟踪查询性能指标，如延迟、错误率和缓存命中。使用LlamaIndex的异步API来处理高并发请求，避免阻塞用户。设计故障恢复机制，例如冗余索引或默认响应，以应对部分索引损坏等故障情况。\n",
    "\n",
    "- 数据安全与合规性： 在数据摄取过程中掩盖个人可识别信息（PII），并执行基于角色的访问控制，确保搜索结果符合数据隐私政策。定期审计数据管道，以符合GDPR等数据隐私法规。    \n",
    "\n",
    "**监控与调试：** LlamaIndex支持与可观测性/评估工具的集成，这对于LLM应用的严格实验、评估和监控至关重要 。例如，LangSmith可以用于检查、监控和评估LLM链的性能和行为 。通过实施适当的追踪，开发者可以深入了解其LLM代理的行为，优化其性能，并确保其在开发和生产环境中可靠、安全地运行 。  \n",
    "\n",
    "LlamaIndex不仅提供了构建RAG和Agent的工具，还提供了生产部署和运维的“最佳实践”。这包括对数据管理、性能优化、可靠性和安全性的深入考虑，表明其在设计时就考虑到了企业级应用的严格要求。这对于开发者来说，意味着可以构建出不仅仅是功能可用，而且是稳定、高效、安全的AI系统。\n",
    "\n",
    "以下是LlamaIndex生产部署最佳实践清单，为开发者提供了可操作的建议：\n",
    "\n",
    "| 类别 (Category)  | 最佳实践 (Best Practice)                                  | 理由 / 益处 (Rationale / Benefit)                        | 相关资料 (Relevant Snippets) |\n",
    "| ---------------- | --------------------------------------------------------- | -------------------------------------------------------- | ---------------------------- |\n",
    "| **数据摄取**     | 结构化数据摄取管道以支持多样格式和可伸缩性。              | 确保从各种来源进行强大的数据处理，并随数据量增长而扩展。 |                              |\n",
    "|                  | 通过将文档分割为最佳块（节点）来预处理文本。              | 平衡检索准确性和计算成本；处理LLM的token限制。           |                              |\n",
    "|                  | 根据上下文需求配置节点大小和重叠。                        | 精细调整嵌入以实现精度或通用性；影响检索质量。           |                              |\n",
    "| **索引策略**     | 选择适当的索引策略（向量、关键词、混合）以优化查询性能。  | 根据数据和查询类型优化语义搜索、关键词搜索或组合搜索。   |                              |\n",
    "|                  | 结合向量和SQL索引以进行结构化元数据过滤。                 | 在语义排名之前高效地按类别/日期过滤结果。                |                              |\n",
    "|                  | 将频繁访问的索引缓存到内存或分布式存储中。                | 减少延迟并避免为常见查询重新计算嵌入。                   |                              |\n",
    "| **可靠性与维护** | 监控并定期更新索引以纳入新数据。                          | 确保信息的时效性；防止过时响应。                         |                              |\n",
    "|                  | 实施日志记录以跟踪查询性能（延迟、错误率、缓存命中）。    | 为实时性能监控和优化提供可操作的洞察。                   |                              |\n",
    "|                  | 使用LlamaIndex的异步API处理高并发。                       | 处理多个用户请求而不阻塞，提高响应速度。                 |                              |\n",
    "|                  | 测试故障场景并设计回退机制（例如，冗余索引）。            | 确保应用程序的弹性，并在出现问题时优雅降级。             |                              |\n",
    "| **安全与合规**   | 限制对敏感数据的访问（掩盖PII，强制基于角色的访问控制）。 | 保护数据隐私并确保符合GDPR等法规。                       |                              |\n",
    "|                  | 定期审计数据管道以符合数据隐私要求。                      | 保持持续的合规性和安全态势。                             |                              |\n",
    "| **开发与测试**   | 为索引定义清晰的用例，避免不必要的复杂性。                | 确保专注的开发和高效的资源分配。                         |                              |\n",
    "|                  | 创建专门的测试环境以进行配置优化。                        | 允许在生产部署前进行迭代测试和验证。                     |                              |\n",
    "| **优化**         | 优化提示以减少幻觉并改进输出格式。                        | 直接影响LLM响应的质量和相关性。                          |                              |\n",
    "|                  | 选择正确的嵌入模型，并在更改时重新索引。                  | 对整体检索性能和准确性至关重要。                         |                              |\n",
    "|                  | 在查询期间采用元数据过滤。                                | 缩小结果范围，提高速度和相关性。                         |                              |\n",
    "|                  | 调整 `similarity_top_k` 参数。                            | 控制为上下文检索到的顶部相关块的数量。                   |                              |\n",
    "\n",
    "  \n",
    "\n",
    "这份清单对于部署LlamaIndex到生产环境的开发者和运维团队而言具有极高价值。它将抽象的“最佳实践”转化为具体可行的行动项，涵盖了数据管理、索引、可靠性、安全性和优化等关键维度。清单中的每一项都附有清晰的理由，解释了其重要性，从而帮助确保构建的LLM应用不仅功能可用，而且稳定、高效、安全。\n",
    "\n",
    "### 持续评估与优化\n",
    "\n",
    "LLM应用开发是一个持续迭代和优化的过程。LlamaIndex框架强调评估的重要性，并提供了相应的工具和策略来支持这一过程。\n",
    "\n",
    "- 评估的重要性： 评估是任何LLM应用生命周期中的关键环节，用于客观衡量其准确性、性能、清晰度和成本。由于不同的策略各有优缺点，持续评估对于确保RAG系统在不断变化的数据和用户需求下保持高性能至关重要。\n",
    "\n",
    "- 评估方法： LlamaIndex提供了评估框架，例如 ```BatchEvalRunner```，以及针对RAG的特定评估指标，如答案相关性（Answer Relevance）、上下文相关性（Context Relevance）和忠实度（Groundedness）。这些指标帮助开发者量化RAG性能，并通过交互式仪表板可视化指标，从而并排比较不同方法。\n",
    "\n",
    "- 优化技术： 除了前述的Prompt Engineering、Embeddings选择、Chunk Sizes调整、混合搜索和元数据过滤等策略外，LlamaIndex还支持多租户RAG的实现，以确保数据安全，防止未经授权的数据共享和保护数据隐私。\n",
    "\n",
    "LlamaIndex对“评估”的重视，并提供具体的评估框架和指标，强调了LLM应用开发是一个迭代和优化的过程。这对于确保RAG系统在不断变化的数据和用户需求下保持高性能至关重要。它鼓励开发者采用数据驱动的方法来改进其AI应用，而不仅仅是“部署即完成”。\n",
    "\n",
    "以下是LlamaIndex高级RAG技术概览，为读者提供了更深入的优化方向：\n",
    "\n",
    "| 高级RAG技术 (Advanced RAG Technique)            | 描述 (Description)                                           | 益处 (Benefits)                                              | 相关资料 (Relevant Snippets) |\n",
    "| ----------------------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ---------------------------- |\n",
    "| **混合搜索 (Hybrid Search)**                    | 结合语义搜索（嵌入相似性）和关键词搜索（如BM25），以提高检索召回率和精确度，尤其适用于特定术语。 | 克服纯语义搜索的局限性；确保不会遗漏相关的关键词。           |                              |\n",
    "| **元数据过滤 (Metadata Filtering)**             | 将结构化元数据附加到文档/节点，并在查询时用于过滤检索结果。  | 通过缩小搜索空间提高精度；实现受控访问和策略遵守。           |                              |\n",
    "| **递归检索 (Recursive Retrieval)**              | 通过递归遍历链接的节点或文档来检索信息，适用于层次结构或相互关联的数据。 | 处理复杂的数据关系（例如，嵌套表格、知识图谱）；提供更深层次的上下文理解。 |                              |\n",
    "| **查询融合 / 重排 (Query Fusion / Re-ranking)** | 从原始查询生成多个子查询，并融合/重新排序结果以提高整体相关性。 | 提高对模糊查询的鲁棒性；利用多样化的检索信号获得更好的结果。 |                              |\n",
    "| **自动检索 (Auto-Retrieval)**                   | 结合语义搜索和结构化过滤，通常通过LLM根据查询自动生成过滤器。 | 实现半结构化查询；简化开发人员的复杂过滤逻辑。               |                              |\n",
    "| **句子窗口检索 (Sentence Window Retrieval)**    | 检索最相关块周围的小“句子窗口”，为LLM提供更多上下文，同时不超过token限制。 | 通过提供更完整的上下文提高答案质量，同时保持效率。           |                              |\n",
    "| **层次分块 (Hierarchical Chunking)**            | 以多粒度级别（例如，段落、章节、完整文档）组织文档块。       | 根据查询复杂性实现最佳粒度检索；支持多层次理解。             |                              |\n",
    "| **自动合并检索 (Auto-Merging Retrieval)**       | 如果较小的检索块在上下文上相关，则动态地将其合并为较大的块，为LLM提供更丰富的上下文。 | 减少上下文碎片化；提高检索信息的连贯性和完整性。             |                              |\n",
    "\n",
    "  \n",
    "\n",
    "这份表格对于希望超越基本RAG的开发者而言具有宝贵价值。它系统地列出了LlamaIndex支持的先进技术，并简要描述了每种技术及其带来的益处。通过突出每种技术的价值，并指向相关的资料，该表格充当了RAG优化的速查表，指导用户如何解决实际场景中常见的性能和准确性挑战。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 LlamaIndex实战宝典"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前言：心法篇 - RAG 的道与术 (The Tao of RAG)\n",
    "\n",
    "欢迎来到 LlamaIndex 的专家级世界。在深入探讨海量的代码、复杂的架构和精妙的算法之前，我们必须首先统一思想，建立正确的“心法”。高级的 RAG 系统开发，本质上是一场思维模型的博弈。错误的认知起点，将导致我们在优化的道路上南辕北辙。\n",
    "\n",
    "本章将为您揭示 RAG 的本质，剖析 LlamaIndex 的设计哲学，并确立一套科学的开发方法论。掌握了这些“道”与“术”，您将能以更宏观、更深刻的视角驾驭后续的所有“器”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重新定义 RAG：从信息检索到知识工程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG 的第一性原理：不是“搜索+生成”，而是“上下文引导的推理”\n",
    "初学者常常将 RAG（Retrieval-Augmented Generation，检索增强生成）简单地理解为一个两步过程：“第一步，从数据库里搜索（Retrieve）相关文档；第二步，把文档和问题一起扔给大语言模型（LLM）去总结生成（Generate）答案”。\n",
    "\n",
    "这个描述不能算错，但它极其肤浅，忽略了 RAG 的真正威力与核心挑战。如果我们从第一性原理出发，RAG 的本质应该是：\n",
    "\n",
    "> **在推理（Inference）那一刻，为大语言模型（LLM）提供最高质量、最精确、最相关的上下文（Context），以引导其完成特定任务。**\n",
    "\n",
    "让我们拆解这个定义：\n",
    "\n",
    "- **核心动作是“引导推理”**：LLM 本身就是一个强大的通用推理引擎。我们使用 RAG，**并非让它学习新知识（那是训练/微调的范畴）**，而是引导它在“开卷考试”中，利用我们给它的“参考资料”（即上下文）进行高质量的推理、分析、归纳或创作。\n",
    "- **成功的关键是“上下文质量”**：最终生成结果的质量上限，几乎完全由你提供的上下文质量所决定。给它垃圾，它只能输出“精致的垃圾”。给它精准、全面、无噪声的上下文，它才能展现出惊人的“专家级”能力。\n",
    "- **“搜索”只是手段之一**：检索（Retrieval）只是获取上下文的手段，而且远非唯一手段。混合检索、图查询、查询重构、元数据过滤……所有这些高级技术，其最终目的都是为了同一个目标服务——**构造出完美的上下文**。\n",
    "\n",
    "**一个恰当的比喻**：想象一下，LLM 是一位才华横溢、博古通今的专家，但被关在一个没有窗户的房间里。你的任务是回答一个极其刁钻的专业问题。你可以把整个图书馆（你的知识库）的所有书都扔给他，让他自己大海捞针，结果可想而知。而 RAG 高手的做法是，精准地找出最关键的三五本书、甚至某几页纸，清晰地标注好重点，递给这位专家。专家只需片刻，就能基于这些精准的材料，给出一份完美的答案。\n",
    "\n",
    "**从“搜索+生成”到“上下文引导的推理”的思维转变，是普通玩家与高级玩家的根本分水岭。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LlamaIndex 的设计哲学：数据为中心的抽象与可组合的未来\n",
    "理解了 RAG 的第一性原理后，LlamaIndex 的设计哲学便豁然开朗。为什么 LlamaIndex 自称为“LLM 应用的数据框架（Data Framework）”？因为它的一切设计，都服务于“上下文构造”这一核心目标。\n",
    "\n",
    "LlamaIndex 的设计哲学可以归结为两点：\n",
    "\n",
    "1. **数据为中心的抽象（Data-centric Abstraction）** LlamaIndex 认为，LLM 应用的根基是数据。因此，它提供了一套以数据为中心的、高度抽象的组件，系统化地解决了“如何将千差万别的数据源，高效、可靠地转化为 LLM 需要的高质量上下文”这一核心问题。这套抽象涵盖了从数据加载（Loading）、转换（Transformation）、索引（Indexing），到最终查询（Querying）的全链路。\n",
    "\n",
    "2. **可组合的未来（Composable Future）** RAG 系统不存在“银弹”或一劳永逸的方案。针对不同的数据、不同的业务场景，你需要像乐高大师一样，自由组合各种组件来搭建最适合的解决方案。LlamaIndex 的所有核心模块——`Reader`、`NodeParser`、`EmbeddingModel`、`VectorStore`、`Retriever`、`ReRanker`、`QueryEngine`——都被设计成可独立、可替换、可组合的“积木”。\n",
    "\n",
    "   - 想换个嵌入模型？拔下 `OpenAIEmbedding`，插上 `BGE`。\n",
    "   - 觉得向量检索不够准？在后面串联一个 `CohereRerank` 重排器。\n",
    "   - 需要同时查询文本和图数据库？用 `RouterQueryEngine` 把两个独立的查询引擎组合起来。\n",
    "\n",
    "   这种高度模块化和可组合性，赋予了开发者极大的灵活性，去应对未来层出不穷的新模型、新算法和新需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### “万物皆可为节点”：Node 作为信息原子的核心思想\n",
    "LlamaIndex 最具威力的核心抽象是什么？答案是 `Node`。\n",
    "\n",
    "如果说 LlamaIndex 是一个操作系统，那么 `Node` 就是这个系统中的“文件”——它是信息存储和流转的最基本、最核心的原子单位。\n",
    "\n",
    "一个 `Node` 对象，远不只是一个“文本块（Chunk）”。它是一个富含信息的结构体，通常包含：\n",
    "\n",
    "- `text`: 节点的文本内容，这是基础。\n",
    "- `id_`: 全局唯一的节点 ID，用于精确追踪和引用。\n",
    "- `metadata`: 一个极其重要的字典，可以存放关于这个节点的任意结构化信息。例如：源文件名、章节标题、作者、创建日期、URL、数据类别等等。\n",
    "- `relationships`: 定义了节点之间的关系。例如，`NEXT` 和 `PREVIOUS` 关系可以将节点串联成原始的文档顺序，`PARENT` 和 `CHILD` 关系可以构建层次化结构。\n",
    "\n",
    "**“万物皆可为节点”** 的思想，意味着你可以将任何来源的信息，都统一封装成 `Node` 这一标准格式。\n",
    "\n",
    "- PDF 的一个段落是一个 `Node`。\n",
    "- 数据库中的一行数据可以是一个 `Node`，它的 `text` 是该行数据的文本化描述，`metadata` 包含了各个列的原始值。\n",
    "- 一张图片也可以是一个 `Node`，它的 `text` 是对图片的描述，`embedding` 则是图片的向量表示。\n",
    "- 一个知识图谱中的实体同样可以是一个 `Node`。\n",
    "\n",
    "正是因为有了 `Node` 这个统一的信息原子，LlamaIndex 才能：\n",
    "\n",
    "1. **处理异构数据**：无论源数据是什么格式，最终都统一为 `Node` 的集合进行处理。\n",
    "2. **实现高级检索**：`metadata` 是实现“元数据过滤”的基础，`relationships` 则是实现“层次化检索”、“图检索”等高级策略的关键。它将简单的文本块，提升到了“知识图谱”的维度。\n",
    "3. **保证答案的可追溯性**：因为每个 `Node` 都有唯一的 `id_` 和来源元数据，所以当 LLM 基于某些 `Node` 生成答案后，我们可以精确地告诉用户，答案来源于哪些文档的哪些部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心方法论：评估驱动开发（EDD）—— 没有评估，就没有优化\n",
    "RAG 系统充满了需要调整的“旋钮”：分块大小、重叠长度、嵌入模型、检索 top_k 值、重排器模型……面对如此多的变量，我们如何知道自己的修改是“优化”还是“劣化”？\n",
    "\n",
    "答案是：**评估驱动开发（Evaluation-Driven Development, EDD）。**\n",
    "\n",
    "在没有建立一套客观、可量化的评估体系之前，任何所谓的“调优”都只是凭感觉的“玄学”。EDD 是将 RAG 开发从“手工作坊”带向“现代工程”的核心方法论。\n",
    "\n",
    "在 RAG 领域，我们通常关注以下几个核心评估维度：\n",
    "\n",
    "1. **检索质量评估（Retrieval Quality）**\n",
    "   - **上下文相关性（Context Relevance）**: 检索出的上下文，与用户的原始问题是否相关？这是最基础的评估，如果检索出的内容风马牛不相及，后续一切都无从谈起。\n",
    "   - **命中率（Hit Rate）**: 预先标注的“黄金标准答案”是否被成功检索到了？\n",
    "2. **生成质量评估（Generation Quality）**\n",
    "   - **答案忠实度/根据性（Groundedness / Faithfulness）**: LLM 生成的答案，是否完全基于我们提供的上下文？有没有“自由发挥”或“凭空捏造”（即幻觉）？\n",
    "   - **答案相关性（Answer Relevance）**: 最终生成的答案，是否直接、清晰地回答了用户的原始问题？\n",
    "\n",
    "LlamaIndex 内置了一套强大的评估工具（如 `ResponseEvaluator`, `RelevancyEvaluator` 等），可以帮助我们自动化地计算这些指标。\n",
    "\n",
    "**一个标准的 EDD 工作流应该是这样的：**\n",
    "\n",
    "1. 建立一个包含代表性问题的评估数据集。\n",
    "2. 针对基线版本的 RAG 系统（Baseline），运行评估集并记录所有核心指标。\n",
    "3. **只修改一个变量**（例如，更换嵌入模型）。\n",
    "4. 重新运行评估集，记录新版本的核心指标。\n",
    "5. 对比新旧版本的指标，用数据判断本次修改是成功还是失败。\n",
    "6. 重复 3-5 步，持续迭代。\n",
    "\n",
    "记住，**没有评估的优化是盲目的，没有数据的论证是无力的。** 在整个教程的学习过程中，请始终将 EDD 的思想贯穿始终。\n",
    "\n",
    "**本章小结**\n",
    "\n",
    "我们重新定义了 RAG，明确了 LlamaIndex 的设计思想，理解了 `Node` 的核心地位，并确立了评估驱动的科学开发方法。带着这些“心法”，您已经为接下来的硬核技术学习做好了最充分的准备。下一章，我们将深入探索“引擎核心篇”，从数据注入与索引构建开始，正式踏上 LlamaIndex 的专家之路。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一部分：引擎核心篇 - 数据注入与索引构建 (The Ingestion & Indexing Engine)\n",
    "\n",
    "在前言中，我们确立了 RAG 的核心目标是**构造最高质量的上下文**。现在，我们进入实战，深入 LlamaIndex 的“引擎室”，亲手打造这一过程。数据注入与索引构建（Ingestion and Indexing）是决定 RAG 系统性能上限的第一个，也是最重要的环节。一个糟糕的注入流程，无论后续的检索和生成环节多么先进，都无法弥补其“先天不足”。\n",
    "\n",
    "本章将带您精通三大核心技术：\n",
    "\n",
    "1. **数据解析（Parsing）**：如何将原始、混乱的文档，转化为干净、结构化的信息。\n",
    "2. **分块（Chunking）**：如何将长信息流，切分为大小适中、上下文完整的“知识片段”。\n",
    "3. **嵌入（Embedding）**：如何将文本化的知识片段，映射为机器能够理解和计算的语义向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据解析：从文档到可计算节点的转换艺术\n",
    "\n",
    "数据解析是构建高质量 `Node` 的起点。它的目标远不止于提取纯文本，而是要尽可能地理解和保留原始文档的**版面布局（Layout）**和**结构信息（Structure）**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 智能解析层：LlamaParse vs. Unstructured.io 在复杂 PDF/PPT/Markdown/HTML 上的对决\n",
    "\n",
    "面对复杂的文档，特别是 PDF，简单的文本提取会丢失大量信息，比如表格、列表、标题层级等。为此，我们需要更智能的解析工具。\n",
    "\n",
    "- **`Unstructured.io`**：一个强大的开源文档解析库，也是 LlamaIndex 许多内置解析器的底层依赖。它擅长处理多种格式（PDF, HTML, DOCX 等），并能识别出标题、段落、列表等基本元素。对于大多数本地部署和常规场景，它是一个优秀且灵活的选择。\n",
    "- **`LlamaParse`**：这是 LlamaIndex 官方推出的云端商业化解析服务。它专门针对极其复杂的 PDF 进行了优化，在以下方面表现卓越：\n",
    "  - **复杂表格解析**：能准确地将 PDF 中的表格提取为 Markdown 或其他结构化格式，而非杂乱的文本。\n",
    "  - **嵌套结构理解**：能更好地处理包含图、文、表混合排版的复杂页面。\n",
    "  - **图像识别**：能够识别文档中的图像。\n",
    "\n",
    "**对决总结：**\n",
    "\n",
    "| 特性         | `Unstructured.io`            | `LlamaParse`                                 |\n",
    "| ------------ | ---------------------------- | -------------------------------------------- |\n",
    "| **部署**     | 本地，开源，灵活             | 云端 API，需 API Key                         |\n",
    "| **核心优势** | 通用性强，支持格式多，可离线 | 极致的复杂 PDF 解析，特别是表格              |\n",
    "| **成本**     | 开源免费                     | 按使用量付费                                 |\n",
    "| **适用场景** | 大多数标准文档，需要本地部署 | 包含大量复杂表格、图表的财报、论文、技术手册 |\n",
    "\n",
    "**代码示例：使用 `LlamaParse`**\n",
    "\n",
    "要使用 `LlamaParse`，你需要先获取一个 [API Key](https://docs.llamaindex.ai/en/stable/module_guides/indexing/llama_cloud_index/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-cloud-services\n",
    "# %pip install llama-parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 98553804-acf6-46fc-9a86-b7ce934efce5\n",
      ".# 一、 项目概述与目标\n",
      "\n",
      "# 1. 项目背景\n",
      "\n",
      "在快节奏的日常沟通中，大量的关键信息——包括待办事项、潜在风险、重要决策——沉淀在企业微信的群聊和私聊中。目前，这些信息的提取、汇总和追踪主要依赖人工，不仅耗时耗力，且容易出现遗漏，导致信息断层和项目延误。\n",
      "\n",
      "# 2. 项目目标\n",
      "\n",
      "本项目旨在构建一个自动化的会话分析与报告系统。其核心目标是：\n",
      "\n",
      "- 自动化信息提炼：每日自动从海量会话内容中，精准识别出所有关键工作事项。\n",
      "- 智能化状态追踪：通过关联历史数据，智能判断并更新每个事项的当前状态（如：新增、进行中、已完成）。\n",
      "- 标准化报告输出：每日生成一份结构清晰、重点突出的工作报告，为团队管理和决策提供及时、准确的数据支持。\n",
      "\n",
      "# 二、 整体解决方案\n",
      "\n",
      "为实现上述目标，我们设计的解决方案包含两个核心服务和一个数据交换中心，确保系统的高内聚、低耦合。\n",
      "\n",
      "# 1. 数据服务 (Java)\n",
      "\n",
      "职责定位：作为整个系统稳定可靠的“数据源”。它专门负责与企业微信API对接，处理所有与数据拉取、解密、清洗和准备相关的工作。\n",
      "\n",
      "核心产出：每日一份标准化的、纯文本格式的聊天记录文件。\n",
      "\n",
      "# 2. AI分析服务 (Python)\n",
      "\n",
      "职责定位：作为系统的“智能大脑”。它负责执行所有复杂的分析任务，消费由数据服务提供的标准化数据，并产出最终的分析报告。\n",
      "\n",
      "核心产出：每日一份Markdown格式的、人类易读的智能分析报告。\n",
      "\n",
      "# 3. 数据交换区 (本地文件系统)\n",
      "\n",
      "职责定位：作为两个服务之间的“数据缓冲与解耦层”。这种设计允许任一服务独立开发、部署和维护，而不影响另一方，大大提高了系统的健壮性和灵活性。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import llama_parse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# 设置你的 LlamaParse API Key\n",
    "# 强烈建议使用环境变量来管理 API Key\n",
    "# os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"your_llama_cloud_api_key\"\n",
    "\n",
    "# 定义解析器\n",
    "# result_type 可以是 \"text\" 或 \"markdown\"\n",
    "# 对于需要保留表格结构的场景，强烈推荐 \"markdown\"\n",
    "parser = llama_parse.LlamaParse(result_type=\"markdown\", verbose=True)\n",
    "\n",
    "# 使用 SimpleDirectoryReader 并指定文件提取器\n",
    "# 这样，阅读器在读取 PDF 文件时，会自动调用 LlamaParse\n",
    "file_extractor = {\".pdf\": parser}\n",
    "documents = SimpleDirectoryReader(\"./data\", file_extractor=file_extractor).load_data()\n",
    "\n",
    "# 查看解析后的第一个文档内容\n",
    "# 你会发现表格被完美地转换成了 Markdown 格式\n",
    "print(documents[0].get_content())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 版面感知分块：保留表格、标题和列表结构的先进策略\n",
    "\n",
    "版面感知分块意味着分块器（NodeParser）不仅仅是基于字符数或分隔符来切分文本，它还会利用从智能解析层（如 LlamaParse）获得的结构化信息。\n",
    "\n",
    "LlamaParse 在输出 Markdown 时，会用特定的语法（如 | 用于表格，# 用于标题）来标记文档结构。LlamaIndex 的 MarkdownNodeParser 就可以理解这些标记。\n",
    "\n",
    "当 MarkdownNodeParser 遇到一个表格的 Markdown 文本时，它会倾向于将整个表格作为一个完整的 Node，而不是在表格中间粗暴地将其切开。这极大地保留了信息的完整性，对于需要对表格内容进行问答的场景至关重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Smith attended Stanford University from 2010 to 2014, where he majored in Computer Science. During college, he participated in various hackathons and interned at Google. He also published two research papers on machine learning and co-founded a student AI club.\n",
      "\n",
      "In college, Sarah Lee studied literature at Harvard. She was active in the university newspaper, serving as editor-in-chief in her final year. Her thesis focused on postmodern American fiction.\n",
      "\n",
      "Mike Chen earned his degree in mechanical engineering from MIT. As a student, he built several robotic prototypes and was a finalist in the National Robotics Competition. He also worked part-time at a 3D printing lab on campus.\n",
      "\n",
      "Emily Davis studied economics at the University of Chicago. She spent a semester abroad in Germany and completed a research internship with a local economic think tank. Her senior project analyzed the impact of urbanization on small businesses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parser = MarkdownNodeParser(result_type=\"markdown\", verbose=True)\n",
    "\n",
    "documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "\n",
    "print(documents[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自定义解析器：攻克专有格式（如财报、法律文书）的最后一公里\n",
    "当遇到 LlamaHub 中没有的、或者极其特殊的专有数据格式时，你可以构建自己的解析器。其核心是识别并保留文档的内在逻辑结构，将结构信息注入到 Node 的 metadata 中。\n",
    "\n",
    "**实战案例：构建 FinancialReportReader**\n",
    "下面是一个完整的、可直接运行的代码，演示如何解析一份包含多个章节的简化财报，并将每个章节解析为带有丰富元数据的独立 Document 节点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建了示例财报文件: temp_data\\quarterly_report_q1_2025.txt\n",
      "\n",
      "成功将一份财报解析为 3 个独立的 Document 节点。\n",
      "\n",
      "--- Document 1 ---\n",
      "内容 (Text):\n",
      "Our performance in the first quarter of 2025 has been outstanding. We launched the new \"QuantumLeap\" processor, which has seen strong market adoption. Revenue grew by 20% year-over-year, driven by our AI division.\n",
      "\n",
      "元数据 (Metadata):\n",
      "{'source_file': 'quarterly_report_q1_2025.txt', 'company_name': 'FutureTech Inc.', 'report_quarter': 'Q1 2025', 'report_date': '2025-04-28', 'section_title': 'Management Discussion and Analysis'}\n",
      "\n",
      "---------------------\n",
      "\n",
      "--- Document 2 ---\n",
      "内容 (Text):\n",
      "- Revenue: $500 Million\n",
      "- Net Profit: $80 Million\n",
      "- Earnings Per Share (EPS): $1.25\n",
      "- Cash Flow from Operations: $120 Million\n",
      "\n",
      "元数据 (Metadata):\n",
      "{'source_file': 'quarterly_report_q1_2025.txt', 'company_name': 'FutureTech Inc.', 'report_quarter': 'Q1 2025', 'report_date': '2025-04-28', 'section_title': 'Financial Highlights'}\n",
      "\n",
      "---------------------\n",
      "\n",
      "--- Document 3 ---\n",
      "内容 (Text):\n",
      "The ongoing global chip shortage continues to be a primary risk. Furthermore, increasing competition in the AI space requires us to innovate continuously. Regulatory changes in data privacy could also impact our cloud services business.\n",
      "\n",
      "元数据 (Metadata):\n",
      "{'source_file': 'quarterly_report_q1_2025.txt', 'company_name': 'FutureTech Inc.', 'report_quarter': 'Q1 2025', 'report_date': '2025-04-28', 'section_title': 'Risk Factors'}\n",
      "\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "from llama_index.core.readers.base import BaseReader\n",
    "from llama_index.core.schema import Document\n",
    "\n",
    "class FinancialReportReader(BaseReader):\n",
    "    \"\"\"\n",
    "    一个用于解析特定格式财报文本文件的自定义读取器。\n",
    "    它能识别不同的章节，并将每个章节创建为一个独立的 Document 对象，\n",
    "    同时提取报告级别的元数据和章节级别的元数据。\n",
    "    \"\"\"\n",
    "    def load_data(\n",
    "        self, file: Path, extra_info: Optional[Dict] = None\n",
    "    ) -> List[Document]:\n",
    "        \"\"\"解析财报文件。\"\"\"\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # 1. 提取报告级别的元数据\n",
    "        report_header_pattern = r\"=== Quarterly Report: (.*?) Q(\\d) (\\d{4}) ===\"\n",
    "        header_match = re.search(report_header_pattern, text)\n",
    "        \n",
    "        company_name = \"Unknown\"\n",
    "        quarter = \"N/A\"\n",
    "        year = \"N/A\"\n",
    "        if header_match:\n",
    "            company_name = header_match.group(1).strip()\n",
    "            quarter = header_match.group(2).strip()\n",
    "            year = header_match.group(3).strip()\n",
    "\n",
    "        report_date_pattern = r\"\\*\\*Report Date:\\*\\* (.*)\"\n",
    "        date_match = re.search(report_date_pattern, text)\n",
    "        report_date = date_match.group(1).strip() if date_match else \"N/A\"\n",
    "\n",
    "        base_metadata = {\n",
    "            \"source_file\": file.name,\n",
    "            \"company_name\": company_name,\n",
    "            \"report_quarter\": f\"Q{quarter} {year}\",\n",
    "            \"report_date\": report_date,\n",
    "            **(extra_info or {}),\n",
    "        }\n",
    "\n",
    "        # 2. 按章节切分文档\n",
    "        section_pattern = r\"--- Section: (.*?) ---\"\n",
    "        parts = re.split(section_pattern, text)\n",
    "        \n",
    "        if not parts:\n",
    "            return [Document(text=text, metadata=base_metadata)]\n",
    "\n",
    "        results = []\n",
    "        for i in range(1, len(parts), 2):\n",
    "            section_title = parts[i].strip()\n",
    "            section_content = parts[i+1].strip().replace(\"=== End of Report ===\", \"\").strip()\n",
    "\n",
    "            if not section_content:\n",
    "                continue\n",
    "\n",
    "            # 3. 为每个章节创建 Document，并合并元数据\n",
    "            section_metadata = base_metadata.copy()\n",
    "            section_metadata[\"section_title\"] = section_title\n",
    "            \n",
    "            doc = Document(text=section_content, metadata=section_metadata)\n",
    "            results.append(doc)\n",
    "\n",
    "        return results\n",
    "\n",
    "# --- 实战演练 ---\n",
    "# 1. 为了让代码可独立运行，我们先在本地创建一个假的财报文件\n",
    "report_content = \"\"\"\n",
    "=== Quarterly Report: FutureTech Inc. Q1 2025 ===\n",
    "\n",
    "**Report Date:** 2025-04-28\n",
    "\n",
    "--- Section: Management Discussion and Analysis ---\n",
    "\n",
    "Our performance in the first quarter of 2025 has been outstanding. We launched the new \"QuantumLeap\" processor, which has seen strong market adoption. Revenue grew by 20% year-over-year, driven by our AI division.\n",
    "\n",
    "--- Section: Financial Highlights ---\n",
    "\n",
    "- Revenue: $500 Million\n",
    "- Net Profit: $80 Million\n",
    "- Earnings Per Share (EPS): $1.25\n",
    "- Cash Flow from Operations: $120 Million\n",
    "\n",
    "--- Section: Risk Factors ---\n",
    "\n",
    "The ongoing global chip shortage continues to be a primary risk. Furthermore, increasing competition in the AI space requires us to innovate continuously. Regulatory changes in data privacy could also impact our cloud services business.\n",
    "\n",
    "=== End of Report ===\n",
    "\"\"\"\n",
    "# 创建数据目录和文件\n",
    "data_dir = Path(\"./temp_data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "report_file_path = data_dir / \"quarterly_report_q1_2025.txt\"\n",
    "with open(report_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report_content)\n",
    "\n",
    "print(f\"创建了示例财报文件: {report_file_path}\\n\")\n",
    "\n",
    "# 2. 实例化并使用我们的自定义读取器\n",
    "reader = FinancialReportReader()\n",
    "documents = reader.load_data(file=report_file_path)\n",
    "\n",
    "# 3. 验证解析结果\n",
    "print(f\"成功将一份财报解析为 {len(documents)} 个独立的 Document 节点。\\n\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"--- Document {i+1} ---\")\n",
    "    print(f\"内容 (Text):\\n{doc.text}\\n\")\n",
    "    print(f\"元数据 (Metadata):\\n{doc.metadata}\\n\")\n",
    "    print(\"---------------------\\n\")\n",
    "\n",
    "# 清理创建的临时文件\n",
    "# import shutil\n",
    "# shutil.rmtree(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分块（Chunking）策略的极限优化\n",
    "如果说解析是准备“食材”，那么分块就是“切菜”。如何切，直接决定了这道菜（上下文）的最终口感。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 上下文感知分块：语义分块（Semantic Chunking）与句窗检索（Sentence Window Retrieval）的实现细节与场景权衡\n",
    "\n",
    "传统的基于固定大小的分块方式是“盲切”，很容易破坏句子的完整语义。上下文感知分块则是“巧切”，力求在语义最自然的地方下刀。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 原始文档和 DashScope 模型准备完毕 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 准备工作 ---\n",
    "# 1. 安装必要的 LlamaIndex 包以及依赖\n",
    "# !pip install llama-index\n",
    "# !pip install llama-index-embeddings-dashscope  # 注意：需要安装 DashScope 的专属包\n",
    "# !pip install sentence-transformers\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "# 2. 设置您的 DashScope API Key\n",
    "# 在实际运行前，请确保您已经设置了环境变量 DASHSCOPE_API_KEY\n",
    "# import os\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"YOUR_DASHSCOPE_API_KEY\"\n",
    "\n",
    "from llama_index.core import Document, Settings\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser, SemanticSplitterNodeParser\n",
    "\n",
    "# --- 准备数据和模型 ---\n",
    "\n",
    "# 创建一个示例文档\n",
    "document = Document(\n",
    "    text=(\n",
    "        \"LlamaIndex 是一个强大的数据框架，用于构建基于大语言模型的应用。\"\n",
    "        \"它提供了数据摄入、索引、查询等一系列工具。其核心理念是连接自定义数据源与大语言模型。\\n\\n\"\n",
    "        \"另一方面，太阳系是以太阳为中心，受其引力约束在一起的天体系统。\"\n",
    "        \"它包括八大行星、矮行星、卫星、小行星、彗星等。地球是太阳系中从内到外的第三颗行星。\\n\\n\"\n",
    "        \"回到软件开发领域，版本控制是现代软件工程中不可或缺的一环。\"\n",
    "        \"Git 是目前最流行的分布式版本控制系统。\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 加载一个用于语义分析的嵌入模型\n",
    "embed_model = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v3\"\n",
    ")\n",
    "Settings.embed_model = embed_model\n",
    "print(\"--- 原始文档和 DashScope 模型准备完毕 ---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`SemanticChunker` (语义分块器)**:\n",
    "  - **原理**: 它首先将文档分割成句子，然后计算相邻句子之间的嵌入向量相似度。当相似度出现一个“断崖式下跌”时，就意味着话题可能发生了转变，这是一个理想的切分点。\n",
    "  - **适用场景**: 处理包含多个独立主题的混合文档，如会议纪要、研究论文。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "方案一: SemanticSplitter 用法\n",
      "==============================\n",
      "语义分割后得到 1 个块:\n",
      "--- 块 1 ---\n",
      "LlamaIndex 是一个强大的数据框架，用于构建基于大语言模型的应用。它提供了数据摄入、索引、查询等一系列工具。其核心理念是连接自定义数据源与大语言模型。\n",
      "\n",
      "另一方面，太阳系是以太阳为中心，受其引力约束在一起的天体系统。它包括八大行星、矮行星、卫星、小行星、彗星等。地球是太阳系中从内到外的第三颗行星。\n",
      "\n",
      "回到软件开发领域，版本控制是现代软件工程中不可或缺的一环。Git 是目前最流行的分布式版本控制系统。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 目标：根据语义的“断裂点”来切分文档。\n",
    "print(\"=\" * 30)\n",
    "print(\"方案一: SemanticSplitter 用法\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 2. 初始化语义分割器时，不再直接传入 embed_model。\n",
    "# 它会自动从全局的 Settings.embed_model 中获取。\n",
    "semantic_splitter = SemanticSplitterNodeParser(\n",
    "    breakpoint_percentile_threshold=95,\n",
    "    embed_model=Settings.embed_model,\n",
    ")\n",
    "\n",
    "# 3. 调用 .get_nodes_from_documents() 来获取切分后的节点 (Node)\n",
    "semantic_nodes = semantic_splitter.get_nodes_from_documents([document])\n",
    "\n",
    "# 4. 打印结果，查看分块效果\n",
    "print(f\"语义分割后得到 {len(semantic_nodes)} 个块:\")\n",
    "for i, node in enumerate(semantic_nodes):\n",
    "    print(f\"--- 块 {i+1} ---\\n{node.get_content().strip()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`SentenceWindowNodeParser` (句窗节点解析器)**:\n",
    "  - **原理**: 它将每个句子都处理成一个独立的 `Node`。在检索时，如果某个句子节点被命中，它不仅返回这个句子本身，还会自动把它**前后 `k` 个句子**（即“窗口”）一同作为上下文返回给 LLM。\n",
    "  - **适用场景**: 需要对非常具体、精细的事实进行问答的场景，如法律条文、技术规格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "方案二: SentenceWindowNodeParser 用法\n",
      "==============================\n",
      "句窗处理后得到 8 个节点。\n",
      "--- 以最后一个节点为例 ---\n",
      "节点核心文本: 'Git 是目前最流行的分布式版本控制系统'\n",
      "节点的窗口上下文: '回到软件开发领域，版本控制是现代软件工程中不可或缺的一环 Git 是目前最流行的分布式版本控制系统'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 目标：将每个句子作为一个节点，并把它的邻居句子作为上下文。\n",
    "print(\"\\n\" + \"=\" * 30)\n",
    "print(\"方案二: SentenceWindowNodeParser 用法\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 初始化句窗节点解析器\n",
    "sentence_window_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=1,\n",
    "    # 为中文文本显式提供一个句子分割函数\n",
    "    # 默认的分割器无法正确处理中文句号 \"。\",导致整个文本被当做一个节点。\n",
    "    sentence_splitter=lambda text: [s.strip() for s in text.split('。') if s.strip()],\n",
    ")\n",
    "\n",
    "# 调用 .get_nodes_from_documents()\n",
    "sentence_nodes = sentence_window_parser.get_nodes_from_documents([document])\n",
    "\n",
    "# 打印一个节点，观察其结构\n",
    "print(f\"句窗处理后得到 {len(sentence_nodes)} 个节点。\")\n",
    "\n",
    "# 检查列表是否为空，避免 IndexError\n",
    "if sentence_nodes:\n",
    "    print(f\"--- 以最后一个节点为例 ---\")\n",
    "    sample_node = sentence_nodes[-1]\n",
    "\n",
    "    # get_content() 获取的是核心句子的内容\n",
    "    print(f\"节点核心文本: '{sample_node.get_content()}'\")\n",
    "\n",
    "    # .metadata['window'] 获取的是完整的上下文窗口\n",
    "    print(f\"节点的窗口上下文: '{sample_node.metadata['window']}'\\n\")\n",
    "else:\n",
    "    print(\"错误: 未能从文档中生成任何句子节点。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 长文档终极方案：层次化分块（Hierarchical Chunking）与自动合并检索（Auto-Merging Retriever）的协同工作流\n",
    "面对上百页的 PDF 或书籍，如何既能进行全局概览，又能深入细节？答案是分层。\n",
    "\n",
    "- **`HierarchicalNodeParser` (层次化节点解析器)**: 它会自动地、递归地对文档进行分块，先切成大的“父块”，再把每个父块细切成小的“子块”。\n",
    "- `AutoMergingRetriever` **(自动合并检索器)**: 它与层次化节点协同工作。在检索时，它首先在“子块”层进行搜索。如果检索到的多个“子块”都来自同一个“父块”，它就会“智能地”放弃返回零碎的子块，转而直接返回那个更大、更完整的“父块”。\n",
    "\n",
    "这个组合实现了**查询自适应（Query-adaptive）**的上下文粒度，是目前处理长文档最优雅和高效的方案之一。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 文档和模型准备完毕 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document, Settings, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "\n",
    "# --- 准备数据和模型 ---\n",
    "\n",
    "# 创建一个比之前更长、更复杂的示例文档\n",
    "# 以便更好地展示分层效果\n",
    "document = Document(\n",
    "    text=(\n",
    "        \"# 现代人工智能导论\\n\\n\"\n",
    "        \"## 第一章：机器学习基础\\n\"\n",
    "        \"机器学习（ML）是人工智能的核心分支，它研究计算机如何从数据中学习。其基本思想是，通过算法分析大量数据，模型可以自动发现数据中的模式并用于预测或决策。\\n\"\n",
    "        \"常见的机器学习范式包括监督学习、无监督学习和强化学习。\\n\"\n",
    "        \"监督学习使用带有标签的数据进行训练，例如，用标记为“猫”或“狗”的图片训练一个图像分类器。其主要任务是分类和回归。\\n\"\n",
    "        \"无监督学习则处理没有标签的数据，试图发现数据中固有的结构或模式，如聚类和降维。\\n\\n\"\n",
    "        \"## 第二章：深度学习与神经网络\\n\"\n",
    "        \"深度学习是机器学习的一个子领域，它基于人工神经网络（ANN）。这些网络的灵感来源于人脑的结构，由多个相互连接的节点（神经元）层组成。\\n\"\n",
    "        \"“深度”指的就是神经网络中包含多个隐藏层。卷积神经网络（CNN）在图像处理方面表现出色，而循环神经网络（RNN）及其变体（如LSTM和GRU）则擅长处理序列数据，如自然语言。\\n\"\n",
    "        \"Transformer架构是近年来自然语言处理（NLP）领域的重大突破，它引入了自注意力机制，能够更好地处理长距离依赖关系。著名的模型如BERT和GPT都基于此架构。\\n\\n\"\n",
    "        \"## 第三章：人工智能伦理\\n\"\n",
    "        \"随着人工智能技术的广泛应用，其伦理问题也日益凸显。核心议题包括算法的公平性、透明度和问责制。\\n\"\n",
    "        \"算法偏见是一个严重问题，如果训练数据中存在偏见，模型就可能会复制甚至放大这种偏见，导致对特定人群的不公平对待。\\n\"\n",
    "        \"“黑箱”问题指的是许多复杂模型（如深度神经网络）的决策过程难以解释，这在医疗、金融等高风险领域是不可接受的。因此，可解释性AI（XAI）的研究变得至关重要。\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"--- 文档和模型准备完毕 ---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "步骤3: 配置自动合并检索器并查询\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 配置并使用 AutoMergingRetriever ---\n",
    "print(\"=\" * 30)\n",
    "print(\"步骤3: 配置自动合并检索器并查询\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 获取基础的向量检索器\n",
    "base_retriever = base_index.as_retriever(similarity_top_k=6)\n",
    "\n",
    "# 将基础检索器包装成 AutoMergingRetriever\n",
    "# 它会自动检查检索到的多个子节点是否属于同一个父节点，如果是，就返回父节点\n",
    "retriever = AutoMergingRetriever(base_retriever, storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询: 请详细解释一下 Transformer 架构及其在NLP领域的应用\n",
      "\n",
      "--- 基础检索器返回的节点 (零散) ---\n",
      "节点 (ID: 1e64b604-833d-4072-9322-70b8c54e0c4a, 大小: 83)\n",
      "节点 (ID: 70cb1cc5-12d0-4056-a86b-0d0b5a20cdd0, 大小: 123)\n",
      "节点 (ID: 052f2062-25ab-4e5a-95c5-f5b62300e778, 大小: 83)\n",
      "节点 (ID: d828b3b4-0802-47d9-9234-287a403746fb, 大小: 102)\n",
      "节点 (ID: fc50666e-0043-4f42-b174-f27e751f3329, 大小: 78)\n",
      "节点 (ID: 7ace9d82-1082-40e4-9d4a-1102ac3a0da3, 大小: 101)\n",
      "--------------------\n",
      "\n",
      "--- 自动合并检索器返回的节点 (更完整) ---\n",
      "节点 (ID: 22737d04-7b20-4272-8bcd-68af03058f42, 大小: 681)\n",
      "内容预览: # 现代人工智能导论\n",
      "\n",
      "## 第一章：机器学习基础\n",
      "机器学习（ML）是人工智能的核心分支，它研究计算机如何从数据中学习。其基本思想是，通过算法分析大量数据，模型可以自动发现数据中的模式并用于预测或决策...\n",
      "--------------------\n",
      "\n",
      "观察上面两个输出的差异，可以看到自动合并检索器返回了更少但内容更完整（size更大）的父节点。\n",
      "\n",
      "--- 最终生成的答案 ---\n",
      "Transformer架构是一种革命性的神经网络结构，它通过自注意力机制来处理序列数据。这种架构的核心在于能够同时关注输入序列中的所有位置，从而有效捕捉长距离依赖关系，而不像传统循环神经网络那样需要逐步处理序列。\n",
      "\n",
      "该架构主要由编码器和解码器组成，每部分都包含多层结构。关键创新点包括：\n",
      "\n",
      "1. 自注意力机制：允许模型在处理每个词时动态计算与其他所有词的相关性权重\n",
      "2. 位置编码：为输入序列添加位置信息，弥补了传统注意力机制缺乏序列顺序感知的缺陷\n",
      "3. 多头注意力：并行运行多个注意力机制，可以捕捉不同子空间的特征表示\n",
      "\n",
      "在自然语言处理领域，Transformer架构带来了重大突破：\n",
      "\n",
      "1. 机器翻译：显著提升了翻译质量，特别是在处理长句子时\n",
      "2. 文本生成：能够生成连贯、上下文相关的文本内容\n",
      "3. 语言理解：大幅提高了问答系统、文本分类等任务的性能\n",
      "\n",
      "基于Transformer的著名模型如BERT采用了双向训练方式，能够更好地理解上下文语义；而GPT系列模型则展示了强大的生成能力。这些模型通过预训练加微调的方式，在多项NLP任务上达到了前所未有的性能水平。\n"
     ]
    }
   ],
   "source": [
    "# 进行查询演示\n",
    "if Settings.llm:\n",
    "    query_str = \"请详细解释一下 Transformer 架构及其在NLP领域的应用\"\n",
    "\n",
    "    print(f\"查询: {query_str}\\n\")\n",
    "\n",
    "    # 首先，我们看看基础检索器会返回什么 (通常是零散的子块)\n",
    "    base_retrieved_nodes = base_retriever.retrieve(query_str)\n",
    "    print(\"--- 基础检索器返回的节点 (零散) ---\")\n",
    "    for n in base_retrieved_nodes:\n",
    "        print(f\"节点 (ID: {n.node_id}, 大小: {len(n.text)})\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    # 然后，看看自动合并检索器返回了什么\n",
    "    merged_retrieved_nodes = retriever.retrieve(query_str)\n",
    "    print(\"\\n--- 自动合并检索器返回的节点 (更完整) ---\")\n",
    "    for n in merged_retrieved_nodes:\n",
    "        print(f\"节点 (ID: {n.node_id}, 大小: {len(n.text)})\")\n",
    "        print(f\"内容预览: {n.text[:100].strip()}...\")\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "    print(\"\\n观察上面两个输出的差异，可以看到自动合并检索器返回了更少但内容更完整（size更大）的父节点。\")\n",
    "\n",
    "    # 最后，用自动合并检索器构建查询引擎并获得最终答案\n",
    "    query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "    response = query_engine.query(query_str)\n",
    "\n",
    "    print(f\"\\n--- 最终生成的答案 ---\\n{response}\")\n",
    "else:\n",
    "    print(\"跳过查询演示。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "步骤1: 执行分层节点解析\n",
      "==============================\n",
      "总节点数 (包括父节点): 11\n",
      "叶子节点数 (最细粒度): 8\n",
      "\n",
      "--- 第一个叶子节点的内容 ---\n",
      "'# 现代人工智能导论\n",
      "\n",
      "## 第一章：机器学习基础\n",
      "机器学习（ML）是人工智能的核心分支，它研究计算机如何从数据中学习。其基本思想是，通过算法分析大量数据，模型可以自动发现数据中的模式并用于预测或决策。'\n",
      "==============================\n",
      "步骤2: 构建索引与存储\n",
      "==============================\n",
      "索引构建完毕。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes\n",
    "from llama_index.core.schema import NodeRelationship\n",
    "\n",
    "# --- 1. 使用 HierarchicalNodeParser 进行分层解析 ---\n",
    "print(\"=\" * 30)\n",
    "print(\"步骤1: 执行分层节点解析\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    chunk_sizes=[2048, 512, 128]\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents([document])\n",
    "leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "print(f\"总节点数 (包括父节点): {len(nodes)}\")\n",
    "print(f\"叶子节点数 (最细粒度): {len(leaf_nodes)}\")\n",
    "\n",
    "if leaf_nodes:\n",
    "    print(f\"\\n--- 第一个叶子节点的内容 ---\\n'{leaf_nodes[0].get_content().strip()}'\")\n",
    "    parent_info = leaf_nodes[0].relationships.get(NodeRelationship.PARENT)\n",
    "    if parent_info and hasattr(parent_info, 'node'):\n",
    "        parent_node = parent_info.node\n",
    "        print(f\"\\n--- 其父节点的内容 ---\\n'{parent_node.get_content().strip()}'\\n\")\n",
    "else:\n",
    "    print(\"未能解析出任何叶子节点。\")\n",
    "\n",
    "# --- 2. 构建索引和存储上下文 ---\n",
    "print(\"=\" * 30)\n",
    "print(\"步骤2: 构建索引与存储\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "\n",
    "docstore = SimpleDocumentStore()\n",
    "docstore.add_documents(nodes)\n",
    "storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "\n",
    "base_index = VectorStoreIndex(\n",
    "    leaf_nodes,\n",
    "    storage_context=storage_context,\n",
    ")\n",
    "\n",
    "print(\"索引构建完毕。\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Token 边界问题：多语言 Tokenizer 对齐与分块效果的微妙关系\n",
    "这是一个在多语言场景下非常隐蔽但重要的问题。绝大多数模型的 Tokenizer 可能会将一个完整的中文词切成多个无意义的 Token。如果分块恰好发生在这几个 Token 中间，词的语义就会被破坏。\n",
    "\n",
    "**解决方案**:\n",
    "\n",
    "1. **感知 Token 的分块器**: 使用 `TokenTextSplitter`，它会尽量在完整的 Token 边界上进行切分。\n",
    "2. **选择对多语言友好的模型**: 如 `BGE` 系列，其 Tokenizer 表现更好。\n",
    "3. **留出 Buffer**: 在分块时，不要把 `chunk_size` 设得太极限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嵌入（Embedding）模型的精细化作战\n",
    "\n",
    "嵌入模型是将文本“翻译”成数学语言（向量）的译者。译者的水平，直接决定了语义搜索的上限。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型选型与评测：基于 MTEB 基准的 BGE, Jina, Cohere 等模型深度对比\n",
    "\n",
    "如何选择最好的嵌入模型？不要凭感觉，要看公开、权威的排行榜，如 Hugging Face 上的 **MTEB (Massive Text Embedding Benchmark) Leaderboard**。\n",
    "\n",
    "**选型决策三角：**\n",
    "\n",
    "1. **性能（Performance）**: 在 MTEB 上的得分。\n",
    "2. **成本（Cost）**: API vs. 本地部署。\n",
    "3. **速度/尺寸（Speed/Size）**: 实时 vs. 离线。\n",
    "\n",
    "**社区热门的开源模型推荐**: `BGE (BAAI General Embedding)`, `Jina Embeddings`, `Cohere Embed v3`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 领域适应性微调：使用 SentenceTransformers 微调嵌入模型以理解特定术语\n",
    "\n",
    "如果你的 RAG 系统处理的是高度专业化的领域（如医疗、法律），通用嵌入模型可能无法很好地理解其中的专有术语。这时，就需要对嵌入模型进行**微调**。这通常需要构造一个包含 `(query, positive_passage, negative_passage)` 的三元组数据集，并使用 `sentence-transformers` 库进行训练。这是一个高级且耗费资源的过程，但对于追求极致性能的专业应用来说，回报是巨大的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前沿嵌入技术：ColBERT 与多向量（Multi-Vector）嵌入如何提升细粒度检索精度\n",
    "- **`ColBERT`**: 一种“晚交互”嵌入范式。它不把整个文本压缩成一个向量，而是为文本中的**每一个 Token** 都生成一个向量。这极大地提升了对关键词和短语的细粒度匹配能力。\n",
    "- **`Multi-Vector Retriever`**: 为一个 `Node` 生成多个向量，如**全文向量**、**摘要向量**和**假设性问题向量**。在检索时，可以用用户的查询同时去匹配这几种不同的向量，从而从不同角度捕捉相关性。\n",
    "\n",
    "**本章小结**\n",
    "\n",
    "我们系统地学习了将原始数据转化为高质量知识 `Node` 的全过程。从智能解析、上下文感知分块，到嵌入模型的精选与微调，你已经掌握了构建一个高性能 RAG 系统所需的最核心的底层技术。这些环节的每一个微小优化，都将在最终的应用效果上产生巨大的放大效应。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二部分：引擎核心篇 - 检索、重排与融合 (The Retrieval & Ranking Engine)\n",
    "欢迎来到 RAG 系统的“决策中枢”。如果说第一部分“注入与索引”是构建一座高质量的知识金矿，那么本章“检索、重排与融合”的核心任务，就是设计一套最高效、最智能的“采矿与提纯”系统。这个系统的最终输出——上下文（Context）的质量，直接决定了 LLM 生成答案的上限。\n",
    "\n",
    "在本章中，我们将从三个层面，层层递进，打造一个专家级的检索引擎：\n",
    "\n",
    "1. **检索策略（Retrieval）**：如何“广撒网”，确保所有可能相关的知识都被初步召回。\n",
    "2. **重排（Re-ranking）**：如何“精加工”，在召回的结果中，优中选优，将最相关的内容排到最前面。\n",
    "3. **融合（Fusion）**：如何“集大成”，当有多个信息源时，智能地合并它们的结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 检索策略的“组合拳”\n",
    "单一的检索策略往往有其局限性。例如，纯向量检索可能找不到包含特定关键词的精准匹配；纯关键词检索又无法理解语义上的相似性。因此，在真实世界的复杂应用中，我们通常需要打出一套“组合拳”。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 混合检索深度实现：融合稀疏（BM25/SPLADE）与稠密（Vector）检索的战术\n",
    "这是最经典、最有效的组合策略，旨在同时利用两种检索方式的优势：\n",
    "\n",
    "- **稠密检索（Dense Retrieval）**：即我们熟知的向量检索。它通过嵌入模型将文本映射到高维向量空间，擅长理解**语义（Semantic）**和**上下文（Context）**。例如，查询“苹果公司的最新手机”能匹配到包含“iPhone 16”的文本。\n",
    "- **稀疏检索（Sparse Retrieval）**：以传统的 `BM25` 算法为代表。它基于词频统计（TF-IDF），不理解语义，但对于**关键词（Keyword）** 和 **专有术语（Jargon）** 的匹配极其精准。例如，查询“BM25 算法”能精确找到包含这个词的文档，而向量检索可能会因为这个词在训练语料中不常见而效果不佳。\n",
    "\n",
    "**战术：** 将两者结合，既能理解用户模糊的意图，又能抓住查询中的关键术语。LlamaIndex 提供了 `QueryFusionRetriever` 来优雅地实现这一点。\n",
    "\n",
    "**代码示例：使用 `QueryFusionRetriever` 实现混合检索**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 查询 1: 苹果发布了什么AI技术? ---\n",
      "Generated queries:\n",
      "1. 苹果公司最新发布的人工智能技术有哪些？\n",
      "2. 苹果AI技术的最新进展和应用场景\n",
      "3. 苹果公司在2023年推出了哪些人工智能产品？\n",
      "答案: 苹果公司发布了一套名为Apple Intelligence的个人化智能系统，深度集成于iOS、iPadOS和macOS平台。\n",
      "\n",
      "来源: doc4.txt, 得分: 0.13224043715846995\n",
      "来源: doc1.txt, 得分: 0.06666666666666667\n",
      "\n",
      "--- 查询 2: 什么是 BM25? ---\n",
      "Generated queries:\n",
      "1. BM25 算法原理及公式详解\n",
      "2. BM25 算法与 TF-IDF 的区别和对比\n",
      "3. BM25 算法在信息检索中的应用场景\n",
      "答案: BM25算法是一种基于词频的传统搜索引擎检索方法，属于稀疏检索技术。它通过统计文档中关键词出现的频率来进行相关性匹配，是信息检索领域常用的基础算法之一。\n",
      "\n",
      "来源: doc2.txt, 得分: 0.13333333333333333\n",
      "来源: doc3.txt, 得分: 0.06557377049180328\n"
     ]
    }
   ],
   "source": [
    "# 安装 BM25 的相关依赖\n",
    "# !pip install rank_bm25 llama-index-retrievers-bm25\n",
    "\n",
    "import os\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext\n",
    ")\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "# --- 1. 准备数据和索引 ---\n",
    "# 为了让代码可独立运行，我们先创建一些示例文档\n",
    "documents_content = {\n",
    "    \"doc1.txt\": \"Apple 在 WWDC 2024 上发布了 Apple Intelligence，这是一套深度集成于 iOS、iPadOS 和 macOS 的个人化智能系统。\",\n",
    "    \"doc2.txt\": \"传统的搜索引擎使用 BM25 算法进行关键词匹配，这是一种基于词频的稀疏检索方法。\",\n",
    "    \"doc3.txt\": \"LlamaIndex 框架通过 QueryFusionRetriever 支持混合检索，能有效结合向量搜索和 BM25 的优势。\",\n",
    "    \"doc4.txt\": \"苹果公司推出的 Vision Pro 头显，开创了空间计算的新纪元。\"\n",
    "}\n",
    "data_dir = \"./temp_hybrid_data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "for name, text in documents_content.items():\n",
    "    with open(os.path.join(data_dir, name), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "\n",
    "# 构建向量索引和存储上下文\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(documents)\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, storage_context=storage_context\n",
    ")\n",
    "\n",
    "# --- 2. 创建两个独立的检索器：向量检索器和 BM25 检索器 ---\n",
    "\n",
    "# 向量检索器 (稠密)\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# BM25 检索器 (稀疏)\n",
    "# 它需要从 docstore 中获取节点来构建\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=storage_context.docstore, similarity_top_k=2\n",
    ")\n",
    "\n",
    "# --- 3. 使用 QueryFusionRetriever 融合两者 ---\n",
    "# num_queries: 让 LLM 生成多少个变体查询（包括原始查询）\n",
    "# mode=\"reciprocal_rerank\": 使用 RRF 算法来融合结果\n",
    "retriever = QueryFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=4,  # 生成 3 个变体查询 + 1 个原始查询\n",
    "    mode=FUSION_MODES.RECIPROCAL_RANK,\n",
    "    use_async=False,\n",
    "    verbose=True,  # 打印生成的查询和融合过程\n",
    ")\n",
    "\n",
    "# --- 4. 构建查询引擎并提问 ---\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "\n",
    "# 问题1: 这个问题更偏向语义理解\n",
    "print(\"\\n--- 查询 1: 苹果发布了什么AI技术? ---\")\n",
    "response_semantic = query_engine.query(\"苹果公司发布了什么人工智能技术?\")\n",
    "print(f\"答案: {response_semantic}\\n\")\n",
    "for node in response_semantic.source_nodes:\n",
    "    print(f\"来源: {node.metadata['file_name']}, 得分: {node.score}\")\n",
    "\n",
    "# 问题2: 这个问题包含一个非常精确的术语\n",
    "print(\"\\n--- 查询 2: 什么是 BM25? ---\")\n",
    "response_keyword = query_engine.query(\"什么是 BM25 算法?\")\n",
    "print(f\"答案: {response_keyword}\\n\")\n",
    "for node in response_keyword.source_nodes:\n",
    "    print(f\"来源: {node.metadata['file_name']}, 得分: {node.score}\")\n",
    "\n",
    "# 清理\n",
    "# import shutil\n",
    "# shutil.rmtree(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图检索的威力：利用知识图谱进行多跳（Multi-hop）推理与关系发现\n",
    "当数据中包含大量实体（人、事、物）及其之间的复杂关系时，知识图谱是比纯文本更强大的知识表示方式。图检索的威力在于，它能回答那些需要 **多跳推理（Multi-hop Reasoning）** 的问题。\n",
    "\n",
    "例如，“谁出演了由《泰坦尼克号》的导演执导的另一部电影？” 这个问题，纯向量检索很难回答。但对于知识图谱，这只是一个简单的两步图遍历：\n",
    "\n",
    "这是一个典型的多跳问题。它需要进行以下图遍历：\n",
    "\n",
    "1. 查找实体《泰坦尼克号》。\n",
    "\n",
    "2. 沿着“导演”关系找到对应的导演——“詹姆斯·卡梅隆”。\n",
    "\n",
    "3. 查找“詹姆斯·卡梅隆”导演的其他电影（如《阿凡达》）。\n",
    "\n",
    "4. 查找这些电影的“主演”关系，从而得到演员“萨姆·沃辛顿”等人。\n",
    "\n",
    "这种跨实体、跨关系的链式推理，是知识图谱天然擅长的能力。\n",
    "\n",
    "在 LlamaIndex 中，PropertyGraphIndex（以及早期版本的 KnowledgeGraphIndex）可以自动调用大语言模型（LLM），从原始文本中抽取出结构化的三元组 (主语, 谓语, 宾语)，并构建图索引。这样，用户不仅可以进行单跳查询，还能轻松实现多跳问题的解析与答案生成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  构建知识图谱（支持多跳）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from kg_storage_cn\\docstore.json.\n",
      "📌 未找到索引，重新构建...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     storage_context = \u001b[43mStorageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     index = PropertyGraphIndex.from_storage(storage_context)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\storage\\storage_context.py:113\u001b[39m, in \u001b[36mStorageContext.from_defaults\u001b[39m\u001b[34m(cls, docstore, index_store, vector_store, image_store, vector_stores, graph_store, property_graph_store, persist_dir, fs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     docstore = docstore \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mSimpleDocumentStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_persist_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpersist_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m     index_store = index_store \u001b[38;5;129;01mor\u001b[39;00m SimpleIndexStore.from_persist_dir(\n\u001b[32m    117\u001b[39m         persist_dir, fs=fs\n\u001b[32m    118\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\storage\\docstore\\simple_docstore.py:59\u001b[39m, in \u001b[36mSimpleDocumentStore.from_persist_dir\u001b[39m\u001b[34m(cls, persist_dir, namespace, fs)\u001b[39m\n\u001b[32m     58\u001b[39m     persist_path = os.path.join(persist_dir, DEFAULT_PERSIST_FNAME)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_persist_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\storage\\docstore\\simple_docstore.py:78\u001b[39m, in \u001b[36mSimpleDocumentStore.from_persist_path\u001b[39m\u001b[34m(cls, persist_path, namespace, fs)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     69\u001b[39m \u001b[33;03mCreate a SimpleDocumentStore from a persist path.\u001b[39;00m\n\u001b[32m     70\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \n\u001b[32m     77\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m simple_kvstore = \u001b[43mSimpleKVStore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_persist_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(simple_kvstore, namespace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\storage\\kvstore\\simple_kvstore.py:55\u001b[39m, in \u001b[36mSimpleKVStore.from_persist_path\u001b[39m\u001b[34m(cls, persist_path, fs)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersist_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpersist_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m     56\u001b[39m     data = json.load(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\fsspec\\spec.py:1310\u001b[39m, in \u001b[36mAbstractFileSystem.open\u001b[39m\u001b[34m(self, path, mode, block_size, cache_options, compression, **kwargs)\u001b[39m\n\u001b[32m   1309\u001b[39m ac = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mautocommit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._intrans)\n\u001b[32m-> \u001b[39m\u001b[32m1310\u001b[39m f = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocommit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mac\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\fsspec\\implementations\\local.py:201\u001b[39m, in \u001b[36mLocalFileSystem._open\u001b[39m\u001b[34m(self, path, mode, block_size, **kwargs)\u001b[39m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.makedirs(\u001b[38;5;28mself\u001b[39m._parent(path), exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mLocalFileOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\fsspec\\implementations\\local.py:365\u001b[39m, in \u001b[36mLocalFileOpener.__init__\u001b[39m\u001b[34m(self, path, mode, autocommit, fs, compression, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m \u001b[38;5;28mself\u001b[39m.blocksize = io.DEFAULT_BUFFER_SIZE\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\fsspec\\implementations\\local.py:370\u001b[39m, in \u001b[36mLocalFileOpener._open\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    369\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.autocommit \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m     \u001b[38;5;28mself\u001b[39m.f = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compression:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/iSPACE/PycharmProjects/rag/llamaindexDemo/langgraph/kg_storage_cn/docstore.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m graph_store = SimplePropertyGraphStore()\n\u001b[32m     36\u001b[39m storage_context = StorageContext.from_defaults(property_graph_store=graph_store)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m index = \u001b[43mPropertyGraphIndex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_async\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     41\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m index.storage_context.persist(persist_dir=persist_dir)\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ 索引构建完成。\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:122\u001b[39m, in \u001b[36mBaseIndex.from_documents\u001b[39m\u001b[34m(cls, documents, storage_context, show_progress, callback_manager, transformations, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m     docstore.set_document_hash(doc.get_doc_id(), doc.hash)\n\u001b[32m    115\u001b[39m nodes = run_transformations(\n\u001b[32m    116\u001b[39m     documents,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    117\u001b[39m     transformations,\n\u001b[32m    118\u001b[39m     show_progress=show_progress,\n\u001b[32m    119\u001b[39m     **kwargs,\n\u001b[32m    120\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\property_graph\\base.py:136\u001b[39m, in \u001b[36mPropertyGraphIndex.__init__\u001b[39m\u001b[34m(self, nodes, llm, kg_extractors, property_graph_store, vector_store, use_async, embed_model, embed_kg_nodes, callback_manager, transformations, storage_context, show_progress, **kwargs)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m._embed_kg_nodes = embed_kg_nodes\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m._override_vector_store = (\n\u001b[32m    132\u001b[39m     vector_store \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m storage_context.property_graph_store.supports_vector_queries\n\u001b[32m    134\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:79\u001b[39m, in \u001b[36mBaseIndex.__init__\u001b[39m\u001b[34m(self, nodes, objects, index_struct, storage_context, callback_manager, transformations, show_progress, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index_struct \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     78\u001b[39m     nodes = nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     index_struct = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m     82\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28mself\u001b[39m._index_struct = index_struct\n\u001b[32m     84\u001b[39m \u001b[38;5;28mself\u001b[39m._storage_context.index_store.add_index_struct(\u001b[38;5;28mself\u001b[39m._index_struct)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:189\u001b[39m, in \u001b[36mBaseIndex.build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **build_kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Build the index from nodes.\"\"\"\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28mself\u001b[39m._docstore.add_documents(nodes, allow_update=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_index_from_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mbuild_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\property_graph\\base.py:336\u001b[39m, in \u001b[36mPropertyGraphIndex._build_index_from_nodes\u001b[39m\u001b[34m(self, nodes, **build_kwargs)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_build_index_from_nodes\u001b[39m(\n\u001b[32m    333\u001b[39m     \u001b[38;5;28mself\u001b[39m, nodes: Optional[Sequence[BaseNode]], **build_kwargs: Any\n\u001b[32m    334\u001b[39m ) -> IndexLPG:\n\u001b[32m    335\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build index from nodes.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m     nodes = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_insert_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# this isn't really used or needed\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m IndexLPG()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\property_graph\\base.py:208\u001b[39m, in \u001b[36mPropertyGraphIndex._insert_nodes\u001b[39m\u001b[34m(self, nodes)\u001b[39m\n\u001b[32m    202\u001b[39m     nodes = asyncio.run(\n\u001b[32m    203\u001b[39m         arun_transformations(\n\u001b[32m    204\u001b[39m             nodes, \u001b[38;5;28mself\u001b[39m._kg_extractors, show_progress=\u001b[38;5;28mself\u001b[39m._show_progress\n\u001b[32m    205\u001b[39m         )\n\u001b[32m    206\u001b[39m     )\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m     nodes = \u001b[43mrun_transformations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_kg_extractors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_progress\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# ensure all nodes have nodes and/or relations in metadata\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m    214\u001b[39m     node.metadata.get(KG_NODES_KEY) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    215\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m node.metadata.get(KG_RELATIONS_KEY) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes\n\u001b[32m    217\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\ingestion\\pipeline.py:101\u001b[39m, in \u001b[36mrun_transformations\u001b[39m\u001b[34m(nodes, transformations, in_place, cache, cache_collection, **kwargs)\u001b[39m\n\u001b[32m     99\u001b[39m             cache.put(\u001b[38;5;28mhash\u001b[39m, nodes, collection=cache_collection)\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         nodes = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\instrumentation\\dispatcher.py:323\u001b[39m, in \u001b[36mDispatcher.span.<locals>.wrapper\u001b[39m\u001b[34m(func, instance, args, kwargs)\u001b[39m\n\u001b[32m    320\u001b[39m             _logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio.Future):\n\u001b[32m    325\u001b[39m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[32m    326\u001b[39m         new_future = asyncio.ensure_future(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\indices\\property_graph\\transformations\\simple_llm.py:78\u001b[39m, in \u001b[36mSimpleLLMPathExtractor.__call__\u001b[39m\u001b[34m(self, nodes, show_progress, **kwargs)\u001b[39m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\n\u001b[32m     75\u001b[39m     \u001b[38;5;28mself\u001b[39m, nodes: Sequence[BaseNode], show_progress: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs: Any\n\u001b[32m     76\u001b[39m ) -> Sequence[BaseNode]:\n\u001b[32m     77\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Extract triples from nodes.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43macall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core.indices.property_graph import PropertyGraphIndex\n",
    "from llama_index.core.graph_stores import SimplePropertyGraphStore\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import os\n",
    "\n",
    "# 文本路径\n",
    "data_dir = \"movie_data_cn\"\n",
    "persist_dir = \"kg_storage_cn\"\n",
    "\n",
    "# 确保数据文件存在\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "with open(os.path.join(data_dir, \"movies_cn.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        \"詹姆斯·卡梅隆执导了电影《泰坦尼克号》。\\n\"\n",
    "        \"《泰坦尼克号》由莱昂纳多·迪卡普里奥和凯特·温斯莱特主演。\\n\"\n",
    "        \"詹姆斯·卡梅隆也执导了科幻电影《阿凡达》。\\n\"\n",
    "        \"《阿凡达》的主演是萨姆·沃辛顿和佐伊·索尔达娜。\\n\"\n",
    "        \"克里斯托弗·诺兰执导了电影《盗梦空间》。\\n\"\n",
    "        \"《盗梦空间》的演员包括莱昂纳多·迪卡普里奥和艾略特·佩吉。\\n\"\n",
    "    )\n",
    "\n",
    "# 读取文档\n",
    "documents = SimpleDirectoryReader(data_dir).load_data()\n",
    "\n",
    "# 尝试加载或构建图索引\n",
    "try:\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)\n",
    "    index = PropertyGraphIndex.from_storage(storage_context)\n",
    "    print(\"✅ 已加载知识图谱索引。\")\n",
    "except:\n",
    "    print(\"📌 未找到索引，重新构建...\")\n",
    "    graph_store = SimplePropertyGraphStore()\n",
    "    storage_context = StorageContext.from_defaults(property_graph_store=graph_store)\n",
    "    index = PropertyGraphIndex.from_documents(\n",
    "        documents,\n",
    "        storage_context=storage_context,\n",
    "        use_async=False\n",
    "    )\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    print(\"✅ 索引构建完成。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 运行多跳问题查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Markdown, display\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 构建图检索引擎\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m retriever = \u001b[43mindex\u001b[49m.as_retriever(verbose=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m query_engine = RetrieverQueryEngine.from_args(retriever)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# 多跳中文查询\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# 构建图检索引擎\n",
    "retriever = index.as_retriever(verbose=True)\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "\n",
    "# 多跳中文查询\n",
    "query = \"谁出演了由《泰坦尼克号》的导演执导的另一部电影？\"\n",
    "print(f\"\\n--- 开始查询: {query} ---\\n\")\n",
    "response = query_engine.query(query)\n",
    "\n",
    "# 展示结果\n",
    "display(Markdown(f\"**查询问题:** {query}\"))\n",
    "display(Markdown(f\"**模型回答:** {response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 元数据过滤的艺术：从精确匹配到利用 LLM 自动生成结构化过滤器\n",
    "\n",
    "在第一部分我们已经展示了自定义解析器如何为 `Node` 附加丰富的元数据。现在，我们来利用它。\n",
    "\n",
    "- **精确匹配过滤**：这是最直接的方式。当你知道答案的某个结构化属性时（如“来源必须是 2024 年的财报”），可以在检索时施加一个过滤器，极大地缩小搜索范围，提升速度和精度。\n",
    "- **LLM 自动过滤 (`VectorIndexAutoRetriever`)**：这是一种更高级的玩法。你不需要手动编写过滤器，而是让 LLM 自己从你的自然语言问题中 **“悟出”** 过滤条件。\n",
    "  - **工作原理**：你首先要向 `VectorIndexAutoRetriever` “介绍”你的元数据有哪些字段、每个字段的含义是什么。然后，当你提问“请给我找一下关于 FutureTech 公司在 2025 年 Q1 的风险因素”时，LLM 会自动分析这个问题，并生成一个结构化的元数据过滤器，类似：`{\"company_name\": \"FutureTech Inc.\", \"report_quarter\": \"Q1 2025\", \"section_title\": \"Risk Factors\"}`。接着，检索器会带着这个过滤器去向量数据库中执行查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造带结构化元数据的文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "# 构造带元数据的文档列表\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"在 2025 年 Q1，FutureTech 报告中指出最大风险是供应链中断。\",\n",
    "        metadata={\n",
    "            \"company_name\": \"FutureTech\",\n",
    "            \"report_quarter\": \"2025 Q1\",\n",
    "            \"section_title\": \"Risk Factors\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"2025 Q1 的市场扩展策略是 FutureTech 成长计划的重点。\",\n",
    "        metadata={\n",
    "            \"company_name\": \"FutureTech\",\n",
    "            \"report_quarter\": \"2025 Q1\",\n",
    "            \"section_title\": \"Strategy\"\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"2024 Q4，FutureTech 报告关注的是成本控制。\",\n",
    "        metadata={\n",
    "            \"company_name\": \"FutureTech\",\n",
    "            \"report_quarter\": \"2024 Q4\",\n",
    "            \"section_title\": \"Risk Factors\"\n",
    "        }\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构建向量索引并添加文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# 构建索引\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 设置 AutoRetriever 的字段说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import VectorIndexAutoRetriever\n",
    "from llama_index.core.vector_stores import VectorStoreInfo, MetadataInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"公司财报文本段落\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(name=\"company_name\", description=\"公司名称\", type=\"str\"),\n",
    "        MetadataInfo(name=\"report_quarter\", description=\"报告季度（格式如 '2025 Q1'）\", type=\"str\"),  # 👈 加提示\n",
    "        MetadataInfo(name=\"section_title\", description=\"文档部分标题\", type=\"str\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ 构建支持自动元数据过滤的 Retriever\n",
    "retriever = VectorIndexAutoRetriever(\n",
    "    index=index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 构造查询引擎并提问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: 风险因素\n",
      "Using filters: [('company_name', '==', 'FutureTech'), ('report_quarter', '==', '2025 Q1')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**查询问题:** 请找一下 FutureTech 公司在 2025 年 Q1 报告中的风险因素"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**模型回答:** FutureTech公司在2025年第一季度报告中指出的主要风险是供应链中断问题。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever)\n",
    "\n",
    "query = \"请找一下 FutureTech 公司在 2025 年 Q1 报告中的风险因素\"\n",
    "\n",
    "response = query_engine.query(query)\n",
    "# 展示结果\n",
    "display(Markdown(f\"**查询问题:** {query}\"))\n",
    "display(Markdown(f\"**模型回答:** {response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 查看每个 Node 的元数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 文本 ---\n",
      "在 2025 年 Q1，FutureTech 报告中指出最大风险是供应链中断。\n",
      "元数据: {'company_name': 'FutureTech', 'report_quarter': '2025 Q1', 'section_title': 'Risk Factors'}\n",
      "--- 文本 ---\n",
      "2025 Q1 的市场扩展策略是 FutureTech 成长计划的重点。\n",
      "元数据: {'company_name': 'FutureTech', 'report_quarter': '2025 Q1', 'section_title': 'Strategy'}\n",
      "--- 文本 ---\n",
      "2024 Q4，FutureTech 报告关注的是成本控制。\n",
      "元数据: {'company_name': 'FutureTech', 'report_quarter': '2024 Q4', 'section_title': 'Risk Factors'}\n"
     ]
    }
   ],
   "source": [
    "for node in index.docstore.docs.values():\n",
    "    print(\"--- 文本 ---\")\n",
    "    print(node.text)\n",
    "    print(\"元数据:\", node.metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重排（Re-ranking）的“精加工”\n",
    "检索器（Retriever）的首要任务是**保召回（Recall）**，即宁可错杀一千，不可放过一个，尽可能把所有可能相关的文档都找回来。但这也会导致结果中包含一些“噪音”。\n",
    "\n",
    "重排器（Node Postprocessor）的任务则是**提精度（Precision）**。它接收检索器返回的初步结果列表，然后对其进行重新排序，将最最相关的文档排到最前面，送给 LLM。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型选择：轻量级 Cross-Encoder vs. 强大但昂贵的 CohereRerank\n",
    "\n",
    "- **`Cross-Encoder` (交叉编码器)**：\n",
    "  - **原理**：它不像嵌入模型（双编码器）那样独立地为问题和文档计算向量。`Cross-Encoder` 会将**问题和文档拼接在一起**，作为一个整体输入到模型中，直接输出一个 0 到 1 的相关性分数。这种方式能捕捉到更细微的交互，因此通常比向量相似度更准确。\n",
    "  - **代表**: `sentence-transformers` 库中的许多模型，如 `ms-marco-MiniLM-L-6-v2`。\n",
    "  - **优缺点**: 准确性高，可本地部署。但因为每个文档都要和问题一起计算一次，所以速度较慢，不适合用于大规模召回，非常适合用于对少量（如 Top 25）候选文档进行重排。\n",
    "\n",
    "**代码示例：使用 `SentenceTransformerRerank`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c45a4146c1d4d7ead0341244cdbe23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\iSPACE\\anaconda3\\envs\\rag\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\iSPACE\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5199cf2708ba4a0d93d0355140c27891",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "046aeaeaa9444b4fa5f35bb3be830f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9074b300ca46ed8c2e8fcfc93d19d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961ac71ba55e41d697a6806fda0165ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebdbb73d19f44d380d0a9685effac08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87943765cf674108a4924200c78ccf3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.66k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 重排前的节点 ---\n",
      "得分: 7.0044, 内容: 苹果公司发布了全新的 M4 芯片，性能卓越。\n",
      "得分: 3.1983, 内容: 香蕉是一种富含钾元素的水果，有益健康。\n",
      "得分: 3.9229, 内容: 最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\n",
      "\n",
      "--- 重排后的节点 ---\n",
      "得分: 7.0044, 内容: 苹果公司发布了全新的 M4 芯片，性能卓越。\n",
      "得分: 3.9229, 内容: 最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\n"
     ]
    }
   ],
   "source": [
    "# 安装依赖\n",
    "# !pip install sentence-transformers llama-index-postprocessor-sbert-rerank\n",
    "\n",
    "from llama_index.core import QueryBundle, Document\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "from llama_index.postprocessor.sbert_rerank import SentenceTransformerRerank\n",
    "\n",
    "# 假设我们已经从某个检索器获取了初步的候选节点列表\n",
    "# 为了演示，我们手动创建一些 NodeWithScore 对象\n",
    "candidate_nodes = [\n",
    "    NodeWithScore(node=Document(text=\"苹果公司发布了全新的 M4 芯片，性能卓越。\"), score=0.8),\n",
    "    NodeWithScore(node=Document(text=\"香蕉是一种富含钾元素的水果，有益健康。\"), score=0.7),\n",
    "    NodeWithScore(node=Document(text=\"最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\"), score=0.9),\n",
    "]\n",
    "\n",
    "query = \"关于苹果公司的 M4 芯片，有什么信息？\"\n",
    "\n",
    "# 初始化重排器\n",
    "# top_n=2 表示重排后只保留最相关的 2 个节点\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\", top_n=2, device=\"cpu\"  # 或 \"cuda\" 如果你有GPU\n",
    ")\n",
    "\n",
    "# 执行重排\n",
    "reranked_nodes = reranker.postprocess_nodes(\n",
    "    candidate_nodes, query_bundle=QueryBundle(query)\n",
    ")\n",
    "\n",
    "print(\"--- 重排前的节点 ---\")\n",
    "for node in candidate_nodes:\n",
    "    print(f\"得分: {node.score:.4f}, 内容: {node.get_content()}\")\n",
    "\n",
    "print(\"\\n--- 重排后的节点 ---\")\n",
    "for node in reranked_nodes:\n",
    "    print(f\"得分: {node.score:.4f}, 内容: {node.get_content()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`CohereRerank`**：\n",
    "  - **原理**：[Cohere](https://dashboard.cohere.com/api-keys) 提供的商业化重排 API，背后是强大的专用重排模型。\n",
    "  - **优缺点**：效果极好，尤其对多语言支持出色，使用简单。但需要 API Key 且付费。\n",
    "\n",
    "**代码示例：使用 `CohereRerank`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # pip install llama-index-postprocessor-cohere-rerank\n",
    "\n",
    "# from llama_index.core import QueryBundle, Document\n",
    "# from llama_index.core.schema import NodeWithScore\n",
    "# from llama_index.postprocessor.cohere_rerank import CohereRerank\n",
    "\n",
    "# # 先构造候选节点列表（示例）\n",
    "# candidate_nodes = [\n",
    "#     NodeWithScore(node=Document(text=\"苹果公司发布了全新的 M4 芯片，性能卓越。\"), score=0.8),\n",
    "#     NodeWithScore(node=Document(text=\"香蕉是一种富含钾元素的水果，有益健康。\"), score=0.7),\n",
    "#     NodeWithScore(node=Document(text=\"最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\"), score=0.9),\n",
    "# ]\n",
    "\n",
    "# query = \"关于苹果公司的 M4 芯片，有什么信息？\"\n",
    "\n",
    "# # 初始化 CohereRerank，需要设置 api_key 和 top_n\n",
    "# reranker = CohereRerank(api_key=\"你的 COHERE_API_KEY\", top_n=2)\n",
    "\n",
    "# # 执行重排\n",
    "# reranked_nodes = reranker.postprocess_nodes(\n",
    "#     candidate_nodes, query_bundle=QueryBundle(query)\n",
    "# )\n",
    "\n",
    "# print(\"--- 重排后的节点 ---\")\n",
    "# for node in reranked_nodes:\n",
    "#     print(f\"得分: {node.score:.4f}, 内容: {node.get_content()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM as a Re-ranker：基于大模型的成对（Pairwise）与列表（Listwise）重排实现\n",
    "\n",
    "当准确性要求达到极致时，我们可以直接让一个强大的 LLM（如 GPT-4）来扮演重排器的角色。LlamaIndex 提供了 `LLMRerank` 后处理器，它会将查询和所有候选文档的信息打包，然后向 LLM 提出一个特殊的问题，例如：“请根据以下问题，对下列文档按相关性从高到低排序，并给出你的理由。”\n",
    "\n",
    "- **优缺点**：准确性理论上最高，能理解非常复杂的语义关系。但成本最高，延迟最大，通常只用于对少数几个最关键的候选文档做最终定夺。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 重排前的节点 ---\n",
      "得分: 0.8000, 内容: 苹果公司发布了全新的 M4 芯片，性能卓越。\n",
      "得分: 0.7000, 内容: 香蕉是一种富含钾元素的水果，有益健康。\n",
      "得分: 0.9000, 内容: 最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\n",
      "\n",
      "--- 重排后的节点 ---\n",
      "得分: 10.0000, 内容: 苹果公司发布了全新的 M4 芯片，性能卓越。\n",
      "得分: 8.0000, 内容: 最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import QueryBundle, Document\n",
    "from llama_index.core.postprocessor import LLMRerank\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# 准备你的查询和候选节点\n",
    "query = \"关于苹果公司的 M4 芯片，有什么信息？\"\n",
    "candidate_nodes = [\n",
    "    NodeWithScore(node=Document(text=\"苹果公司发布了全新的 M4 芯片，性能卓越。\"), score=0.8),\n",
    "    NodeWithScore(node=Document(text=\"香蕉是一种富含钾元素的水果，有益健康。\"), score=0.7),\n",
    "    NodeWithScore(node=Document(text=\"最新款的 MacBook Pro 笔记本电脑搭载了 M4 芯片。\"), score=0.9),\n",
    "]\n",
    "\n",
    "# 初始化 LLMRerank，指定使用的 LLM\n",
    "reranker = LLMRerank(top_n=2)\n",
    "\n",
    "# 执行重排\n",
    "reranked_nodes = reranker.postprocess_nodes(candidate_nodes, query_bundle=QueryBundle(query))\n",
    "\n",
    "print(\"--- 重排前的节点 ---\")\n",
    "for node in candidate_nodes:\n",
    "    print(f\"得分: {node.score:.4f}, 内容: {node.get_content()}\")\n",
    "\n",
    "print(\"\\n--- 重排后的节点 ---\")\n",
    "for node in reranked_nodes:\n",
    "    print(f\"得分: {node.score:.4f}, 内容: {node.get_content()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多样性与相关性的平衡：Maximal Marginal Relevance (MMR) 的高级调参技巧\n",
    "检索结果常常会面临一个问题：最相关的几个文档可能内容高度同质化，都在重复同一个观点。这会浪费宝贵的上下文窗口。\n",
    "\n",
    "**MMR (Maximal Marginal Relevance, 最大边际相关性)** 算法就是为了解决这个问题。\n",
    "\n",
    "- **原理**: 它在选择下一个文档时，不仅考虑该文档与 **查询（Query）** 的相关性，还考虑它与 **已选文档（Selected Docs）** 的相似性。它试图找到一个平衡点，选出的文档既要和查询相关，又要和已经选出来的文档不那么相似，从而保证上下文的多样性。\n",
    "- **使用**: 在 LlamaIndex 中，许多向量索引的 `as_retriever` 方法都支持 `vector_store_query_mode=\"mmr\"`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "苹果公司发布了全新的 M4 芯片，性能卓越，支持更高的能效。\n",
      "最新款 MacBook Pro 搭载了苹果的 M4 芯片，提升了图形处理能力。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "\n",
    "# 假设已经有一些文档\n",
    "documents = [\n",
    "    Document(text=\"苹果公司发布了全新的 M4 芯片，性能卓越，支持更高的能效。\"),\n",
    "    Document(text=\"最新款 MacBook Pro 搭载了苹果的 M4 芯片，提升了图形处理能力。\"),\n",
    "    Document(text=\"香蕉是一种富含钾元素的水果，有益健康。\"),\n",
    "    Document(text=\"市场上出现了多款高性能笔记本电脑，搭载不同品牌芯片。\"),\n",
    "]\n",
    "\n",
    "# 构建向量索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 获取检索器，启用 MMR 模式\n",
    "retriever = index.as_retriever(vector_store_query_mode=\"mmr\")\n",
    "\n",
    "# 使用检索器查询\n",
    "query = \"关于苹果公司的 M4 芯片，有什么信息？\"\n",
    "response = retriever.retrieve(query)\n",
    "\n",
    "# 输出结果\n",
    "for doc in response:\n",
    "    print(doc.get_content())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果融合（Fusion）的“集大成”\n",
    "当你使用了混合检索，或者同时查询了多个数据源，就会得到多个不同的、排好序的结果列表。如何将它们智能地合并成一个最终的权威列表？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reciprocal Rank Fusion (RRF)：不依赖分数的智能结果聚合算法详解\n",
    "\n",
    "不同的检索器返回的分数，其尺度和含义完全不同（BM25 的分数和向量余弦相似度无法直接比较）。RRF 算法的聪明之处在于，它**完全忽略分数，只看排名**。\n",
    "\n",
    "- **原理**: 对于一个文档，它在每个结果列表中的 RRF 得分是 `1 / (k + rank)`。`rank` 是它在该列表中的排名（从 1 开始），`k` 是一个常数（LlamaIndex 默认为 60），用于降低低排名文档的影响。最后，将一个文档在所有列表中的 RRF 得分相加，得到其最终的总分，按总分重新排序。\n",
    "- **优势**: 极其简单、高效，且效果惊人。因为它奖励那些在**多个不同信息源中都排名靠前**的文档。\n",
    "- **实现**: 前文提到的 `QueryFusionRetriever` 默认就使用了 RRF 算法进行融合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询: 苹果公司的 M4 芯片信息\n",
      "\n",
      "文档1内容:\n",
      "苹果公司发布了全新的 M4 芯片，性能卓越。\n",
      "\n",
      "文档2内容:\n",
      "最新款 MacBook Pro 搭载了苹果的 M4 芯片。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleKeywordTableIndex, Document\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "# 1. 构造文档\n",
    "documents = [\n",
    "    Document(text=\"苹果公司发布了全新的 M4 芯片，性能卓越。\"),\n",
    "    Document(text=\"最新款 MacBook Pro 搭载了苹果的 M4 芯片。\"),\n",
    "    Document(text=\"香蕉是一种富含钾元素的水果，有益健康。\"),\n",
    "]\n",
    "\n",
    "# 2. 构建两个不同类型的索引\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "keyword_index = SimpleKeywordTableIndex.from_documents(documents)\n",
    "\n",
    "# 3. 创建各自的检索器\n",
    "vector_retriever = vector_index.as_retriever()\n",
    "keyword_retriever = keyword_index.as_retriever()\n",
    "\n",
    "# 4. 用 QueryFusionRetriever 聚合多个检索器\n",
    "fusion_retriever = QueryFusionRetriever(retrievers=[vector_retriever, keyword_retriever])\n",
    "\n",
    "# 5. 查询\n",
    "query = \"苹果公司的 M4 芯片信息\"\n",
    "results = fusion_retriever.retrieve(query)\n",
    "\n",
    "# 6. 输出融合后的结果\n",
    "print(f\"查询: {query}\\n\")\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"文档{i}内容:\\n{doc.get_content()}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分数归一化与自定义融合\n",
    "\n",
    "如果你的确想利用不同检索器的分数，那么在融合前必须进行**分数归一化（Normalization）**，例如将所有分数都映射到 0-1 区间。之后，你可以实现自己的**自定义融合策略**，比如对不同检索器的归一化分数进行加权求和，给予你认为更可靠的检索器更高的权重。这是一个更复杂、需要更多实验的领域，但 LlamaIndex 的可组合性也完全支持你这样做。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "查询问题: 未来科技公司2025年第一季度风险因素\n",
      "\n",
      "融合检索结果（按加权归一化分数排序）：\n",
      "得分: 0.6000, 文本: 未来科技公司在2025年第一季度的风险因素主要是供应链不稳定。\n",
      "得分: 0.4000, 文本: 2025年第一季度，未来科技公司发布了新产品。\n",
      "得分: 0.0000, 文本: 2025年Q1，未来科技公司加大了研发投入。\n",
      "得分: 0.0000, 文本: 风险因素包括市场竞争激烈和法规变化。\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core import GPTVectorStoreIndex, Document\n",
    "from llama_index.core.base.base_retriever import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "\n",
    "class NormalizedWeightedFusionRetriever(BaseRetriever):\n",
    "    def __init__(self, retrievers: List[BaseRetriever], weights: List[float]):\n",
    "        super().__init__()\n",
    "        assert len(retrievers) == len(weights), \"检索器和权重数量必须一致\"\n",
    "        self.retrievers = retrievers\n",
    "        self.weights = weights\n",
    "\n",
    "    def _retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        **kwargs\n",
    "    ) -> List[NodeWithScore]:\n",
    "        all_results = []\n",
    "        for retriever, weight in zip(self.retrievers, self.weights):\n",
    "            # 调用公开的 retrieve() 方法，不传 filters\n",
    "            results = retriever.retrieve(query, **kwargs)\n",
    "            scores = [res.score for res in results]\n",
    "            if not scores:\n",
    "                continue\n",
    "            max_score = max(scores)\n",
    "            min_score = min(scores)\n",
    "            norm_scores = [\n",
    "                (s - min_score) / (max_score - min_score) if max_score > min_score else 1.0\n",
    "                for s in scores\n",
    "            ]\n",
    "            weighted_results = []\n",
    "            for norm_score, res in zip(norm_scores, results):\n",
    "                weighted_score = norm_score * weight\n",
    "                weighted_results.append(NodeWithScore(node=res.node, score=weighted_score))\n",
    "            all_results.extend(weighted_results)\n",
    "\n",
    "        # 去重\n",
    "        seen_texts = set()\n",
    "        unique_results = []\n",
    "        for item in sorted(all_results, key=lambda x: x.score, reverse=True):\n",
    "            content = item.node.get_content()\n",
    "            if content not in seen_texts:\n",
    "                unique_results.append(item)\n",
    "                seen_texts.add(content)\n",
    "\n",
    "        return unique_results\n",
    "\n",
    "\n",
    "\n",
    "# 准备两个文档集合（示例）\n",
    "documents1 = [\n",
    "    Document(text=\"未来科技公司在2025年第一季度的风险因素主要是供应链不稳定。\"),\n",
    "    Document(text=\"2025年Q1，未来科技公司加大了研发投入。\"),\n",
    "]\n",
    "\n",
    "documents2 = [\n",
    "    Document(text=\"2025年第一季度，未来科技公司发布了新产品。\"),\n",
    "    Document(text=\"风险因素包括市场竞争激烈和法规变化。\"),\n",
    "]\n",
    "\n",
    "# 建立两个索引\n",
    "index1 = GPTVectorStoreIndex.from_documents(documents1)\n",
    "index2 = GPTVectorStoreIndex.from_documents(documents2)\n",
    "\n",
    "# 获取对应的检索器\n",
    "retriever1 = index1.as_retriever()\n",
    "retriever2 = index2.as_retriever()\n",
    "\n",
    "# 构造加权融合检索器\n",
    "fusion_retriever = NormalizedWeightedFusionRetriever(\n",
    "    retrievers=[retriever1, retriever2],\n",
    "    weights=[0.6, 0.4]  # 给retriever1更高权重\n",
    ")\n",
    "\n",
    "# 执行查询\n",
    "query = \"未来科技公司2025年第一季度风险因素\"\n",
    "results = fusion_retriever.retrieve(query)\n",
    "\n",
    "print(f\"查询问题: {query}\")\n",
    "print(\"\\n融合检索结果（按加权归一化分数排序）：\")\n",
    "for res in results:\n",
    "    print(f\"得分: {res.score:.4f}, 文本: {res.node.get_content()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本章小结**\n",
    "\n",
    "我们已经为 RAG 系统装配了一套强大的“武器库”。通过“组合拳”式的检索策略，我们保证了召回的广度；通过“精加工”式的重排，我们提升了排序的精度；通过“集大成”式的融合，我们能应对多个信息源。至此，我们的 RAG 系统已经具备了从海量知识中精准提取高质量上下文的能力。\n",
    "\n",
    "在下一部分“高级应用篇”中，我们将探索如何让系统变得更“聪明”，学会主动重构查询、编排复杂工作流，并最终演化为拥有自主能力的智能体（Agent）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三部分：高级应用篇 - 查询、推理与 Agentic 工作流 (Advanced Applications)\n",
    "\n",
    "欢迎来到本宝典的“中枢神经系统”篇。至此，我们已经拥有了一个强大的“信息核心”——一个经过精心解析、分块、嵌入和优化的知识索引。然而，一个只会被动等待检索的知识库，其价值是有限的。本章的使命，是为这个强大的核心装上一个聪明的“大脑”，让它学会**主动思考、规划、推理**，并**编排复杂的工作流**，最终演化为一个能自主解决问题的**智能体（Agent）**。\n",
    "\n",
    "我们将探索三个核心主题：\n",
    "\n",
    "1. **查询重构（Query Rewriting）**：在接触知识库之前，如何让系统先“优化”用户提出的问题？\n",
    "2. **编排的艺术（Orchestration）**：如何像导演一样，设计和控制信息在各个组件之间的流动路径？\n",
    "3. **构建智能体（Agents）**：如何从简单的问答，迈向能够使用工具、协同工作的终极形态？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查询重构（Query Rewriting）的“魔法”\n",
    "用户的原始问题，往往不是最适合直接用来检索的。它可能过于模糊、过于复杂，或者与文档中的措辞存在偏差。查询重构，就是在执行检索**之前**，利用 LLM 对原始问题进行“预处理”和“增强”的一系列魔法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提升相关性：HyDE（假设性文档嵌入）与 Step-Back Prompting 的实战\n",
    "- **HyDE (Hypothetical Document Embeddings，假设性文档嵌入)**\n",
    "  - **核心思想**：直接对用户的简短问题进行嵌入，可能难以匹配到内容详尽的文档。HyDE 的做法是，先让 LLM 根据用户问题，**凭空想象并生成一篇“假设性的”答案文档**。然后，我们不对原始问题进行嵌入，而是对这篇更丰富、更详细的假设性文档进行嵌入，再用其向量去检索真实文档。因为假设的答案在措辞和内容上与真实答案更可能相似，所以这种方式往往能找到更相关的结果。\n",
    "  - **LlamaIndex 实现**: 通过 `HyDEQueryTransform` 实现。\n",
    "\n",
    "**代码示例：使用 `HyDEQueryTransform`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始问题: 句窗解析器中的窗口大小参数是做什么用的？\n",
      "\n",
      "HyDE 引擎的回答:\n",
      "窗口大小参数用于控制在检索某个句子时，同时包含其前后相邻句子的数量。这样可以为语言模型提供更完整的上下文信息，帮助准确理解核心句子的含义，避免孤立解读的情况。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    Document,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "\n",
    "# 1. 准备数据\n",
    "documents = [\n",
    "    Document(\n",
    "        text=(\n",
    "            \"LlamaIndex的句窗节点解析器（SentenceWindowNodeParser）通过一个'window_size'参数来控制上下文。\"\n",
    "            \"当一个句子被检索到时，它会额外包含该句子前后各'window_size'个句子，\"\n",
    "            \"这为LLM提供了更丰富的语境来理解核心句子的含义，从而防止断章取义。\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 2. 创建一个基础的查询引擎\n",
    "base_query_engine = index.as_query_engine()\n",
    "\n",
    "# 3. 创建 HyDE 转换器\n",
    "hyde_transform = HyDEQueryTransform(include_original=True, llm=Settings.llm)\n",
    "\n",
    "# 4. 用 TransformQueryEngine 包装基础引擎和转换器\n",
    "hyde_query_engine = TransformQueryEngine(base_query_engine, hyde_transform)\n",
    "\n",
    "# 5. 提问\n",
    "query = \"句窗解析器中的窗口大小参数是做什么用的？\"\n",
    "print(f\"原始问题: {query}\\n\")\n",
    "\n",
    "# hyde_transform.run(\"句窗解析器中的窗口大小参数是做什么用的？\").transformed_query\n",
    "# 上面这行代码可以让你看到 LLM 生成的假设性文档\n",
    "\n",
    "response = hyde_query_engine.query(query)\n",
    "print(f\"HyDE 引擎的回答:\\n{response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Step-Back Prompting (溯源式提示)**\n",
    "  - **核心思想**：当面对一个非常具体的问题时（例如：“LlamaIndex 中的 `SentenceWindowNodeParser` 的 `window_size` 参数有什么作用？”），直接检索可能找不到一个文档刚好只讲这一点。Step-Back 的策略是，让 LLM 先“退一步”，从具体问题中提炼出一个更**高层次、更泛化**的核心概念（例如：“LlamaIndex 中如何处理文档分块以保留上下文？”）。然后，我们用这个泛化后的问题去检索，找到包含核心概念的文档。最后，将检索到的文档和**原始的具体问题**一起提供给 LLM，让它在丰富的上下文中找到具体答案。\n",
    "1. LlamaIndex 提供了 DecomposeQueryTransform 类，专门用于将复杂问题分解为更简单的子问题，适用于 Step-Back Prompting 的场景。您可以使用该类来实现问题的泛化处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;33m> Current query: SentenceWindowNodeParser 中 window_size 参数的作用是什么？\n",
      "\u001b[0m\u001b[1;3;38;5;200m> New query: 由于提供的知识源上下文为空（\"None\"），无法基于上下文生成新的子问题或修改原问题。因此，建议保持原问题不变，或补充相关上下文后再尝试重构问题。\n",
      "\n",
      "保持原问题：  \n",
      "**SentenceWindowNodeParser 中 window_size 参数的作用是什么？**  \n",
      "\n",
      "若需进一步帮助，请提供与 `SentenceWindowNodeParser` 或 `window_size` 参数相关的上下文信息。例如：  \n",
      "- 该类的功能或应用场景  \n",
      "- 相关代码片段或文档说明  \n",
      "- 参数的具体定义或取值范围  \n",
      "\n",
      "有了上下文后，可尝试生成更具体的问题，例如：  \n",
      "- *如何通过 `window_size` 控制句子窗口的上下文范围？*  \n",
      "- *`window_size` 的默认值是多少？*\n",
      "\u001b[0m\n",
      "🎯 最终回答：\n",
      "The `window_size` parameter in `SentenceWindowNodeParser` determines the number of surrounding sentences to include before and after a given sentence, helping to maintain contextual information for improved language model comprehension. This parameter allows control over the contextual scope when processing text segments.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.core.indices.query.query_transform import DecomposeQueryTransform\n",
    "\n",
    "# 文档数据\n",
    "documents = [\n",
    "    Document(\n",
    "        text=\"LlamaIndex 的 SentenceWindowNodeParser 使用 window_size 参数控制前后句子范围，\"\n",
    "             \"以便保留上下文并提高语言模型理解的准确性。\"\n",
    "    ),\n",
    "    Document(\n",
    "        text=\"LlamaIndex 允许多种分块策略，如按句子、按段落、按 token。\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# 向量索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "base_query_engine = index.as_query_engine()\n",
    "\n",
    "# 定义 Step-Back Prompting 的查询转换逻辑\n",
    "decompose_transform = DecomposeQueryTransform(llm=Settings.llm, verbose=True)\n",
    "\n",
    "# 使用 TransformQueryEngine\n",
    "stepback_query_engine = TransformQueryEngine(\n",
    "    base_query_engine, query_transform=decompose_transform\n",
    ")\n",
    "\n",
    "# 提问\n",
    "query = \"SentenceWindowNodeParser 中 window_size 参数的作用是什么？\"\n",
    "response = stepback_query_engine.query(query)\n",
    "\n",
    "# 显示结果\n",
    "print(\"\\n🎯 最终回答：\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 如果希望更灵活地控制问题的处理逻辑，可以自定义一个继承自 BaseQueryTransform 的类，重写其中的 run 方法来实现 Step-Back Prompting。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 原始问题： SentenceWindowNodeParser 的 window_size 参数是做什么用的？\n",
      ">>> 泛化问题： 抽象问题：某个类或方法的特定参数（如 window_size）的功能和作用是什么？\n",
      "\n",
      "或者更通用地：  \n",
      "如何理解 [特定类/方法] 中 [参数名称] 参数的具体用途和影响？  \n",
      "\n",
      "这种抽象形式适用于任何技术文档或API查询场景，核心是提取“参数功能解释”这一通用需求。\n",
      "\n",
      ">>> 最终回答：\n",
      "在技术实现中，特定参数（如window_size）通常用于控制功能模块的处理范围或上下文边界。这类数值型参数通过限定操作窗口的尺寸，直接影响系统处理数据的完整性和连贯性，避免信息碎片化。其核心作用是平衡精确度与上下文覆盖范围，确保后续处理阶段能基于足够但不过量的上下文信息进行决策。具体实现中，该参数的取值需要根据实际场景在信息完整性和计算效率之间进行权衡调整。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, Settings\n",
    "from llama_index.core.query_engine import TransformQueryEngine\n",
    "from llama_index.core.indices.query.query_transform.base import BaseQueryTransform\n",
    "from llama_index.core.base.llms.base import LLMMetadata\n",
    "\n",
    "\n",
    "# 自定义 Step-Back Transform\n",
    "class StepBackQueryTransform(BaseQueryTransform):\n",
    "    def __init__(self, llm, verbose=False):\n",
    "        self._llm = llm\n",
    "        self._verbose = verbose\n",
    "\n",
    "    def _run(self, query_str: str, metadata: LLMMetadata = None) -> str:\n",
    "        \"\"\"主逻辑：用 LLM 将具体问题转换为更抽象的问题\"\"\"\n",
    "        prompt = (\n",
    "            f\"请将以下具体问题抽象成一个更通用的问题，以便进行信息检索：\\n\"\n",
    "            f\"具体问题：{query_str}\\n\"\n",
    "            f\"抽象问题：\"\n",
    "        )\n",
    "        response = self._llm.complete(prompt).text.strip()\n",
    "        if self._verbose:\n",
    "            print(\">>> 原始问题：\", query_str)\n",
    "            print(\">>> 泛化问题：\", response)\n",
    "        return response\n",
    "\n",
    "    def _get_prompts(self) -> dict:\n",
    "        return {}\n",
    "\n",
    "    def _update_prompts(self, prompts_dict: dict) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    Document(\n",
    "        text=(\n",
    "            \"LlamaIndex 的 SentenceWindowNodeParser 使用 window_size 参数控制检索时包含的上下文句子数量，\"\n",
    "            \"可以防止断章取义，增强模型的上下文理解能力。\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "\n",
    "# 构建索引\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 替换为你自己的 LLM，如果你用的是 DeepSeek 或其他自定义 LLM，请确保它兼容 LlamaIndex\n",
    "# from llama_index.llms.openai import OpenAI\n",
    "# Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# 创建 Step-Back 查询引擎\n",
    "stepback_transform = StepBackQueryTransform(llm=Settings.llm, verbose=True)\n",
    "stepback_query_engine = TransformQueryEngine(\n",
    "    query_engine=index.as_query_engine(),\n",
    "    query_transform=stepback_transform\n",
    ")\n",
    "\n",
    "# 查询示例\n",
    "query = \"SentenceWindowNodeParser 的 window_size 参数是做什么用的？\"\n",
    "response = stepback_query_engine.query(query)\n",
    "\n",
    "print(\"\\n>>> 最终回答：\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应对复杂问题：利用 LLM 将复杂问题自动分解为子问题（Sub-Questions）\n",
    "\n",
    "当一个问题需要从多个不同来源或文档的不同部分获取信息才能回答时（例如，“请比较 BM25 和向量检索的优缺点”），最好的方法是将其分解。\n",
    "\n",
    "- **`SubQuestionQueryEngine`**: 这个引擎是解决这类问题的利器。\n",
    "  - **工作原理**: 它首先接收一个复杂问题，然后让 LLM 将其分解成多个更简单的**子问题**。接着，它会依次执行每个子问题（可以路由到不同的检索引擎），收集每个子问题的答案。最后，它将所有子问题的答案汇总起来，交给 LLM 进行最终的综合，形成一个条理清晰、内容全面的最终答案。\n",
    "  - **适用场景**：对比分析、多角度总结、需要整合多个知识点的查询。\n",
    "\n",
    "**代码示例：使用 SubQuestionQueryEngine 进行复杂问题分解**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 4 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[bm25] Q: BM25检索的优点是什么\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[bm25] Q: BM25检索的缺点是什么\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[vector] Q: 向量检索的优点是什么\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[vector] Q: 向量检索的缺点是什么\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[bm25] A: BM25算法存在几个主要局限性：首先，它对文档长度归一化的处理方式可能在某些场景下不够精确，特别是当文档长度差异极大时。其次，该算法主要依赖词频统计，难以有效捕捉词语间的语义关系。此外，BM25作为词袋模型，无法处理词语顺序和短语匹配的问题。最后，其参数需要针对不同数据集进行手动调整，缺乏自适应能力。\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[vector] A: 向量检索的主要优势在于能够有效识别和匹配语义层面的相似性，这使得它在处理大规模数据时表现出色。通过将复杂的信息转化为向量形式，可以快速找到内容相关但表述不同的项目，提升搜索效率和准确性。此外，这种方法适用于多种数据类型，包括文本、图像和音频等，具有较好的通用性和扩展性。\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[vector] A: 向量检索虽然能有效捕捉语义相似度，但也存在一些局限性。主要包括：对高维数据的处理效率可能下降，计算和存储成本较高；检索效果受向量质量影响较大，依赖训练数据的覆盖度和质量；缺乏可解释性，难以直观理解相似度计算过程；此外，在数据分布不均匀时可能出现检索偏差。\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[bm25] A: BM25检索算法具有以下几个主要优点：\n",
      "\n",
      "1. 考虑了词频因素，能够更好地区分高频词和低频词的重要性差异\n",
      "2. 引入了文档长度归一化处理，有效解决了长文档与短文档的公平性问题\n",
      "3. 通过参数调节可以适应不同领域和类型的文档集合\n",
      "4. 在计算相关性时同时考虑了查询词项在文档中的频率和在整个集合中的分布情况\n",
      "5. 相比传统TF-IDF方法具有更好的检索效果和排序质量\n",
      "6. 计算效率较高，适合大规模文档集合的检索需求\n",
      "\u001b[0mBM25和向量检索各有其特点，适用于不同的场景和需求。以下是它们的优缺点比较：\n",
      "\n",
      "### BM25的优点：\n",
      "1. 能够区分高频词和低频词的重要性，提升检索准确性。\n",
      "2. 通过文档长度归一化处理，平衡长文档和短文档的检索公平性。\n",
      "3. 参数可调，适应不同领域和类型的文档集合。\n",
      "4. 计算效率高，适合大规模文档检索。\n",
      "\n",
      "### BM25的缺点：\n",
      "1. 文档长度归一化处理在极端情况下可能不够精确。\n",
      "2. 依赖词频统计，难以捕捉语义关系。\n",
      "3. 无法处理词语顺序和短语匹配问题。\n",
      "4. 参数需要手动调整，缺乏自适应能力。\n",
      "\n",
      "### 向量检索的优点：\n",
      "1. 能够识别语义相似性，适用于内容相关但表述不同的项目。\n",
      "2. 适用于多种数据类型（如文本、图像、音频），通用性强。\n",
      "3. 在大规模数据中表现高效，提升搜索准确性和效率。\n",
      "\n",
      "### 向量检索的缺点：\n",
      "1. 高维数据处理效率可能下降，计算和存储成本较高。\n",
      "2. 检索效果依赖向量质量，受训练数据覆盖度和质量影响较大。\n",
      "3. 缺乏可解释性，难以直观理解相似度计算过程。\n",
      "4. 数据分布不均匀时可能出现检索偏差。\n",
      "\n",
      "### 总结：\n",
      "- **BM25**更适合传统文本检索场景，尤其是对词频和文档长度敏感的任务，计算效率高但语义理解能力有限。  \n",
      "- **向量检索**在语义匹配和多模态数据中表现更优，但计算成本较高且依赖数据质量。  \n",
      "\n",
      "选择哪种方法需根据具体需求，如注重效率和词频统计可优先BM25，而需要语义理解或多模态检索则向量检索更合适。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import ToolMetadata\n",
    "from llama_index.core.tools.query_engine import QueryEngineTool\n",
    "\n",
    "\n",
    "# 准备文档和索引\n",
    "docs_bm25 = [Document(text=\"BM25 是一种基于词频和文档长度的检索算法...\")]\n",
    "docs_vec = [Document(text=\"向量检索可以捕捉语义相似度...\")]\n",
    "\n",
    "index_bm25 = VectorStoreIndex.from_documents(docs_bm25)\n",
    "index_vec = VectorStoreIndex.from_documents(docs_vec)\n",
    "\n",
    "\n",
    "# 定义工具的元数据\n",
    "metadata_bm25 = ToolMetadata(name=\"bm25\", description=\"基于词频的检索引擎\")\n",
    "metadata_vec = ToolMetadata(name=\"vector\", description=\"基于向量的语义检索引擎\")\n",
    "\n",
    "# 构造不同的查询引擎工具\n",
    "tool_bm25 = QueryEngineTool(\n",
    "    query_engine=index_bm25.as_query_engine(),\n",
    "    metadata=metadata_bm25\n",
    ")\n",
    "\n",
    "tool_vec = QueryEngineTool(\n",
    "    query_engine=index_vec.as_query_engine(),\n",
    "    metadata=metadata_vec\n",
    ")\n",
    "\n",
    "# 创建 SubQuestionQueryEngine\n",
    "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=[tool_bm25, tool_vec],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 复杂问题\n",
    "complex_question = \"请比较 BM25 和向量检索的优缺点\"\n",
    "\n",
    "# 查询\n",
    "response = sub_question_engine.query(complex_question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多视角查询：RAG-Fusion 背后的多查询生成与结果融合策略\n",
    "这与 `QueryFusionRetriever` 的工作原理一致，我们在第二部分已经详细介绍过。其核心思想是，让 LLM 从不同角度生成多个原始查询的**变体**，对每个变体都执行检索，最后用 RRF 等算法智能地融合所有检索结果。这种“从多个方向包抄”的策略，能极大地提高召回率和对模糊查询的鲁棒性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NodeWithScore(node=TextNode(id_='68850725-4ac9-4fa9-b426-855c5132fbd1', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='0365f38e-505c-4df4-b777-f5d3624c57a9', node_type='4', metadata={}, hash='77ff2fe78ad38b169649618e99f1362f0b461aa23571a8e8065e063a284791f1')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='BM25 适合关键词匹配，向量检索更适合模糊和语义匹配。', mimetype='text/plain', start_char_idx=0, end_char_idx=28, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.13333333333333333), NodeWithScore(node=TextNode(id_='68c9d888-a0fb-46ac-9fb9-f49584e51899', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='4cef0e51-4d26-428e-b893-7877a61ec825', node_type='4', metadata={}, hash='726d349a9d219d556d4a9d6bfe7a8efc9204e14829eceddcaf8b09d4c8bd6eb2')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='BM25 是一种基于词频和逆文档频率的经典检索算法，优点是计算高效且效果稳定。', mimetype='text/plain', start_char_idx=0, end_char_idx=39, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.0819672131147541), NodeWithScore(node=TextNode(id_='1235289e-b357-4832-aac7-ca60f82be9b2', embedding=None, metadata={}, excluded_embed_metadata_keys=[], excluded_llm_metadata_keys=[], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text='向量检索使用语义嵌入来表示文本，能够捕捉深层语义信息，但计算成本较高。', mimetype='text/plain', start_char_idx=None, end_char_idx=None, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'), score=0.04918032786885246)]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, Document, ServiceContext\n",
    "from llama_index.core.retrievers.fusion_retriever import FUSION_MODES, QueryFusionRetriever\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "\n",
    "documents = [\n",
    "    Document(text=\"BM25 是一种基于词频和逆文档频率的经典检索算法，优点是计算高效且效果稳定。\"),\n",
    "    Document(text=\"向量检索使用语义嵌入来表示文本，能够捕捉深层语义信息，但计算成本较高。\"),\n",
    "    Document(text=\"BM25 适合关键词匹配，向量检索更适合模糊和语义匹配。\"),\n",
    "]\n",
    "\n",
    "vector_index = VectorStoreIndex.from_documents(documents)\n",
    "vector_retriever = vector_index.as_retriever()\n",
    "\n",
    "bm25_retriever = BM25Retriever(documents)\n",
    "\n",
    "retriever_list = [\n",
    "    bm25_retriever,\n",
    "    vector_retriever,\n",
    "    # 你还可以添加更多检索器，例如 keyword retriever 等\n",
    "]\n",
    "\n",
    "\n",
    "fusion_retriever = QueryFusionRetriever(\n",
    "    retrievers=retriever_list,\n",
    "    mode=FUSION_MODES.RECIPROCAL_RANK,  # 选择融合模式\n",
    "    num_queries=4,  # 生成查询数（包含原始查询）\n",
    "    similarity_top_k=10,\n",
    "    use_async=True,\n",
    ")\n",
    "\n",
    "query = \"如何比较 BM25 和向量检索的优缺点？\"\n",
    "results = fusion_retriever.retrieve(query)\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 编排的艺术：QueryPipeline vs. QueryEngine\n",
    "\n",
    "当我们的 RAG 逻辑越来越复杂时，如何清晰、高效地组织和管理它？LlamaIndex 提供了两种不同的编排范式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QueryPipeline：构建声明式、可序列化、可视化的复杂 RAG 链路\n",
    "`QueryPipeline` 是一种更现代、更灵活的编排方式。你可以把它想象成一个**乐高积木图**。\n",
    "\n",
    "- **核心思想**：将 RAG 的每一步（如提问、检索、格式化提示词、调用 LLM）都定义成一个独立的、有明确输入和输出的**组件（Component）**。然后，你像连接水管一样，将这些组件**“连接”**起来，形成一个**有向无环图（DAG）**。\n",
    "- **优点**：\n",
    "  - **声明式与可视化**：代码即流程图，逻辑一目了然。\n",
    "  - **可组合与可复用**：任何组件或子流程都可以被轻松替换或复用。\n",
    "  - **可序列化**：可以将整个复杂的工作流保存成 JSON，方便分享和部署。\n",
    "\n",
    "**代码示例：用 `QueryPipeline` 构建一个完整的 RAG 工作流**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在设置使用 GPT-4 模型...\n",
      "索引构建完成。\n",
      "\n",
      "--- 运行 QueryPipeline ---\n",
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query: 爱因斯坦最出名的是什么？\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module retriever with input: \n",
      "input: 爱因斯坦最出名的是什么？\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module prompt_tmpl with input: \n",
      "query_str: 爱因斯坦最出名的是什么？\n",
      "context_str: 爱因斯坦因其相对论而闻名于世。\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module llm with input: \n",
      "messages: 上下文信息如下:\n",
      "---------------------\n",
      "爱因斯坦因其相对论而闻名于世。\n",
      "---------------------\n",
      "请根据上下文信息，而不是你的先验知识，回答这个问题: 爱因斯坦最出名的是什么？\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "--- 最终结果 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**模型回答:** 根据提供的上下文信息，爱因斯坦最出名的是他的**相对论**。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "# -- 2. 核心模块导入 --\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    PromptTemplate,\n",
    "    Settings\n",
    ")\n",
    "# 导入 InputComponent，用于显式定义管道的输入\n",
    "from llama_index.core.query_pipeline import QueryPipeline, InputComponent\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# -- 1. 依赖库安装 --\n",
    "# !pip install --upgrade llama-index llama-index-llms-openai\n",
    "\n",
    "# -- 3. 全局设置 --\n",
    "# 程序将依赖您在外部环境中设置的 OPENAI_API_KEY。\n",
    "print(\"正在设置使用 GPT-4 模型...\")\n",
    "# Settings.llm = OpenAI(temperature=0, model=\"gpt-4\", timeout=60.0)\n",
    "\n",
    "# -- 4. 准备数据和基础组件 --\n",
    "# 为了代码完整性，我们重新构建\n",
    "documents = [Document(text=\"爱因斯坦因其相对论而闻名于世。\")]\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"索引构建完成。\")\n",
    "\n",
    "# 1. 定义工作流中的各个组件\n",
    "retriever = index.as_retriever()\n",
    "llm = Settings.llm  # 使用全局配置的 LLM\n",
    "\n",
    "# Prompt 模板，它接收 query_str 和 context_str 两个输入\n",
    "qa_prompt_tmpl = PromptTemplate(\n",
    "    \"上下文信息如下:\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"请根据上下文信息，而不是你的先验知识，回答这个问题: {query_str}\\n\"\n",
    ")\n",
    "\n",
    "# -- 5. 【核心】定义并连接 QueryPipeline --\n",
    "\n",
    "# 2. 定义 QueryPipeline\n",
    "p = QueryPipeline(verbose=True)\n",
    "\n",
    "# 3. 将组件添加到 pipeline 中，并为它们命名\n",
    "# 添加 InputComponent 作为管道的正式入口\n",
    "p.add_modules(\n",
    "    {\n",
    "        \"input\": InputComponent(),\n",
    "        \"retriever\": retriever,\n",
    "        \"llm\": llm,\n",
    "        \"prompt_tmpl\": qa_prompt_tmpl,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 4. 定义组件之间的连接关系\n",
    "# 【最终修复】使用 src_key 和 dest_key 参数来精确定义数据流\n",
    "\n",
    "# a) 将 \"input\" 组件的 \"query\" 输出，连接到 \"retriever\" 的输入\n",
    "p.add_link(src=\"input\", dest=\"retriever\", src_key=\"query\")\n",
    "\n",
    "# b) 将 \"input\" 组件的 \"query\" 输出，连接到 \"prompt_tmpl\" 的 \"query_str\" 输入槽位\n",
    "p.add_link(src=\"input\", dest=\"prompt_tmpl\", src_key=\"query\", dest_key=\"query_str\")\n",
    "\n",
    "\n",
    "# c) 定义一个转换函数，将 retriever 的输出（节点列表）转换为字符串\n",
    "def nodes_to_str(nodes: List) -> str:\n",
    "    return \"\\n\\n\".join(str(n.node.get_content()) for n in nodes)\n",
    "\n",
    "\n",
    "# d) 将 \"retriever\" 的输出，经过转换后，连接到 \"prompt_tmpl\" 的 \"context_str\" 输入槽位\n",
    "p.add_link(\n",
    "    src=\"retriever\", dest=\"prompt_tmpl\", dest_key=\"context_str\", input_fn=nodes_to_str\n",
    ")\n",
    "\n",
    "# e) 将 \"prompt_tmpl\" 的输出连接到 \"llm\" 的输入\n",
    "p.add_link(src=\"prompt_tmpl\", dest=\"llm\")\n",
    "\n",
    "# -- 6. 运行 pipeline --\n",
    "print(\"\\n--- 运行 QueryPipeline ---\")\n",
    "# 我们在这里传入的参数名 `query`，必须和上面 add_link 中使用的 src_key \"query\" 保持一致。\n",
    "output = p.run(query=\"爱因斯坦最出名的是什么？\")\n",
    "\n",
    "print(\"\\n--- 最终结果 ---\")\n",
    "# QueryPipeline 的输出是一个 Response 对象，我们需要访问 .message.content 来获取文本\n",
    "final_response = output.message.content.strip()\n",
    "display(Markdown(f\"**模型回答:** {final_response}\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 条件路由与循环：在 QueryPipeline 中实现 if/else 和 while 逻辑\n",
    "`QueryPipeline` 的强大之处在于它支持**条件路由**，允许你构建 `if/else` 逻辑。你可以定义一个自定义函数组件，它的返回值决定了工作流的下一个走向。虽然 `QueryPipeline` 本身不支持原生循环，但可以通过 agentic 的方式，让一个组件的输出决定是否再次调用整个 pipeline，从而模拟循环。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在构建索引...\n",
      "✅ 科学知识库索引构建完成。\n",
      "✅ 通用历史知识库索引构建完成。\n",
      "\n",
      "--- 运行查询: '爱因斯坦最出名的是什么？' ---\n",
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query: 爱因斯坦最出名的是什么？\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module science_retriever with input: \n",
      "input: 爱因斯坦最出名的是什么？\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**模型回答：**\n",
       "\n",
       "【检索自: 科学文档1】\n",
       "爱因斯坦提出了狭义相对论和广义相对论。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "--- 运行查询: '文艺复兴是什么？' ---\n",
      "\u001b[1;3;38;2;155;135;227m> Running module input with input: \n",
      "query: 文艺复兴是什么？\n",
      "\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m> Running module history_retriever with input: \n",
      "input: 文艺复兴是什么？\n",
      "\n",
      "\u001b[0m"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**模型回答：**\n",
       "\n",
       "【检索自: 历史文档2】\n",
       "文艺复兴是14世纪至16世纪在欧洲发生的一场思想文化运动。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index.core import VectorStoreIndex, Document\n",
    "from llama_index.core.query_pipeline import QueryPipeline, InputComponent\n",
    "from llama_index.core.schema import NodeWithScore\n",
    "\n",
    "# -- 3. 准备数据和基础组件 --\n",
    "print(\"正在构建索引...\")\n",
    "\n",
    "science_docs = [\n",
    "    Document(text=\"爱因斯坦提出了狭义相对论和广义相对论。\", metadata={\"doc_id\": \"科学文档1\"}),\n",
    "    Document(text=\"牛顿发现了万有引力定律，并建立了经典力学的三大定律。\", metadata={\"doc_id\": \"科学文档2\"})\n",
    "]\n",
    "science_index = VectorStoreIndex.from_documents(science_docs)\n",
    "science_retriever = science_index.as_retriever(similarity_top_k=1)\n",
    "print(\"✅ 科学知识库索引构建完成。\")\n",
    "\n",
    "history_docs = [\n",
    "    Document(text=\"第二次世界大战是20世纪的一场全球性军事冲突。\", metadata={\"doc_id\": \"历史文档1\"}),\n",
    "    Document(text=\"文艺复兴是14世纪至16世纪在欧洲发生的一场思想文化运动。\", metadata={\"doc_id\": \"历史文档2\"})\n",
    "]\n",
    "history_index = VectorStoreIndex.from_documents(history_docs)\n",
    "history_retriever = history_index.as_retriever(similarity_top_k=1)\n",
    "print(\"✅ 通用历史知识库索引构建完成。\")\n",
    "\n",
    "# -- 4. 条件逻辑修复和标准化匹配 --\n",
    "def is_science_query(query_str: str) -> bool:\n",
    "    \"\"\"判断查询是否与科学有关\"\"\"\n",
    "    query_lower = query_str.lower()\n",
    "    science_keywords = [\"爱因斯坦\", \"牛顿\", \"相对论\", \"万有引力\", \"力学\"]\n",
    "    return any(keyword.lower() in query_lower for keyword in science_keywords)\n",
    "\n",
    "def nodes_to_str(nodes: List[NodeWithScore]) -> str:\n",
    "    if not nodes:\n",
    "        return \"未找到相关信息。\"\n",
    "    return \"\\n\\n\".join(f\"【检索自: {n.metadata.get('doc_id', '未知文档')}】\\n{n.get_content()}\" for n in nodes)\n",
    "\n",
    "# -- 5. 定义 Pipeline --\n",
    "p = QueryPipeline(verbose=True)\n",
    "\n",
    "p.add_modules({\n",
    "    \"input\": InputComponent(),\n",
    "    \"science_retriever\": science_retriever,\n",
    "    \"history_retriever\": history_retriever,\n",
    "})\n",
    "\n",
    "# 分支链接\n",
    "p.add_link(\"input\", \"science_retriever\", condition_fn=is_science_query)\n",
    "p.add_link(\"input\", \"history_retriever\", condition_fn=lambda query: not is_science_query(query))\n",
    "\n",
    "# -- 6. 执行并显示结果 --\n",
    "def run_and_display_result(query: str):\n",
    "    print(f\"\\n--- 运行查询: '{query}' ---\")\n",
    "    output = p.run(query=query)\n",
    "\n",
    "    # ✅ 转换节点内容为可读回答\n",
    "    if isinstance(output, list) and output:\n",
    "        final_response = nodes_to_str(output)\n",
    "    else:\n",
    "        final_response = \"⚠️ 没有检索到结果，可能未命中任何文档。\"\n",
    "\n",
    "    # ✅ 用 markdown 显示回答\n",
    "    display(Markdown(f\"**模型回答：**\\n\\n{final_response}\"))\n",
    "\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# 测试\n",
    "run_and_display_result(query=\"爱因斯坦最出名的是什么？\")\n",
    "run_and_display_result(query=\"文艺复兴是什么？\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QueryEngine 的场景：何时需要命令式的、有状态的动态查询逻辑\n",
    "`QueryEngine` 是我们早期接触得最多的接口，如 `index.as_query_engine()`。\n",
    "\n",
    "- **核心思想**：它是一种更**命令式、更高级的抽象**。你把它当成一个“黑盒”，给它一个问题，它给你一个答案。它内部已经封装好了一套固定的（但可配置的）RAG 逻辑。\n",
    "- **适用场景**：\n",
    "  - **快速原型开发**：当你需要快速验证一个想法时，`as_query_engine()` 是最快的途径。\n",
    "  - **标准 RAG 流程**：如果你的需求就是标准的“检索-重排-合成”，那么 `QueryEngine` 完全够用。\n",
    "  - **有状态的交互**：`ChatEngine` 就是 `QueryEngine` 的一个有状态的变体，它需要维护对话历史，这种场景用类（Class）的封装比用函数式的 Pipeline 更自然。\n",
    "\n",
    "**总结：何时选择？**\n",
    "\n",
    "- 当你需要**透明、灵活、可组合、可复用**的工作流时，优先选择 **`QueryPipeline`**。\n",
    "- 当你需要**快速启动、标准流程、或处理有状态的对话**时，**`QueryEngine`** 是一个很好的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建智能体（Agents）的“大脑”\n",
    "当 RAG 系统不仅能回答问题，还能 **使用工具（Tools）** 去执行动作时，它就进化成了智能体（Agent）。这里的“工具”可以是任何东西：一个检索引擎、一个计算器、一个数据库查询 API、甚至另一个 Agent。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 工具使用的进化：从 ReActAgent 到基于 OpenAI Function Calling 的智能体\n",
    "**`ReActAgent`**:\n",
    "\n",
    "- **原理**: 基于“思考（Reason）+行动（Act）”的循环。LLM 会生成它的“内心独白”（思考），然后决定调用哪个工具（行动）。这是一个通用的框架，理论上适用于任何足够强大的 LLM。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is: Chinese. I need to compare the advantages and disadvantages of BM25 and vector retrieval.\n",
      "Answer: BM25和向量检索是两种常见的信息检索方法，各有优缺点：\n",
      "\n",
      "**BM25（关键词检索）的优点：**\n",
      "1. 解释性强：基于关键词匹配，结果易于理解和解释。\n",
      "2. 计算效率高：适合处理大规模文档集合。\n",
      "3. 对拼写错误和变体有一定的鲁棒性。\n",
      "4. 不需要训练数据，直接应用于文档集合。\n",
      "\n",
      "**BM25的缺点：**\n",
      "1. 无法捕捉语义相似性，仅依赖关键词匹配。\n",
      "2. 对同义词和多义词处理能力有限。\n",
      "3. 需要精确的关键词匹配，对查询表达要求较高。\n",
      "\n",
      "**向量检索（语义检索）的优点：**\n",
      "1. 能够捕捉语义相似性，理解查询的深层含义。\n",
      "2. 可以处理同义词和多义词问题。\n",
      "3. 支持更自然的查询表达，不依赖精确关键词匹配。\n",
      "\n",
      "**向量检索的缺点：**\n",
      "1. 需要预训练模型或大量数据来生成向量表示。\n",
      "2. 计算复杂度较高，尤其是处理大规模数据时。\n",
      "3. 结果有时难以解释（黑箱问题）。\n",
      "4. 对领域外数据可能表现不佳，除非进行微调。\n",
      "\n",
      "总结：BM25更适合需要快速、可解释结果的场景，而向量检索更适合需要理解语义的复杂查询场景。实际应用中，两者常结合使用以获得更好的效果。\n",
      "\u001b[0mBM25和向量检索是两种常见的信息检索方法，各有优缺点：\n",
      "\n",
      "**BM25（关键词检索）的优点：**\n",
      "1. 解释性强：基于关键词匹配，结果易于理解和解释。\n",
      "2. 计算效率高：适合处理大规模文档集合。\n",
      "3. 对拼写错误和变体有一定的鲁棒性。\n",
      "4. 不需要训练数据，直接应用于文档集合。\n",
      "\n",
      "**BM25的缺点：**\n",
      "1. 无法捕捉语义相似性，仅依赖关键词匹配。\n",
      "2. 对同义词和多义词处理能力有限。\n",
      "3. 需要精确的关键词匹配，对查询表达要求较高。\n",
      "\n",
      "**向量检索（语义检索）的优点：**\n",
      "1. 能够捕捉语义相似性，理解查询的深层含义。\n",
      "2. 可以处理同义词和多义词问题。\n",
      "3. 支持更自然的查询表达，不依赖精确关键词匹配。\n",
      "\n",
      "**向量检索的缺点：**\n",
      "1. 需要预训练模型或大量数据来生成向量表示。\n",
      "2. 计算复杂度较高，尤其是处理大规模数据时。\n",
      "3. 结果有时难以解释（黑箱问题）。\n",
      "4. 对领域外数据可能表现不佳，除非进行微调。\n",
      "\n",
      "总结：BM25更适合需要快速、可解释结果的场景，而向量检索更适合需要理解语义的复杂查询场景。实际应用中，两者常结合使用以获得更好的效果。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.agent.legacy.react.base import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# 加载文档\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# 构建两个索引（可使用不同参数/数据）\n",
    "index_bm25 = VectorStoreIndex.from_documents(documents)\n",
    "index_vector = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 各自的 QueryEngine\n",
    "bm25_engine = index_bm25.as_query_engine(similarity_top_k=3)\n",
    "vector_engine = index_vector.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# 工具封装\n",
    "tools = [\n",
    "    QueryEngineTool.from_defaults(query_engine=bm25_engine, name=\"BM25\", description=\"用于关键词检索\"),\n",
    "    QueryEngineTool.from_defaults(query_engine=vector_engine, name=\"Vector\", description=\"用于语义检索\")\n",
    "]\n",
    "\n",
    "agent = ReActAgent.from_tools(tools,  verbose=True)\n",
    "\n",
    "# 发起查询\n",
    "response = agent.query(\"请对比 BM25 和向量检索的优缺点\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`OpenAIAgent` (基于 Function Calling/Tool Calling)**:\n",
    "\n",
    "- **原理**: 利用了 OpenAI 等模型内置的“函数调用”能力。你向模型描述你的工具集，模型在需要时，会返回一个结构化的 JSON 对象，精确地告诉你应该调用哪个函数以及传入什么参数。\n",
    "- **优势**: 更结构化，更可靠，更不容易出错。是目前构建 Agent 的主流和推荐方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.openai import OpenAI\n",
    "# Settings.llm = OpenAI(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     api_key=\"xxxxx\",\n",
    "#     api_base=\"https://api.chatanywhere.tech\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: 请对比 BM25 和向量检索的优缺点\n",
      "BM25 和向量检索是两种常用的信息检索技术，它们各自有不同的优缺点。以下是对这两种方法的对比：\n",
      "\n",
      "### BM25\n",
      "\n",
      "#### 优点：\n",
      "1. **简单易懂**：BM25 是基于词频和文档长度的统计模型，易于理解和实现。\n",
      "2. **高效性**：在处理大规模文档时，BM25 的计算效率较高，尤其是在传统的文本检索任务中。\n",
      "3. **可调参数**：BM25 允许用户调整参数（如 k1 和 b），以优化检索效果。\n",
      "4. **良好的性能**：在许多标准信息检索任务中，BM25 表现出色，尤其是在短文本和关键词检索中。\n",
      "\n",
      "#### 缺点：\n",
      "1. **词义理解不足**：BM25 主要依赖于词的出现频率，无法理解词的语义关系。\n",
      "2. **对同义词和上下文敏感**：BM25 可能无法有效处理同义词或上下文相关的查询。\n",
      "3. **无法处理长文本**：在处理长文本时，BM25 的效果可能不如向量检索。\n",
      "\n",
      "### 向量检索\n",
      "\n",
      "#### 优点：\n",
      "1. **语义理解**：向量检索能够捕捉词语之间的语义关系，处理同义词和上下文信息。\n",
      "2. **高维空间**：通过将文本表示为高维向量，向量检索可以更好地处理复杂的查询。\n",
      "3. **适应性强**：向量检索可以与深度学习模型结合，适应不同类型的文本和任务。\n",
      "4. **处理长文本**：在处理长文本和复杂查询时，向量检索通常表现更好。\n",
      "\n",
      "#### 缺点：\n",
      "1. **计算成本高**：向量检索在计算和存储上通常比 BM25 更昂贵，尤其是在大规模数据集上。\n",
      "2. **实现复杂**：向量检索的实现和调优相对复杂，需要更多的技术知识。\n",
      "3. **依赖训练数据**：向量检索的效果往往依赖于训练数据的质量和数量。\n",
      "\n",
      "### 总结\n",
      "BM25 更适合于传统的关键词检索任务，尤其是在处理大规模文档时。而向量检索则在理解语义和处理复杂查询方面具有优势。选择哪种方法取决于具体的应用场景和需求。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.agent.openai import OpenAIAgent\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "# 加载文档\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# 构建两个索引，模拟两种检索方式\n",
    "index_bm25 = VectorStoreIndex.from_documents(documents)\n",
    "index_vector = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# 分别构建检索引擎（此处两个一样，仅做示例）\n",
    "bm25_engine = index_bm25.as_query_engine(similarity_top_k=3)\n",
    "vector_engine = index_vector.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "# 创建工具列表\n",
    "tools = [\n",
    "    QueryEngineTool.from_defaults(query_engine=bm25_engine, name=\"bm25\", description=\"基于关键词的 BM25 检索工具\"),\n",
    "    QueryEngineTool.from_defaults(query_engine=vector_engine, name=\"vector\", description=\"基于语义向量的检索工具\")\n",
    "]\n",
    "\n",
    "agent = OpenAIAgent.from_tools(tools,llm=Settings.llm, verbose=True)\n",
    "\n",
    "# 运行查询\n",
    "response = agent.query(\"请对比 BM25 和向量检索的优缺点\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 多代理系统架构：实现“主管-下属”模式，协同解决复杂任务\n",
    "\n",
    "对于一个需要多种能力才能解决的复杂任务（例如，“研究特斯拉最近的财报，总结其 AI 方面的主要进展，并写一封邮件摘要给 CEO”），单一 Agent 往往难以胜任。这时就需要一个**Agent 团队**。\n",
    "\n",
    "- **主管-下属（Supervisor-Worker）模式**：\n",
    "  - 一个**主管 Agent (Supervisor)**：它负责理解用户的最高层指令，将复杂任务分解成多个子任务。\n",
    "  - 多个**下属 Agent (Workers)**：每个下属都是一个特定领域的专家，拥有专门的工具。例如：\n",
    "    - `FinancialAnalystAgent`: 拥有查询财报数据库的工具。\n",
    "    - `TechResearcherAgent`: 拥有网络搜索和 RAG 检索技术文档的工具。\n",
    "    - `WriterAgent`: 拥有起草和修改文本的工具。\n",
    "  - **工作流程**：主管接收任务 -> 分解并派发给合适的下属 -> 下属执行并返回结果 -> 主管汇总所有结果，形成最终答案。\n",
    "\n",
    "**与** LangGraph 的协同 正如您所偏好，**`LangGraph` 是实现这种复杂、有状态、多代理协作工作流的业界标准**。LlamaIndex 可以无缝地作为 `LangGraph` 的“工具提供方”。\n",
    "\n",
    "你可以将 LlamaIndex 构建的任何 `QueryEngine` 或 `Retriever` 包装成一个 `Tool`，然后将这个 `Tool` 交给 `LangGraph` 中的 Agent 节点来调用。\n",
    "\n",
    "- **LlamaIndex 的角色**：提供强大的**知识工具**。\n",
    "- **LangGraph 的角色**：担任智能的**任务编排与调度中心**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在准备知识工具...\n",
      "知识工具准备完毕。\n",
      "\n",
      "--- 工作流图构建完成 ---\n",
      "\n",
      "--- 开始执行任务: <bound method BaseMessage.text of HumanMessage(content='请研究并总结一下 LlamaIndex 和 LangGraph', additional_kwargs={}, response_metadata={})> ---\n",
      "{'messages': [HumanMessage(content='请研究并总结一下 LlamaIndex 和 LangGraph', additional_kwargs={}, response_metadata={})]}\n",
      "---\n",
      "--- [Supervisor] 主管正在决策 ---\n",
      "{'messages': [HumanMessage(content='请研究并总结一下 LlamaIndex 和 LangGraph', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'function': {'arguments': '{\"input\": \"LlamaIndex\"}', 'name': 'research_tool'}, 'type': 'function'}, {'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'function': {'arguments': '{\"input\": \"LangGraph\"}', 'name': 'research_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 61, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BhVjL6DDMz85URDtzLQHgnmGzxPP9', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d701d656-ca30-41a9-88f8-44e732119d5e-0', tool_calls=[{'name': 'research_tool', 'args': {'input': 'LlamaIndex'}, 'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'type': 'tool_call'}, {'name': 'research_tool', 'args': {'input': 'LangGraph'}, 'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 50, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "---\n",
      "{'messages': [HumanMessage(content='请研究并总结一下 LlamaIndex 和 LangGraph', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'function': {'arguments': '{\"input\": \"LlamaIndex\"}', 'name': 'research_tool'}, 'type': 'function'}, {'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'function': {'arguments': '{\"input\": \"LangGraph\"}', 'name': 'research_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 61, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BhVjL6DDMz85URDtzLQHgnmGzxPP9', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d701d656-ca30-41a9-88f8-44e732119d5e-0', tool_calls=[{'name': 'research_tool', 'args': {'input': 'LlamaIndex'}, 'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'type': 'tool_call'}, {'name': 'research_tool', 'args': {'input': 'LangGraph'}, 'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 50, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='LlamaIndex 是一个用于构建、查询和评估大型语言模型（LLM）应用的框架。', name='research_tool', tool_call_id='call_k14gbZN7TqNJMi4ak3MFZECZ'), ToolMessage(content='LangGraph 是一个用于构建有状态、多代理应用的库，它将工作流表示为图。', name='research_tool', tool_call_id='call_5Y7WT2y0JWHr3km54Ob0zGPo')]}\n",
      "---\n",
      "--- [Supervisor] 主管正在决策 ---\n",
      "{'messages': [HumanMessage(content='请研究并总结一下 LlamaIndex 和 LangGraph', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'function': {'arguments': '{\"input\": \"LlamaIndex\"}', 'name': 'research_tool'}, 'type': 'function'}, {'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'function': {'arguments': '{\"input\": \"LangGraph\"}', 'name': 'research_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 61, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BhVjL6DDMz85URDtzLQHgnmGzxPP9', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d701d656-ca30-41a9-88f8-44e732119d5e-0', tool_calls=[{'name': 'research_tool', 'args': {'input': 'LlamaIndex'}, 'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'type': 'tool_call'}, {'name': 'research_tool', 'args': {'input': 'LangGraph'}, 'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 50, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='LlamaIndex 是一个用于构建、查询和评估大型语言模型（LLM）应用的框架。', name='research_tool', tool_call_id='call_k14gbZN7TqNJMi4ak3MFZECZ'), ToolMessage(content='LangGraph 是一个用于构建有状态、多代理应用的库，它将工作流表示为图。', name='research_tool', tool_call_id='call_5Y7WT2y0JWHr3km54Ob0zGPo'), AIMessage(content='LlamaIndex 是一个用于构建、查询和评估大型语言模型（LLM）应用的框架。\\n\\nLangGraph 是一个用于构建有状态、多代理应用的库，它将工作流表示为图。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 329, 'total_tokens': 378, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BhVjP1pTXzycfQ4TWuZSGnsKflu99', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--53e6c05c-8b42-40ef-b8b9-ae7c9dfaa8ec-0', usage_metadata={'input_tokens': 329, 'output_tokens': 49, 'total_tokens': 378, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n",
      "---\n",
      "--- [Worker] 作家 Agent 开始工作 ---\n",
      "{'messages': [HumanMessage(content='请研究并总结一下 LlamaIndex 和 LangGraph', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'function': {'arguments': '{\"input\": \"LlamaIndex\"}', 'name': 'research_tool'}, 'type': 'function'}, {'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'function': {'arguments': '{\"input\": \"LangGraph\"}', 'name': 'research_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 61, 'total_tokens': 111, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BhVjL6DDMz85URDtzLQHgnmGzxPP9', 'service_tier': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d701d656-ca30-41a9-88f8-44e732119d5e-0', tool_calls=[{'name': 'research_tool', 'args': {'input': 'LlamaIndex'}, 'id': 'call_k14gbZN7TqNJMi4ak3MFZECZ', 'type': 'tool_call'}, {'name': 'research_tool', 'args': {'input': 'LangGraph'}, 'id': 'call_5Y7WT2y0JWHr3km54Ob0zGPo', 'type': 'tool_call'}], usage_metadata={'input_tokens': 61, 'output_tokens': 50, 'total_tokens': 111, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='LlamaIndex 是一个用于构建、查询和评估大型语言模型（LLM）应用的框架。', name='research_tool', tool_call_id='call_k14gbZN7TqNJMi4ak3MFZECZ'), ToolMessage(content='LangGraph 是一个用于构建有状态、多代理应用的库，它将工作流表示为图。', name='research_tool', tool_call_id='call_5Y7WT2y0JWHr3km54Ob0zGPo'), AIMessage(content='LlamaIndex 是一个用于构建、查询和评估大型语言模型（LLM）应用的框架。\\n\\nLangGraph 是一个用于构建有状态、多代理应用的库，它将工作流表示为图。', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 49, 'prompt_tokens': 329, 'total_tokens': 378, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_ee1d74bde0', 'id': 'chatcmpl-BhVjP1pTXzycfQ4TWuZSGnsKflu99', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--53e6c05c-8b42-40ef-b8b9-ae7c9dfaa8ec-0', usage_metadata={'input_tokens': 329, 'output_tokens': 49, 'total_tokens': 378, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), HumanMessage(content='在当今迅速发展的人工智能领域，开发者和研究人员不断寻求更有效的工具来简化复杂的模型应用程序的开发和优化。两项新的创新工具——LlamaIndex和LangGraph——正为构建和管理先进的语言模型应用程序提供了解决方案。\\n\\nLlamaIndex是一个强大的框架，专门用于构建、查询和评估大型语言模型（LLM）应用。随着大型语言模型在文本生成、对话系统和语言理解等领域的广泛应用，对一个能够简化操作和优化性能的框架的需求变得愈发迫切。LlamaIndex提供了一个集成环境，使开发者能够高效地设计和调整LLM应用程序，并从中获取最大价值。它的功能不仅限于构建和查询，还包括完善的评估机制，确保模型的输出符合预期标准。\\n\\n另一方面，LangGraph是一个创新库，专注于创建有状态、多代理应用，它将工作流表示为图。随着人工智能应用程序变得越来越复杂，尤其是在涉及多个智能代理和动态交互的领域，LangGraph的出现及时满足了这类应用程序的开发需求。通过将工作流转化为图结构，开发者可以更直观地管理各个代理之间的复杂交互，并维护应用程序的状态。LangGraph不仅简化了多代理系统的构建，还优化了工作流的可视化和调整。\\n\\n总而言之，LlamaIndex和LangGraph这两个工具的引入，为人工智能开发者提供了更为精细和系统化的解决方案，助力他们在大型语言模型和多智能体系统领域中取得更大的突破。随着这两个工具的不断完善和成熟，它们势必在未来的高效应用开发中发挥关键作用。', additional_kwargs={}, response_metadata={})]}\n",
      "---\n",
      "\n",
      "\n",
      "--- 任务完成，最终报告 ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "在当今迅速发展的人工智能领域，开发者和研究人员不断寻求更有效的工具来简化复杂的模型应用程序的开发和优化。两项新的创新工具——LlamaIndex和LangGraph——正为构建和管理先进的语言模型应用程序提供了解决方案。\n",
       "\n",
       "LlamaIndex是一个强大的框架，专门用于构建、查询和评估大型语言模型（LLM）应用。随着大型语言模型在文本生成、对话系统和语言理解等领域的广泛应用，对一个能够简化操作和优化性能的框架的需求变得愈发迫切。LlamaIndex提供了一个集成环境，使开发者能够高效地设计和调整LLM应用程序，并从中获取最大价值。它的功能不仅限于构建和查询，还包括完善的评估机制，确保模型的输出符合预期标准。\n",
       "\n",
       "另一方面，LangGraph是一个创新库，专注于创建有状态、多代理应用，它将工作流表示为图。随着人工智能应用程序变得越来越复杂，尤其是在涉及多个智能代理和动态交互的领域，LangGraph的出现及时满足了这类应用程序的开发需求。通过将工作流转化为图结构，开发者可以更直观地管理各个代理之间的复杂交互，并维护应用程序的状态。LangGraph不仅简化了多代理系统的构建，还优化了工作流的可视化和调整。\n",
       "\n",
       "总而言之，LlamaIndex和LangGraph这两个工具的引入，为人工智能开发者提供了更为精细和系统化的解决方案，助力他们在大型语言模型和多智能体系统领域中取得更大的突破。随着这两个工具的不断完善和成熟，它们势必在未来的高效应用开发中发挥关键作用。"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import operator\n",
    "import os\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "# -- 核心模块导入 --\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, ToolMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "\n",
    "# -- 【LlamaIndex 的角色】创建并包装知识工具 --\n",
    "# LlamaIndex 是我们强大的知识工具提供方。\n",
    "print(\"正在准备知识工具...\")\n",
    "if not os.path.exists(\"research_data\"):\n",
    "    os.makedirs(\"research_data\")\n",
    "with open(\"research_data/agent_intro.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"LlamaIndex 是一个用于构建、查询和评估大型语言模型（LLM）应用的框架。\"\n",
    "            \"LangGraph 是一个用于构建有状态、多代理应用的库，它将工作流表示为图。\")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"research_data\").load_data()\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "research_tool_llama = QueryEngineTool(\n",
    "    query_engine=query_engine,\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"research_tool\",\n",
    "        description=\"用于查询关于 LlamaIndex 和 LangGraph 的信息。\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 将 LlamaIndex 工具转换为 LangChain/LangGraph 兼容的工具\n",
    "research_tool_langchain = research_tool_llama.to_langchain_tool()\n",
    "\n",
    "print(\"知识工具准备完毕。\")\n",
    "\n",
    "# -- 【LangGraph 的角色】定义 Agent 团队和工作流 --\n",
    "\n",
    "# a) 定义模型和工具\n",
    "# 我们将使用一个统一的模型作为所有 Agent 的大脑\n",
    "model = ChatOpenAI(temperature=0, model=\"gpt-4\",base_url=\"https://api.chatanywhere.tech\")\n",
    "tools = [research_tool_langchain]  # 【修复】使用转换后的 LangChain 工具\n",
    "\n",
    "\n",
    "# b) 定义工作流的状态 (Graph State)\n",
    "# 我们增加一个 messages 字段来跟踪整个对话历史，这让主管能做出更明智的决策。\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "\n",
    "\n",
    "# c) 【重构】定义“主管 Agent” (Supervisor Agent)\n",
    "# 主管现在是一个由 LLM 驱动的、真正的 Agent。它会根据对话历史和可用工具来决定下一步。\n",
    "def supervisor_agent(state: AgentState) -> dict:\n",
    "    \"\"\"这是一个由 LLM 驱动的主管，负责路由到正确的下属或结束任务。\"\"\"\n",
    "    print(\"--- [Supervisor] 主管正在决策 ---\")\n",
    "    # 获取对话历史\n",
    "    messages = state['messages']\n",
    "    # 将工具绑定到模型上，让模型知道它有哪些工具可用\n",
    "    model_with_tools = model.bind_tools(tools)\n",
    "    # 调用模型进行决策\n",
    "    response = model_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# d) 【重构】使用预置的 ToolNode 作为“研究员”\n",
    "# 我们不再需要自己写一个函数来调用工具，ToolNode 是完成此任务的标准模块。\n",
    "research_node = ToolNode(tools)\n",
    "\n",
    "\n",
    "# e) 定义“作家”节点\n",
    "# 这个节点保持简单，因为它不调用工具，只负责生成文本。\n",
    "def writer_node(state: AgentState) -> dict:\n",
    "    \"\"\"负责整合信息，撰写最终文稿\"\"\"\n",
    "    print(\"--- [Worker] 作家 Agent 开始工作 ---\")\n",
    "    # 获取最后一条 ToolMessage 的内容作为写作素材\n",
    "    tool_outputs = [msg.content for msg in state['messages'] if isinstance(msg, ToolMessage)]\n",
    "    content_to_write = \"\\n\".join(tool_outputs)\n",
    "\n",
    "    prompt = (f\"你是一位专业的科技作家。请根据以下研究结果，撰写一篇简洁明了的介绍。\\n\"\n",
    "              f\"研究结果: {content_to_write}\")\n",
    "\n",
    "    response = model.invoke(prompt)\n",
    "    # 我们用 HumanMessage 来包装最终的写作结果，以便清晰地识别\n",
    "    return {\"messages\": [HumanMessage(content=response.content)]}\n",
    "\n",
    "\n",
    "# -- 使用 LangGraph 构建图 --\n",
    "\n",
    "# a) 创建 StateGraph 实例\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# b) 添加所有节点\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)\n",
    "workflow.add_node(\"researcher\", research_node)\n",
    "workflow.add_node(\"writer\", writer_node)\n",
    "\n",
    "# c) 设置入口点\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "# d) 添加边\n",
    "# 1. 主管决策后，根据是否有工具调用指令，决定是去研究还是去写作\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",\n",
    "    # tools_condition 是 LangGraph 内置的路由逻辑，它会自动检查是否有工具调用\n",
    "    tools_condition,\n",
    "    # 如果需要调用工具，则流向 researcher；否则流向 writer\n",
    "    {\"tools\": \"researcher\", END: \"writer\"}\n",
    ")\n",
    "# 2. 研究员（ToolNode）工作完成后，将结果返回给主管进行下一轮决策（通常是写作）\n",
    "workflow.add_edge(\"researcher\", \"supervisor\")\n",
    "# 3. 作家工作完成后，结束流程\n",
    "workflow.add_edge(\"writer\", END)\n",
    "\n",
    "# e) 编译成可运行的图\n",
    "app = workflow.compile()\n",
    "print(\"\\n--- 工作流图构建完成 ---\")\n",
    "\n",
    "# -- 运行并观察 Agent 团队的协作过程 --\n",
    "# 初始任务现在以消息的形式给出\n",
    "initial_task = {\"messages\": [HumanMessage(content=\"请研究并总结一下 LlamaIndex 和 LangGraph\")]}\n",
    "\n",
    "print(f\"\\n--- 开始执行任务: {initial_task['messages'][0].text} ---\")\n",
    "\n",
    "# 使用 stream() 方法可以清晰地看到每一步的状态变化\n",
    "final_state = {}\n",
    "for s in app.stream(initial_task, stream_mode=\"values\"):\n",
    "    # s 会包含当前步骤的节点名和该步骤更新后的状态\n",
    "    final_state.update(s)\n",
    "    print(s)\n",
    "    print(\"---\")\n",
    "\n",
    "final_writing = final_state[\"messages\"][-1].content\n",
    "\n",
    "print(\"\\n\\n--- 任务完成，最终报告 ---\")\n",
    "display(Markdown(final_writing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 共享记忆与状态管理：让代理拥有长期记忆和跨任务上下文\n",
    "\n",
    "Agent 团队要能高效协作，必须要有**共享的记忆**。\n",
    "\n",
    "- **状态管理**: `LangGraph` 的核心就是一个**状态图（State Graph）**。它定义了一个全局的 `State` 对象（例如一个 Pydantic 模型），其中包含了任务的所有信息，如原始问题、子任务列表、每个子任务的结果、聊天历史等。\n",
    "- **共享记忆**: 这个 `State` 对象在图的每个节点（Agent）之间传递和更新。每个 Agent 执行完自己的任务后，都会将结果写回这个共享的 `State` 中。这样，后续的 Agent 就能看到前面所有 Agent 的工作成果，从而实现上下文共享和协同工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**本章小结**\n",
    "\n",
    "我们已经走出了简单 RAG 的范畴，进入了更广阔的 Agentic 应用世界。我们学会了用查询重构来“武装”我们的输入，用 `QueryPipeline` 来像艺术家一样“编排”我们的工作流，并理解了如何构建一个由 `LangGraph` 驱动、LlamaIndex 赋能的 Agent 团队。\n",
    "\n",
    "至此，我们的理论和核心技术学习已接近尾声。在下一部分“生产化篇”中，我们将讨论如何将我们构建的精密系统，稳健、高效、经济地部署到真实世界中。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四部分：生产化篇 - 性能、运维与 MLOps (Production-Ready RAG)\n",
    "\n",
    "### 📘 背景简介\n",
    "\n",
    "RAG（Retrieval-Augmented Generation）架构融合了信息检索与生成模型的优势，提升了 LLM 在特定领域知识和上下文感知能力下的回答质量。在实验环境中，构建一个简单的 RAG 系统并不困难，但在**生产环境中**部署高性能、可维护且具备监控能力的 RAG 系统，则需要一整套性能工程与 MLOps 实践支撑。\n",
    "\n",
    "本篇将深入探讨在 **性能优化、成本控制、可观测性、CI/CD 运维** 等方面的系统性建设方法，确保你构建的 RAG 系统可以稳定运行于真实业务环境中。\n",
    "\n",
    "------\n",
    "\n",
    "### 🧠 理论基础与核心问题\n",
    "\n",
    "#### 为什么 RAG 在生产环境中挑战重重？\n",
    "\n",
    "- **延迟敏感**：每次查询可能涉及多个远程组件（向量检索、LLM 调用等）。\n",
    "- **吞吐瓶颈**：LLM 和向量数据库都存在并发访问限制。\n",
    "- **监控困难**：上下游组件调用链复杂，性能瓶颈和数据偏差难以追踪。\n",
    "- **系统演进频繁**：Prompt、模型、索引等频繁变化，需 CI/CD 支持。\n",
    "\n",
    "------\n",
    "\n",
    "### 📌 应用场景举例\n",
    "\n",
    "| 场景           | 应用示例           | 挑战                       |\n",
    "| -------------- | ------------------ | -------------------------- |\n",
    "| 企业知识库助手 | 内部文档问答       | 数据变化频繁、权限控制复杂 |\n",
    "| 法律/医疗问答  | 高精度问答系统     | 精度要求高、生成必须可溯源 |\n",
    "| 客服机器人     | 多轮对话理解与响应 | 实时性强、上下文长         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 性能与成本工程\n",
    "在生产环境中，每一毫秒的延迟都影响着用户体验，每一分钱的成本都关系到项目的生死存亡。性能与成本优化是一场永无止境的、需要精打细算的追求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 三级缓存策略：LLM 响应、嵌入向量与索引节点的缓存实现\n",
    "**背景与理论**\n",
    "\n",
    "RAG 系统中最耗时、最昂贵的两个操作，莫过于调用 LLM API 和计算嵌入向量。一个智能的缓存策略，是降低延迟、节省成本最立竿见影的手段。我们可以从三个层面设计缓存：\n",
    "\n",
    "1. **一级缓存 (LLM 响应缓存)**：缓存对**完全相同问题**的最终回答。这是最高级别的缓存。\n",
    "   - **原理**：将用户查询字符串作为 Key，LLM 生成的最终响应作为 Value。当另一个用户提出完全相同的查询时，系统直接从缓存中返回结果，完全跳过检索、重排、LLM 调用等所有后续步骤。\n",
    "   - **适用场景**：高频查询（FAQ）、热点问题、功能演示等。\n",
    "2. **二级缓存 (嵌入向量缓存)**：避免对**相同文本**重复计算嵌入。\n",
    "   - **原理**：在数据注入（Ingestion）阶段，对每个文本块（Node）计算嵌入是一笔巨大的、一次性的开销。LlamaIndex 默认会在内存中启用嵌入缓存。当你使用 `StorageContext` 将索引持久化到磁盘时，这个缓存也会被一并保存。下次从磁盘加载索引时，无需为已有节点重新计算嵌入，极大地加速了索引的加载和更新。\n",
    "3. **三级缓存 (索引节点缓存)**：缓存对**相同查询**的检索结果。\n",
    "   - **原理**：比一级缓存更进一步，它不缓存最终答案，而是缓存检索器返回的 `Node` 列表。当遇到一个语义上相似但表述略有不同的查询时，可能可以复用这个缓存的节点列表，只重新执行最后的 LLM 合成步骤。\n",
    "   - **适用场景**：查询意图集中，但问法多样的场景。这通常需要更复杂的逻辑，例如将用户查询先归一化，再使用外部缓存（如 Redis）实现。\n",
    "\n",
    "**完整代码示例：启用全局 LLM 响应缓存**\n",
    "\n",
    "下面的代码将演示如何轻松启用 LlamaIndex 内置的 LLM 响应缓存，并验证其效果。\n",
    "\n",
    "**环境依赖**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-index\n",
    "# # 如果使用OpenAI，请确保安装了相关库并配置了API Key\n",
    "# # pip install llama-index-llms-openai llama-index-embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 一级缓存：LLM 响应缓存（使用 Redis）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "import redis\n",
    "\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "\n",
    "def hash_query(query: str) -> str:\n",
    "    return hashlib.md5(query.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def get_llm_response_cache(query: str):\n",
    "    return redis_client.get(f\"llm_response:{hash_query(query)}\")\n",
    "\n",
    "\n",
    "def set_llm_response_cache(query: str, response: str):\n",
    "    redis_client.set(f\"llm_response:{hash_query(query)}\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 二级缓存：嵌入向量缓存（LlamaIndex 内置）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化时构建\n",
    "documents = [\n",
    "        Document(text=\"向量数据库是一种用于存储和搜索高维向量的数据库，常用于机器学习和推荐系统中。\"),\n",
    "        Document(text=\"RAG（Retrieval-Augmented Generation）是一种结合检索和生成的问答方法，能增强LLM对知识的掌握。\"),\n",
    "        Document(text=\"OpenAI 的 GPT-4 是一种强大的大型语言模型，可用于代码生成、文本摘要和问答等任务。\")\n",
    "]\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 三级缓存：索引节点缓存（查询结果缓存）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_cache(query: str):\n",
    "    return redis_client.get(f\"retrieval_nodes:{hash_query(query)}\")\n",
    "\n",
    "def set_node_cache(query: str, nodes):\n",
    "    serialized = json.dumps([node.to_dict() for node in nodes])\n",
    "    redis_client.set(f\"retrieval_nodes:{hash_query(query)}\", serialized)\n",
    "\n",
    "\n",
    "def deserialize_nodes(data):\n",
    "    from llama_index.core.schema import NodeWithScore\n",
    "    return [NodeWithScore.from_dict(d) for d in json.loads(data)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 整合查询流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "\n",
    "def query_with_caching(query: str):\n",
    "    # 一级缓存\n",
    "    cached_response = get_llm_response_cache(query)\n",
    "    if cached_response:\n",
    "        return cached_response.decode(\"utf-8\")\n",
    "\n",
    "    # 三级缓存\n",
    "    node_data = get_node_cache(query)\n",
    "    if node_data:\n",
    "        nodes = deserialize_nodes(node_data)\n",
    "        synthesizer = get_response_synthesizer()\n",
    "        response = synthesizer.synthesize(query=query, nodes=nodes)\n",
    "    else:\n",
    "        retriever = VectorIndexRetriever(index=index, similarity_top_k=5)\n",
    "        nodes = retriever.retrieve(query)\n",
    "        set_node_cache(query, nodes)\n",
    "        synthesizer = get_response_synthesizer()\n",
    "        response = synthesizer.synthesize(query=query, nodes=nodes)\n",
    "\n",
    "    # 缓存最终响应\n",
    "    set_llm_response_cache(query, str(response))\n",
    "    return str(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  使用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量数据库是专门设计用于高效存储和检索高维数据结构的数据库系统。这类数据库通过数学计算快速找到相似向量，主要应用于需要处理复杂数据模式的领域，比如人工智能推荐引擎和机器学习系统中的特征匹配。其核心优势在于能对海量向量数据进行快速近邻搜索，为相似性检索任务提供专业支持。\n"
     ]
    }
   ],
   "source": [
    "response = query_with_caching(\"什么是向量数据库？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 如需扩展，可以加：\n",
    "\n",
    ">查询归一化（如 lower-case + 去除标点）以提升缓存命中率。\n",
    "\n",
    ">向量近似匹配的缓存命中（用于语义相似查询）。\n",
    "\n",
    ">缓存过期时间（Redis 设置 EXPIRE）防止缓存膨胀。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 吞吐量优化：asyncio 异步处理与智能批处理（Smart Batching）\n",
    "当应用需要同时处理多个用户并发请求时（例如一个 Web 服务），吞吐量成为瓶颈。\n",
    "\n",
    "- `asyncio` **异步处理**:\n",
    "  - **理论**：RAG 系统是典型的 I/O 密集型应用，大部分时间都花在等待网络上（等待 LLM API 返回、等待数据库查询返回）。同步阻塞模式下，一个请求在等待时，整个服务线程都被卡住，无法处理其他请求。`asyncio` 允许程序在等待一个任务完成时，自动切换去执行另一个任务，从而实现高并发。\n",
    "  - **应用场景**：在任何需要处理并发请求的后端服务中（如使用 FastAPI, Sanic, Tornado 等框架），都必须使用异步接口。\n",
    "  - **实现**：LlamaIndex 的核心组件都提供了对应的异步方法，命名约定通常是在同步方法前加 `a`。例如：`query_engine.query()` 对应 `query_engine.aquery()`；`retriever.retrieve()` 对应 `retriever.aretrieve()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM (模型) 已配置\n",
      "⏳ 开始加载数据...\n",
      "✅ 数据加载完成，共找到 1 份文档。\n",
      "⏳ 开始创建索引...\n",
      "✅ 索引创建完成。\n",
      "⏳ 开始创建查询引擎...\n",
      "✅ 查询引擎创建成功！应用已准备就绪。\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from fastapi import FastAPI\n",
    "# LlamaIndex 相关的导入\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Response\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 1. 初始化和加载设置\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# 从 .env 文件加载环境变量 (OPENAI_API_KEY)\n",
    "load_dotenv()\n",
    "\n",
    "# 配置 LlamaIndex 使用的模型，这里我们使用 OpenAI 的 gpt-3.5-turbo\n",
    "# 如果不设置，LlamaIndex 会默认尝试使用 OpenAI 的模型\n",
    "Settings.llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "print(\"✅ LLM (模型) 已配置\")\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. 创建 Query Engine (这是解决你问题的核心)\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def create_query_engine():\n",
    "    \"\"\"一个用于创建查询引擎的函数\"\"\"\n",
    "    print(\"⏳ 开始加载数据...\")\n",
    "    # 从 'data' 文件夹加载文档\n",
    "    documents = SimpleDirectoryReader(\"./data\").load_data()\n",
    "    print(f\"✅ 数据加载完成，共找到 {len(documents)} 份文档。\")\n",
    "\n",
    "    print(\"⏳ 开始创建索引...\")\n",
    "    # 基于加载的文档创建向量存储索引\n",
    "    # 索引是对数据进行预处理，以便快速检索\n",
    "    index = VectorStoreIndex.from_documents(documents)\n",
    "    print(\"✅ 索引创建完成。\")\n",
    "\n",
    "    print(\"⏳ 开始创建查询引擎...\")\n",
    "    # 从索引创建查询引擎\n",
    "    # 查询引擎接收问题，从索引中检索相关信息，然后让 LLM 生成答案\n",
    "    engine = index.as_query_engine()\n",
    "    print(\"✅ 查询引擎创建成功！应用已准备就绪。\")\n",
    "    return engine\n",
    "\n",
    "\n",
    "# 在应用启动时，全局只创建一次 query_engine\n",
    "# 这样可以避免每次 API 调用都重新加载数据和创建索引，从而提高效率\n",
    "query_engine = create_query_engine()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. 设置 FastAPI 应用\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "app = FastAPI(\n",
    "    title=\"LlamaIndex Query API\",\n",
    "    description=\"一个使用 LlamaIndex 和 FastAPI 构建的问答 API\"\n",
    ")\n",
    "\n",
    "\n",
    "class QueryRequest(BaseModel):\n",
    "    query: str\n",
    "\n",
    "\n",
    "@app.post(\"/query\", response_model=dict)\n",
    "async def query_api(request: QueryRequest):\n",
    "    \"\"\"\n",
    "    接收用户查询，使用 query_engine 获取答案，并返回。\n",
    "    \"\"\"\n",
    "    print(f\"收到查询: {request.query}\")\n",
    "    try:\n",
    "        # 现在 query_engine 已经是一个定义好的全局变量了\n",
    "        response: Response = await query_engine.aquery(request.query)\n",
    "\n",
    "        # 从 response 对象中提取答案文本\n",
    "        answer_text = response.response\n",
    "\n",
    "        return {\"answer\": answer_text}\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"查询时发生错误: {e}\")\n",
    "        return {\"error\": f\"处理您的请求时出现问题: {e}\"}\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def read_root():\n",
    "    return {\"message\": \"欢迎使用 LlamaIndex 查询 API，请向 /query 端点发送 POST 请求。\"}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **智能批处理（Smart Batching）**:\n",
    "  - **理论**：向 LLM 或嵌入模型 API 单独发送请求的开销很大（网络延迟、API 网关处理等）。将多个独立的请求打包成一个批次（batch）再统一发送，可以极大地摊销这些固定开销，利用模型和硬件的并行处理能力，显著提升吞吐量。\n",
    "  - **应用场景**：在数据注入（indexing）阶段，需要为成千上万个 `Node` 计算嵌入时，批处理是必须的。在服务查询阶段，如果能在短时间内聚合多个用户的查询请求，也可以进行批处理。\n",
    "  - **实现**：`Settings.embed_model.embed_batch_size` 和 `LLM` 的相关参数可以控制批处理的大小。你需要根据你的硬件（特别是 GPU 显存）、网络状况和 API 的速率限制，通过实验来找到最佳的 `batch_size`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.settings import Settings\n",
    "\n",
    "Settings.embed_model.embed_batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型部署与压缩：vLLM/TGI 服务优化与模型量化（Quantization）实战\n",
    "对于需要本地部署开源模型的场景，优化模型本身的推理性能，直接关系到服务的延迟和成本。\n",
    "\n",
    "- **推理服务器（Inference Servers）**:\n",
    "  - **理论**：直接使用 `transformers` 库加载并运行模型，其性能往往无法满足生产要求。业界领先的推理服务框架 **`vLLM`** 和 **`TGI`** (Text **`Generation Inference)`** 通过 PagedAttention、持续批处理（Continuous Batching）等先进技术，能将 LLM 的吞吐量提升一个数量级，并有效降低显存占用。\n",
    "  - **集成**：最佳实践是将你的本地模型用 vLLM 或 TGI 部署成一个独立的、兼容 OpenAI API 格式的微服务。然后，在 LlamaIndex 中，你可以使用 `llama-index-llms-openai-like` 库，像调用 OpenAI 一样调用你自己的、经过高度优化的模型服务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例：使用 vLLM 启动服务**\n",
    "\n",
    "```bash\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model path/to/quantized-model \\\n",
    "  --port 8000\n",
    "```\n",
    "\n",
    "并在 LlamaIndex 中这样调用：\n",
    "\n",
    "```python\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "llm = OpenAILike(api_base=\"http://localhost:8000/v1\", api_key=\"fake-key\", model=\"mymodel\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **模型量化（Quantization）**:\n",
    "  - **理论**：将模型权重从高精度浮点数（如 32-bit 或 16-bit 浮点数）转换为低精度整数（如 8-bit, 4-bit 整数）。这是一种以微小精度损失为代价，换取巨大性能提升的技术。\n",
    "  - **优势**：\n",
    "    1. **减小模型体积**：量化后的模型文件大小会显著减小（例如，4-bit 量化能将体积缩小为原来的 1/4）。\n",
    "    2. **降低显存占用**：可以在更小的硬件（如消费级显卡）上部署更大的模型。\n",
    "    3. **提升推理速度**：一些硬件对低精度整数运算有特殊优化。\n",
    "  - **常见格式与集成**：\n",
    "    - **`GGUF`**: 主要用于在 CPU 上进行高效推理，可以通过 `llama-index-llms-llama-cpp` 加载。\n",
    "    - **`AWQ`, `GPTQ`**: 主流的 GPU 量化格式，可以通过 `transformers` 库加载，并与 `vLLM`/`TGI` 结合使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**示例：量化模型加载**\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"model_name\", device_map=\"auto\", quantization_config=\"gptq\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model_name\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 向量数据库高级运维\n",
    "向量数据库是 RAG 系统的“地基”，其性能、稳定性和可扩展性直接影响上层应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引参数调优：HNSW vs IVFPQ\n",
    "在构建向量索引时，选择合适的参数是在**速度、内存、精度**之间进行权衡的艺术。这是向量数据库运维中最核心的技术环节。\n",
    "\n",
    "- **HNSW (Hierarchical Navigable Small World)**：这是目前绝大多数现代向量数据库（如 Qdrant, Weaviate, Milvus）默认或推荐的索引算法，适用于亿级规模以下的绝大多数场景。\n",
    "  - `M` (Max Connections): 图中每个节点的最大连接数。`M` 越大，图的连接越稠密，构建索引越慢，占用内存越多，但检索精度（召回率）也越高。\n",
    "  - `ef_construction`: 构建图时搜索的邻居节点数量。`ef_construction` 越大，构建越慢，但最终图的质量（精度）越高。\n",
    "  - `ef_search / ef`: 查询时搜索的邻居节点数量。`ef` 越大，搜索范围越广，精度越高，但查询延迟也越高。\n",
    "\n",
    "Milvus 支持 HNSW 索引（通过 `index_type=\"HNSW\"` 指定），以下是 Milvus 中关键参数的具体建议：\n",
    "\n",
    "| 参数             | 含义               | 推荐区间    | 说明                                                |\n",
    "| ---------------- | ------------------ | ----------- | --------------------------------------------------- |\n",
    "| `M`              | 最大连接数         | `16 ~ 64`   | 默认为 16，建议增大以提升召回。M 越大，占用内存越多 |\n",
    "| `efConstruction` | 构建阶段的搜索宽度 | `100 ~ 500` | 提升索引质量，构建慢但精准                          |\n",
    "| `ef`             | 查询时搜索宽度     | `64 ~ 512`  | 实时查询可动态设置，建议根据延迟要求调节            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection\n",
    "\n",
    "# 1. 连接 Milvus\n",
    "connections.connect(\"default\", host=\"localhost\", port=\"19530\")\n",
    "\n",
    "# 2. 定义字段 Schema\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, auto_id=True),\n",
    "    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=768)\n",
    "]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, description=\"example collection\")\n",
    "\n",
    "# 3. 创建 Collection（如果不存在）\n",
    "collection = Collection(name=\"my_collection\", schema=schema)\n",
    "\n",
    "# 4. 创建索引（如 HNSW）\n",
    "index_params = {\n",
    "    \"index_type\": \"HNSW\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\n",
    "        \"M\": 32,\n",
    "        \"efConstruction\": 200\n",
    "    }\n",
    "}\n",
    "\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
    "\n",
    "# 5. 加载 collection 以供查询\n",
    "collection.load()\n",
    "\n",
    "\n",
    "search_params = {\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"ef\": 128}\n",
    "}\n",
    "\n",
    "import numpy as np\n",
    "# 示例：一个模拟的查询向量（维度必须与 collection 中的向量字段一致）\n",
    "your_query_vector = np.random.rand(768).tolist()\n",
    "results = collection.search(\n",
    "    data=[your_query_vector],  # 列表包向量\n",
    "    anns_field=\"embedding\",\n",
    "    param=search_params,\n",
    "    limit=5,\n",
    "    output_fields=[\"id\"]  # 返回主键字段id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **IVFPQ** (Inverted File with Product **Quantization)**：一种适用于超大规模数据（十亿级以上）的算法。它通过聚类（IVF）和向量压缩（PQ）来牺牲一定精度换取极高的查询速度和极低的内存占用。\n",
    "  - `nlist`: 聚类中心的数量。`nlist` 越大，数据被划分得越细，查询时需要检查的数据越少，速度越快，但可能会损失精度。\n",
    "  - `nprobe`: 查询时要搜索的聚类中心的数量。`nprobe` 越大，搜索范围越广，精度越高，但速度越慢。\n",
    "\n",
    "**调优建议**：这是一个需要大量实验的工作。**切勿使用默认参数**。你应该建立一个包含代表性查询和“黄金标准”答案的评估集。然后像调节旋钮一样，逐步调整这些参数，观察它们对 **查询延迟（p99 延迟）** 和 **召回率（Recall@K）** 指标的影响，找到最适合你业务场景（例如，是优先保证低延迟，还是优先保证高召回）的“甜点区”。\n",
    "\n",
    "Milvus 中通过 `index_type=\"IVF_PQ\"` 启用，适合资源有限的大规模场景。\n",
    "\n",
    "| 参数     | 含义                 | 推荐区间      | 说明                                   |\n",
    "| -------- | -------------------- | ------------- | -------------------------------------- |\n",
    "| `nlist`  | 聚类中心数量         | `1K ~ 16K`    | 越大精度越高，Milvus 1.x 默认 1024     |\n",
    "| `nprobe` | 查询时搜索的中心数   | `8 ~ 64`      | 可以动态设置，推荐在负载与精度之间调优 |\n",
    "| `m`      | PQ 中子向量个数      | 通常 `4 ~ 16` | 影响压缩精度，和原始向量维度有关       |\n",
    "| `nbits`  | 每个 PQ 子向量的位数 | `8`           | 决定压缩精度，越小越快但损失越大       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Status(code=0, message=)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = Collection(name=\"my_collection2\", schema=schema)\n",
    "\n",
    "index_params = {\n",
    "    \"index_type\": \"IVF_PQ\",\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\n",
    "        \"nlist\": 4096,\n",
    "        \"m\": 8,\n",
    "        \"nbits\": 8\n",
    "    }\n",
    "}\n",
    "collection.create_index(field_name=\"embedding\", index_params=index_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_params = {\n",
    "    \"metric_type\": \"L2\",\n",
    "    \"params\": {\"nprobe\": 32}\n",
    "}\n",
    "results = collection.search(data=[your_query_vector], anns_field=\"embedding\", param=search_params, limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 可扩展架构：分片（Sharding）与复制（Replication）方案设计\n",
    "\n",
    "当数据量或查询量超过单个节点的承载能力时，就需要水平扩展。\n",
    "\n",
    "- **分片（Sharding）**: 当**数据量**过大，单一节点的 CPU、内存或磁盘无法容纳完整的索引时，就需要将索引水平分片到多台机器上。每个分片只负责数据的一个子集。这对于处理数十亿级别的向量至关重要。\n",
    "- **复制（Replication）**: 当 **查询量（QPS）** 过大时，为每个分片创建多个副本。这主要带来两个好处：\n",
    "  1. **高可用（High Availability）**: 当某个节点宕机时，请求可以自动路由到其副本，保证服务不中断。\n",
    "  2. **读扩展（Read Scalability）**: 可以将读请求负载均衡到所有副本上，极大地提升查询吞吐量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# def init_sample_data():\n",
    "#     os.makedirs(\"data/shard1\", exist_ok=True)\n",
    "#     os.makedirs(\"data/shard2\", exist_ok=True)\n",
    "\n",
    "#     with open(\"data/shard1/doc1.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(\"本系统是一个支持分片与复制的检索架构。\")\n",
    "\n",
    "#     with open(\"data/shard2/doc2.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#         f.write(\"本文介绍如何用 LlamaIndex 构建多索引组合。\")\n",
    "\n",
    "# # 初始化数据\n",
    "# init_sample_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "该产品采用两种关键技术来实现分布式索引：首先通过数据分片机制将索引信息分散存储在不同节点上，同时运用数据复制技术来保证系统的可靠性和容错能力。此外，系统还采用了多索引组合的架构设计来增强分布式索引功能。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    GPTVectorStoreIndex,\n",
    "    GPTListIndex,\n",
    "    StorageContext, ComposableGraph,\n",
    ")\n",
    "\n",
    "# 加载两个分片数据并构建子索引\n",
    "docs1 = SimpleDirectoryReader(\"data/shard1\").load_data()\n",
    "docs2 = SimpleDirectoryReader(\"data/shard2\").load_data()\n",
    "\n",
    "index1 = GPTVectorStoreIndex.from_documents(docs1)\n",
    "index2 = GPTVectorStoreIndex.from_documents(docs2)\n",
    "\n",
    "# 构建组合索引（使用列表组合器作为 root）\n",
    "summary1 = \"分片 1：产品文档\"\n",
    "summary2 = \"分片 2：技术博客\"\n",
    "\n",
    "graph = ComposableGraph.from_indices(\n",
    "    root_index_cls=GPTListIndex,\n",
    "    children_indices=[index1, index2],\n",
    "    index_summaries=[summary1, summary2]\n",
    ")\n",
    "\n",
    "# 执行跨分片查询\n",
    "query_engine = graph.as_query_engine()\n",
    "response = query_engine.query(\"这个产品如何实现分布式索引？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 成本考量：On-disk vs. In-memory 存储的选型指南\n",
    "\n",
    "**In-memory (内存型)**：以 `FAISS` 的标准索引为代表。将所有向量数据完整加载到内存中。\n",
    "\n",
    "- **优点**：速度最快，无 I/O 瓶颈。\n",
    "- **缺点**：成本极其高昂，规模完全受限于单机内存大小。\n",
    "- **适用场景**：对延迟要求达到极致（如金融高频交易）、且数据集规模较小（百万级以内）的特定场景。\n",
    "\n",
    "**On-disk (磁盘型)**：以 `Qdrant`, `Weaviate`, `Milvus` 等现代向量数据库为代表。\n",
    "\n",
    "- **原理**：主要将向量数据存储在磁盘（通常是高性能的 NVMe SSD）上，内存中只保留必要的索引结构（如图的连接关系）和热点数据缓存。\n",
    "- **优点**：成本低得多，可扩展性极强，能轻松处理海量数据。\n",
    "- **结论**：**对于 99% 的生产应用，基于磁盘的现代向量数据库是更理智、更具性价比、更可扩展的选择。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG 系统的可观测性（Observability）\n",
    "一个无法被观察的系统，就是一个黑盒。当生产环境出现问题时，如果不能快速定位，后果将是灾难性的。“可观测性”是保障生产稳定性的眼睛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 端到端链路追踪：与 Arize, Phoenix, LangSmith 等工具集成，可视化每一步调用\n",
    "**目的**: 可视化一个查询从用户输入到最终响应所经过的**完整路径**。例如：查询重写 -> 混合检索（向量检索+BM25）-> 重排 -> LLM 合成。\n",
    "\n",
    "**价值**:\n",
    "\n",
    "- **性能瓶颈定位**: 一眼看出哪个环节耗时最长。\n",
    "- **错误调试**: 精确知道错误发生在哪个组件，以及当时的输入是什么。\n",
    "- **效果分析**: 查看每一步的中间结果（如检索到了哪些 `Node`，最终的 Prompt 是什么样子的）。\n",
    "\n",
    "**实现**: LlamaIndex 通过 `CallbackManager` 支持与多种可观测性平台（如 Arize, Phoenix, LangSmith, Traceloop）的无缝集成。你通常只需在代码开始处进行简单的配置，LlamaIndex 就会自动将丰富的追踪数据（traces）发送到指定的平台。\n",
    "\n",
    "> 用户问题 ➜ 查询重写 ➜ 混合检索（向量 + BM25）➜ 结果重排 ➜ LLM 合成 ➜ 最终响应\n",
    ">  最新可观测性集成方式（以 Langfuse 为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get a copy of the latest Langfuse repository\n",
    "# git clone https://github.com/langfuse/langfuse.git\n",
    "# cd langfuse\n",
    "\n",
    "# # Run the langfuse docker compose\n",
    "# docker compose up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"LANGFUSE_SECRET_KEY\"] = \"sk-lf-...\"\n",
    "# os.environ[\"LANGFUSE_PUBLIC_KEY\"] = \"pk-lf-...\"\n",
    "# os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的上下文信息，没有提到任何关于如何启用链路追踪的内容。上下文主要描述了四位不同人物的教育背景和在校经历，与链路追踪无关。\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    GPTVectorStoreIndex\n",
    ")\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.callbacks import CallbackManager\n",
    "from langfuse.llama_index import LlamaIndexCallbackHandler\n",
    "\n",
    "# 1️⃣ 初始化 Langfuse 回调处理器\n",
    "langfuse_handler = LlamaIndexCallbackHandler()\n",
    "\n",
    "# 2️⃣ 注册到 LlamaIndex 全局 Handler（也可以手动注入）\n",
    "Settings.callback_manager = CallbackManager([langfuse_handler])\n",
    "\n",
    "# 后续构建索引 / 查询时将自动触发追踪\n",
    "\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "index = GPTVectorStoreIndex.from_documents(documents)\n",
    "\n",
    "response = index.as_query_engine().query(\"如何启用链路追踪？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 核心指标监控：设定 Groundedness, Context Relevance 等关键评估指标的自动化监控与告警\n",
    "\n",
    "在前言中我们强调了评估驱动开发（EDD）。在生产环境中，我们需要将这种评估**自动化**和**持续化**。\n",
    "\n",
    "- **核心指标**:\n",
    "  - **忠实度/根据性 (Groundedness/Faithfulness)**: 答案是否完全基于所提供的上下文？这是衡量“幻觉”的关键指标。\n",
    "  - **上下文相关性 (Context Relevance)**: 检索到的上下文，与用户的原始问题是否相关？\n",
    "  - **答案相关性 (Answer Relevance)**: 最终生成的答案，是否直接、清晰地回答了用户的原始问题？\n",
    "- **实现流程**:\n",
    "  1. 从生产环境的真实流量中**采样**一小部分请求（例如 1%）。\n",
    "  2. 使用 LlamaIndex 的 `Evaluation` 模块或第三方工具（如 `deepeval`, `ragas`），通过 LLM-as-a-judge 的方式，自动为这些采样的响应计算核心指标分数。\n",
    "  3. 将这些指标数据（如“忠实度平均分”）发送到你的监控系统（如 Prometheus, Datadog, Grafana）。\n",
    "  4. 设定**告警规则**：例如，如果“忠实度”指标的 P50 分位数在过去一小时内低于 0.8，就立即通过 PagerDuty 或 Slack 发送告警给开发团队。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 什么是 LlamaIndex？\n",
      "Score: 1.0, Feedback: YES\n",
      "Query: 如何启用链路追踪？\n",
      "Score: 1.0, Feedback: YES\n",
      "Query: 什么是三级缓存策略？\n",
      "Score: 1.0, Feedback: YES\n",
      "平均忠实度评分: 1.000\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "samples = [\n",
    "    {\n",
    "        \"query\": \"什么是 LlamaIndex？\",\n",
    "        \"contexts\": [\n",
    "            \"LlamaIndex 是一个用于构建基于文档的问答系统的库。\",\n",
    "            \"它支持向量检索和大语言模型集成。\"\n",
    "        ],\n",
    "        \"response\": \"LlamaIndex 是一个开源框架，可以帮助你构建基于文档的问答系统。\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"如何启用链路追踪？\",\n",
    "        \"contexts\": [\n",
    "            \"链路追踪可以帮助监控请求流程。\",\n",
    "            \"LlamaIndex 支持 CallbackManager 集成多种追踪工具。\"\n",
    "        ],\n",
    "        \"response\": \"你可以通过 CallbackManager 配置支持链路追踪，比如集成 Langfuse。\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"什么是三级缓存策略？\",\n",
    "        \"contexts\": [\n",
    "            \"三级缓存包括 LLM 响应缓存、嵌入向量缓存和索引节点缓存。\"\n",
    "        ],\n",
    "        \"response\": \"三级缓存策略通过不同层级缓存提高查询性能，减少延迟。\"\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluator = FaithfulnessEvaluator()\n",
    "\n",
    "scores = []\n",
    "for sample in samples:\n",
    "    result = evaluator.evaluate(\n",
    "        query=sample[\"query\"],\n",
    "        contexts=sample[\"contexts\"],\n",
    "        response=sample[\"response\"]\n",
    "    )\n",
    "    print(f\"Query: {sample['query']}\")\n",
    "    print(f\"Score: {result.score}, Feedback: {result.feedback}\")\n",
    "    scores.append(result.score)\n",
    "\n",
    "avg_score = sum(scores) / len(scores)\n",
    "print(f\"平均忠实度评分: {avg_score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 成本分析：追踪每个查询的 Token 消耗与 API 调用成本\n",
    "- **目的**: 精确计算每个 API 调用的成本，分析不同类型查询的成本分布，防止预算失控。\n",
    "- **实现**: 使用 LlamaIndex 内置的 `TokenCountingHandler` 回调。它可以精确地追踪每次 LLM 调用消耗的 `prompt_tokens`, `completion_tokens`, 和 `total_tokens`。\n",
    "\n",
    "**完整代码示例：使用 `TokenCountingHandler` 追踪成本**\n",
    "\n",
    "**环境依赖**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index tiktoken\n",
    "# tokenizer = tiktoken.encoding_for_model(\"gpt-3.5-turbo\").encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Token Usage: 0\n",
      "\n",
      "--- 正在执行查询以追踪 Token 消耗 ---\n",
      "Embedding Token Usage: 0\n",
      "LLM Prompt Token Usage: 6\n",
      "LLM Completion Token Usage: 0\n",
      "回答: LlamaIndex 是一个专门设计用于构建大型语言模型（LLM）应用程序的数据框架。\n",
      "\n",
      "--- Token 消耗统计 ---\n",
      "嵌入 Token (Embedding Tokens): 0\n",
      "LLM 提示 Token (Prompt Tokens): 6\n",
      "LLM 完成 Token (Completion Tokens): 0\n",
      "总 LLM Token (Total LLM Tokens): 6\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer, models\n",
    "from llama_index.core.callbacks import CallbackManager, TokenCountingHandler\n",
    "from llama_index.core import Settings, Document, VectorStoreIndex\n",
    "\n",
    "# 初始化一个空的 BPE tokenizer（未训练，仅用于演示）\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# ✅ 包装一个函数，用于返回 token list（兼容 TokenCountingHandler）\n",
    "def simple_tokenizer(text: str) -> list[int]:\n",
    "    return tokenizer.encode(text).ids  # 返回 token ID 列表\n",
    "\n",
    "# 注册到 TokenCountingHandler\n",
    "token_counter = TokenCountingHandler(tokenizer=simple_tokenizer, verbose=True)\n",
    "\n",
    "# 添加到 LlamaIndex 的全局 CallbackManager\n",
    "Settings.callback_manager = CallbackManager([token_counter])\n",
    "\n",
    "# 构建简单文档和索引\n",
    "documents = [Document(text=\"LlamaIndex 是一个用于构建 LLM 应用的数据框架。\")]\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# 执行查询\n",
    "print(\"\\n--- 正在执行查询以追踪 Token 消耗 ---\")\n",
    "response = query_engine.query(\"LlamaIndex 是什么?\")\n",
    "print(f\"回答: {response}\")\n",
    "\n",
    "# 查看 token 统计信息\n",
    "print(f\"\\n--- Token 消耗统计 ---\")\n",
    "print(f\"嵌入 Token (Embedding Tokens): {token_counter.total_embedding_token_count}\")\n",
    "print(f\"LLM 提示 Token (Prompt Tokens): {token_counter.prompt_llm_token_count}\")\n",
    "print(f\"LLM 完成 Token (Completion Tokens): {token_counter.completion_llm_token_count}\")\n",
    "print(f\"总 LLM Token (Total LLM Tokens): {token_counter.total_llm_token_count}\")\n",
    "\n",
    "# 重置计数器\n",
    "token_counter.reset_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 面向 RAG 的 CI/CD\n",
    "将 DevOps 的最佳实践应用于 RAG 系统，是保证其长期、可靠、高质量迭代的关键。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 版本化一切：提示词（Prompt）、索引配置、评估数据集的版本管理\n",
    "**提示词（Prompts）**: **绝不能硬编码在代码里**。应将其作为独立的模板文件（如 `.txt` 或 `.yaml`）存储在 Git 中，并由程序在运行时加载。这样修改提示词就不需要重新部署整个服务。\n",
    "\n",
    "**索引与Agent配置**: 定义分块策略、嵌入模型、检索器参数、Agent 工具集等所有配置文件，都必须纳入版本控制。\n",
    "\n",
    "**评估数据集**: 使用 Git LFS 或 DVC 等工具来版本化你的评估问题集和“黄金标准”答案，确保评估的一致性和可复现性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 自动化评估流水线：在部署前自动运行评估，防止性能衰退\n",
    "这是 CI/CD 的核心。一个典型的针对 RAG 的自动化评估流水线应该如下：\n",
    "\n",
    "1. **触发**: 当一个 Pull Request 被创建时（例如，有人修改了一个 Prompt 模板或检索器配置）。\n",
    "2. **构建**: 流水线在一个隔离的环境中，使用新的配置构建一个临时的 RAG 系统。\n",
    "3. **评估**: 自动运行完整的评估数据集，并计算所有核心指标（忠实度、相关性等）。\n",
    "4. **报告**: 将新配置下的指标与主分支（`main`）的基线指标进行对比，并将一份清晰的对比报告（例如，`忠实度: 0.95 -> 0.92 (-0.03)`）评论到 Pull Request 中。\n",
    "5. **决策（门禁）**: 如果任何关键指标出现显著下降，则**自动阻止该 PR 的合并**，或要求团队中的高级工程师进行人工审查。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 索引的热更新与蓝绿部署\n",
    "**热更新**: 如何在不中断服务的情况下更新知识库？对于支持实时 `upsert`/`delete` 的现代向量数据库，可以直接通过 API 增量更新。对于不支持的（如本地的 FAISS 文件），则需要在后台构建一个全新的索引。\n",
    "\n",
    "**蓝绿部署（Blue-Green Deployment）**: 这是最安全、最平滑的索引更新策略。\n",
    "\n",
    "1. 假设当前线上所有查询流量都指向**蓝色环境**的索引 A。\n",
    "2. 你在一个隔离的**绿色环境**中，基于最新的数据，构建一个全新的索引 B。\n",
    "3. 在新索引 B 构建完成并通过所有自动化评估后，将流量负载均衡器或应用网关的配置修改，将所有**新**的查询流量**平滑地切换**到绿色环境。\n",
    "4. 观察一段时间（例如 1 小时），如果一切正常（监控指标平稳），则可以安全地销毁旧的蓝色环境中的索引 A。如果出现任何问题，可以瞬间将流量切回蓝色环境，实现秒级回滚，对用户影响降到最低。\n",
    "\n",
    "\n",
    "**本章小结与进阶建议**\n",
    "\n",
    "我们已经系统地学习了将一个 RAG 原型锻造成生产级服务所需的全部工程知识。从性能与成本的精打细算，到数据库的深度运维，再到可观测性的建立和 CI/CD 流程的搭建。掌握了这些，您不仅是一个 RAG 开发者，更是一位能交付可靠、可扩展、可维护的 AI 应用的软件工程师。\n",
    "\n",
    "**进阶之路**:\n",
    "\n",
    "- **安全加固**: 深入研究 PII（个人可识别信息）的自动检测与脱敏，以及更精细化的基于角色的访问控制（RBAC）在 RAG 中的实现。\n",
    "- **多租户架构**: 学习如何为多个客户或部门在同一个 RAG 系统中提供服务，同时保证他们之间的数据严格隔离。\n",
    "- **A/B 测试与强化学习**: 探索如何在线上对不同的 Prompt 或重排模型进行 A/B 测试，并最终利用强化学习（RLAIF）根据用户真实反馈来自动优化 RAG 链路。\n",
    "\n",
    "至此，本宝典的核心技术内容已经全部呈现。在最后的“项目实战篇”中，我们将把前面所有章节学到的知识融会贯通，挑战构建真实世界的复杂 RAG 系统。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五部分：项目实战篇 - 构建真实世界的 RAG 系统 (Capstone Projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目一：构建企业级财报分析助手\n",
    "\n",
    "技术栈：LlamaParse 解析 PDF，层次化分块，混合检索，SubQuestionQueryEngine\n",
    "\n",
    "目标：能回答“对比 A 公司和 B 公司过去三年的研发投入和毛利率变化趋势”等复杂问题\n",
    "\n",
    "**1. 背景与业务价值**\n",
    "\n",
    "上市公司发布的年度或季度财报（如 10-K, 10-Q 文件）是投资分析、市场研究和竞品分析的基石。然而，这些文档通常是上百页的 PDF，结构复杂，包含大量表格、文本和脚注，人工阅读和对比分析极其耗时且容易出错。\n",
    "\n",
    "我们的目标是构建一个智能助手，它能深度理解多份财报内容，并回答需要进行**对比、归纳和计算**的复杂问题。例如：“请对比 A 公司和 B 公司过去三年的研发投入（R&D Expenses）和毛利率（Gross Margin）变化趋势，并总结 A 公司在管理层讨论中提到的主要原因。”\n",
    "\n",
    "**2. 核心技术栈解析**\n",
    "\n",
    "为了应对这一挑战，我们选择的技术栈是经过深思熟虑的：\n",
    "\n",
    "- **`LlamaParse`**：财报中充满了复杂的表格。`LlamaParse` 是处理这种半结构化 PDF 的不二之选，它能将表格完美地解析为 Markdown，保留其行列结构，这是后续进行数据提取和计算的基础。\n",
    "- **层次化分块 (`HierarchicalNodeParser`)**：一份财报天然具有层次结构（章节 -> 子章节 -> 段落）。层次化分块能让我们在检索时，根据问题的粒度，自适应地选择返回精炼的段落，还是完整的章节上下文，这对于理解管理层的长篇论述至关重要。\n",
    "- **混合检索 (`QueryFusionRetriever` 或 `AutoMergingRetriever` 组合)**：财报中包含了大量的专业术语（如 \"amortization\", \"goodwill\"）和数字。混合检索能结合 BM25 的关键词匹配能力和向量的语义理解能力，确保既能找到精确的术语，又能理解相关的讨论。\n",
    "- **子问题查询引擎 (`SubQuestionQueryEngine`)**：这是解决我们核心——“对比类问题”的关键。它能将一个复杂的对比问题，智能地分解为多个针对单一对象的简单子问题（例如，分解为“A 公司过去三年的研发投入是多少？”、“B 公司的毛利率是多少？”等），分别执行后再进行综合，极大地提升了答案的准确性和结构性。\n",
    "\n",
    "**3. 完整代码示例**\n",
    "\n",
    "下面的代码将构建一个完整的财报分析流程。\n",
    "\n",
    "**环境依赖**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install llama-parse\n",
    "# pip install llama-index-llms-deepseek\n",
    "# # 如果使用OpenAI，请确保安装了相关库并配置了API Key\n",
    "# # pip install llama-index-llms-openai llama-index-embeddings-openai llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**配置说明**:\n",
    "\n",
    "- 如果您要使用 `LlamaParse`，需要一个 API Key 并将其设置为环境变量 `LLAMA_CLOUD_API_KEY`。为了代码的通用性，我们将默认使用标准解析器，但注释中保留了 `LlamaParse` 的用法。\n",
    "- 代码将首先在本地创建两份模拟的财报 PDF 文件，以确保示例可独立、完整地运行。\n",
    "- 我们将为每家公司构建一个独立的查询引擎工具，然后交由 `SubQuestionQueryEngine` 进行调度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-...\"\n",
    "# os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-...\"\n",
    "# os.environ[\"LLAMA_CLOUD_API_KEY\"] = \"llx-...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM 和 Embedding 模型配置完成 (使用 DeepSeek 和 DashScope)。\n",
      "在 './financial_reports' 目录下成功创建了模拟的 PDF 财报。\n",
      "✅ 已为两份财报分别构建高级查询引擎。\n",
      "✅ Sub-question 查询引擎已创建，并装载了两个财报工具。\n",
      "\n",
      "--- 正在执行复杂查询 ---\n",
      "问题: 对比 FutureTech 和 InnovateCorp 在 2024 年的研发投入和毛利率，并解释 FutureTech 高研发投入的原因。\n",
      "Generated 5 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[futuretech_report_2024] Q: FutureTech 2024 年的研发投入是多少？\n",
      "\u001b[0m> Merging 1 nodes into parent node.\n",
      "> Parent node id: 589db68f-5f59-498a-9129-4a81c92bafe1.\n",
      "> Parent node text: # FutureTech Inc. 2024 Annual Report\n",
      "## Management Discussion\n",
      "In 2024, our R&D expenses increased...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: d78536a5-62fb-4ff7-ac43-3bdee4666e78.\n",
      "> Parent node text: # FutureTech Inc. 2024 Annual Report\n",
      "## Management Discussion\n",
      "In 2024, our R&D expenses increased...\n",
      "\n",
      "\u001b[1;3;38;2;237;90;200m[futuretech_report_2024] A: FutureTech 2024年的研发投入是5000万美元。\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[innovatecorp_report_2024] Q: InnovateCorp 2024 年的研发投入是多少？\n",
      "\u001b[0m> Merging 1 nodes into parent node.\n",
      "> Parent node id: e085fea1-577f-4817-9a1a-ec781b470915.\n",
      "> Parent node text: # InnovateCorp 2024 Annual Report\n",
      "## Management Discussion\n",
      "InnovateCorp's 2024 strategy focused o...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 2e816e85-82af-47ec-9b66-30bf06c64c90.\n",
      "> Parent node text: # InnovateCorp 2024 Annual Report\n",
      "## Management Discussion\n",
      "InnovateCorp's 2024 strategy focused o...\n",
      "\n",
      "\u001b[1;3;38;2;90;149;237m[innovatecorp_report_2024] A: InnovateCorp 2024年的研发投入为3000万美元。\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[futuretech_report_2024] Q: FutureTech 2024 年的毛利率是多少？\n",
      "\u001b[0m> Merging 1 nodes into parent node.\n",
      "> Parent node id: 589db68f-5f59-498a-9129-4a81c92bafe1.\n",
      "> Parent node text: # FutureTech Inc. 2024 Annual Report\n",
      "## Management Discussion\n",
      "In 2024, our R&D expenses increased...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: d78536a5-62fb-4ff7-ac43-3bdee4666e78.\n",
      "> Parent node text: # FutureTech Inc. 2024 Annual Report\n",
      "## Management Discussion\n",
      "In 2024, our R&D expenses increased...\n",
      "\n",
      "\u001b[1;3;38;2;11;159;203m[futuretech_report_2024] A: FutureTech 2024年的毛利率是60%。\n",
      "\u001b[0m\u001b[1;3;38;2;155;135;227m[innovatecorp_report_2024] Q: InnovateCorp 2024 年的毛利率是多少？\n",
      "\u001b[0m> Merging 1 nodes into parent node.\n",
      "> Parent node id: e085fea1-577f-4817-9a1a-ec781b470915.\n",
      "> Parent node text: # InnovateCorp 2024 Annual Report\n",
      "## Management Discussion\n",
      "InnovateCorp's 2024 strategy focused o...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: 2e816e85-82af-47ec-9b66-30bf06c64c90.\n",
      "> Parent node text: # InnovateCorp 2024 Annual Report\n",
      "## Management Discussion\n",
      "InnovateCorp's 2024 strategy focused o...\n",
      "\n",
      "\u001b[1;3;38;2;155;135;227m[innovatecorp_report_2024] A: InnovateCorp 2024年的毛利率为65%。\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[futuretech_report_2024] Q: FutureTech 2024 年管理层对高研发投入的讨论或解释是什么？\n",
      "\u001b[0m> Merging 1 nodes into parent node.\n",
      "> Parent node id: 589db68f-5f59-498a-9129-4a81c92bafe1.\n",
      "> Parent node text: # FutureTech Inc. 2024 Annual Report\n",
      "## Management Discussion\n",
      "In 2024, our R&D expenses increased...\n",
      "\n",
      "> Merging 1 nodes into parent node.\n",
      "> Parent node id: d78536a5-62fb-4ff7-ac43-3bdee4666e78.\n",
      "> Parent node text: # FutureTech Inc. 2024 Annual Report\n",
      "## Management Discussion\n",
      "In 2024, our R&D expenses increased...\n",
      "\n",
      "\u001b[1;3;38;2;237;90;200m[futuretech_report_2024] A: 2024年，管理层解释公司研发费用增至5000万美元的主要原因是大力投资于\"QuantumLeap\"人工智能项目。这一战略性举措旨在确保未来市场领导地位。\n",
      "\u001b[0m\n",
      "\n",
      "--- 最终综合回答 ---\n",
      "2024年，FutureTech的研发投入为5000万美元，高于InnovateCorp的3000万美元。在毛利率方面，FutureTech为60%，略低于InnovateCorp的65%。  \n",
      "\n",
      "FutureTech较高的研发投入主要是由于公司对\"QuantumLeap\"人工智能项目进行了战略性投资，管理层认为这一举措有助于巩固其未来的市场领导地位。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# -- 核心模块导入 --\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, get_leaf_nodes\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.core.query_engine import SubQuestionQueryEngine, RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "import nest_asyncio\n",
    "import llama_parse\n",
    "\n",
    "# --- 1. 环境与模型配置 ---\n",
    "\n",
    "# 配置 Embedding 模型\n",
    "Settings.embed_model = DashScopeEmbedding(\n",
    "    model_name=\"text-embedding-v2\"\n",
    ")\n",
    "\n",
    "# 【修复】使用专门的 DeepSeek 类来配置 LLM\n",
    "Settings.llm = DeepSeek(\n",
    "    model=\"deepseek-chat\",\n",
    "    api_key=os.environ.get(\"DEEPSEEK_API_KEY\"),\n",
    ")\n",
    "print(\"✅ LLM 和 Embedding 模型配置完成 (使用 DeepSeek 和 DashScope)。\")\n",
    "\n",
    "# 应用 nest_asyncio 以在 Jupyter 等环境中运行 LlamaParse\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- 2. 创建模拟的财报 PDF 文件 ---\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except ImportError:\n",
    "    print(\"fpdf 未安装，将跳过 PDF 创建。请手动创建文本文件。\")\n",
    "    FPDF = None\n",
    "\n",
    "DATA_DIR = \"./financial_reports\"\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR)\n",
    "\n",
    "report_a_content = \"\"\"\n",
    "# FutureTech Inc. 2024 Annual Report\n",
    "\n",
    "## Management Discussion\n",
    "In 2024, our R&D expenses increased to $50M due to heavy investment in our 'QuantumLeap' AI project. This strategic move aims to secure future market leadership. Our gross margin stood at 60%.\n",
    "\n",
    "## Financials\n",
    "| Metric         | Value  |\n",
    "|----------------|--------|\n",
    "| R&D Expenses   | $50M   |\n",
    "| Gross Margin   | 60%    |\n",
    "\"\"\"\n",
    "report_b_content = \"\"\"\n",
    "# InnovateCorp 2024 Annual Report\n",
    "\n",
    "## Management Discussion\n",
    "InnovateCorp's 2024 strategy focused on optimizing operational efficiency. R&D spending was maintained at $30M. This cost control helped us achieve a robust gross margin of 65%.\n",
    "\n",
    "## Financials\n",
    "| Metric         | Value  |\n",
    "|----------------|--------|\n",
    "| R&D Expenses   | $30M   |\n",
    "| Gross Margin   | 65%    |\n",
    "\"\"\"\n",
    "\n",
    "if FPDF:\n",
    "    pdf_a_path = os.path.join(DATA_DIR, \"FutureTech_2024_report.pdf\")\n",
    "    pdf_b_path = os.path.join(DATA_DIR, \"InnovateCorp_2024_report.pdf\")\n",
    "\n",
    "    pdf_a = FPDF()\n",
    "    pdf_a.add_page()\n",
    "    pdf_a.set_font(\"Arial\", \"B\", 16)  # 加粗标题\n",
    "    pdf_a.multi_cell(0, 10, report_a_content)\n",
    "    pdf_a.output(pdf_a_path)\n",
    "\n",
    "    pdf_b = FPDF()\n",
    "    pdf_b.add_page()\n",
    "    pdf_b.set_font(\"Arial\", \"B\", 16)  # 加粗标题\n",
    "    pdf_b.multi_cell(0, 10, report_b_content)\n",
    "    pdf_b.output(pdf_b_path)\n",
    "\n",
    "    print(f\"在 '{DATA_DIR}' 目录下成功创建了模拟的 PDF 财报。\")\n",
    "else:\n",
    "    pdf_a_path = os.path.join(DATA_DIR, \"FutureTech_2024_report.txt\")\n",
    "    pdf_b_path = os.path.join(DATA_DIR, \"InnovateCorp_2024_report.txt\")\n",
    "    with open(pdf_a_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_a_content)\n",
    "    with open(pdf_b_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(report_b_content)\n",
    "    print(f\"在 '{DATA_DIR}' 目录下成功创建了模拟的 TXT 财报。\")\n",
    "\n",
    "\n",
    "# --- 3. 为每家公司构建一个高级 RAG 引擎 ---\n",
    "def build_report_query_engine(report_path: str) -> RetrieverQueryEngine:\n",
    "    \"\"\"\n",
    "    为单份财报构建一个集成了层次化、自动合并检索的高级查询引擎。\n",
    "    \"\"\"\n",
    "    documents = SimpleDirectoryReader(input_files=[report_path]).load_data()\n",
    "\n",
    "    node_parser = HierarchicalNodeParser.from_defaults(chunk_sizes=[2048, 512, 128])\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    leaf_nodes = get_leaf_nodes(nodes)\n",
    "\n",
    "    docstore = SimpleDocumentStore()\n",
    "    docstore.add_documents(nodes)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(docstore=docstore)\n",
    "    vector_index = VectorStoreIndex(leaf_nodes, storage_context=storage_context)\n",
    "\n",
    "    base_retriever = vector_index.as_retriever(similarity_top_k=3)\n",
    "    retriever = AutoMergingRetriever(base_retriever, storage_context, verbose=True)\n",
    "\n",
    "    return RetrieverQueryEngine.from_args(retriever)\n",
    "\n",
    "\n",
    "engine_a = build_report_query_engine(pdf_a_path)\n",
    "engine_b = build_report_query_engine(pdf_b_path)\n",
    "print(\"✅ 已为两份财报分别构建高级查询引擎。\")\n",
    "\n",
    "# --- 4. 将每个引擎包装成工具，并交给 SubQuestionQueryEngine ---\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=engine_a,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"futuretech_report_2024\",\n",
    "            description=\"提供关于 FutureTech 公司 2024 年财报的信息，包括管理层讨论、财务数据、研发投入和毛利率等。\",\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=engine_b,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"innovatecorp_report_2024\",\n",
    "            description=\"提供关于 InnovateCorp 公司 2024 年财报的信息，包括其战略、成本控制、财务表现和毛利率等。\",\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "s_query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    use_async=False,  # 为了简单起见，禁用异步\n",
    "    verbose=True,\n",
    ")\n",
    "print(\"✅ Sub-question 查询引擎已创建，并装载了两个财报工具。\")\n",
    "\n",
    "# --- 5. 提出一个复杂的对比性问题 ---\n",
    "complex_query = \"对比 FutureTech 和 InnovateCorp 在 2024 年的研发投入和毛利率，并解释 FutureTech 高研发投入的原因。\"\n",
    "\n",
    "print(f\"\\n--- 正在执行复杂查询 ---\")\n",
    "print(f\"问题: {complex_query}\")\n",
    "response = s_query_engine.query(complex_query)\n",
    "\n",
    "print(\"\\n\\n--- 最终综合回答 ---\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 关键点解析**\n",
    "\n",
    "- **分而治之**：`SubQuestionQueryEngine` 的核心思想是“分而治之”。它将一个人类看似简单但对机器而言极其复杂的对比任务，分解成了多个 LLM 可以轻易完成的、针对单一工具的“事实查找”型子任务。\n",
    "- **工具描述的重要性**：`ToolMetadata` 中的 `description` 字段至关重要。`SubQuestionQueryEngine` 的 LLM 正是依靠这些描述来判断应该将哪个子问题路由到哪个工具。描述越清晰、越准确，路由的效果就越好。\n",
    "- **底层引擎的质量**：子问题引擎的最终表现，完全取决于其下属工具（`QueryEngineTool`）的能力。正因如此，我们为每个工具都配置了层次化、自动合并的高级检索器，确保每个子问题都能得到高质量的回答，为最终的综合提供了优质的“原料”。\n",
    "\n",
    "**5. 常见问题与解答 (FAQ)**\n",
    "\n",
    "- **问**：如果财报的格式不完全一样怎么办？\n",
    "  - **答**：这是真实世界的常态。你需要增强你的自定义解析器（如果使用的话），使其能处理多种正则表达式模式。或者，依赖 `LlamaParse` 这样更强大的通用解析器，它对格式变化的鲁棒性更强。\n",
    "- **问**：`SubQuestionQueryEngine` 会不会生成太多子问题，导致成本过高？\n",
    "  - **答**：有可能。你可以通过选择一个更“经济”的 LLM（如 `gpt-3.5-turbo`）专门用于子问题生成，来控制成本。同时，清晰的工具描述也能帮助 LLM 更精准地生成必要的子问题，避免冗余。\n",
    "- **问**：如何处理跨越多年份的对比？\n",
    "  - **答**：你需要为每一年的财报都构建一个独立的 `QueryEngineTool`，并清晰地在 `description` 中注明其年份（例如，“提供关于 FutureTech 公司 **2023** 年财报的信息”）。这样，子问题引擎就能正确地将涉及特定年份的子问题路由到对应的工具。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目二：构建多模态电商产品搜索引擎\n",
    "技术栈：Multi-Modal RAG，CLIP 嵌入，OpenAIAgent，CohereRerank\n",
    "\n",
    "目标：支持图文混合搜索，能理解“帮我找一款看起来像A，但有B功能的红色椅子”\n",
    "\n",
    "**1. 背景与业务价值**\n",
    "\n",
    "传统的电商搜索依赖于文本标签和描述。但用户的真实购物意图往往是视觉化的、模糊的。例如，用户可能会想“我想要一个像我家沙发一样的扶手椅，但是是蓝色的”，或者“我在街上看到了这款运动鞋，它长什么样我说不清，但我有照片”。\n",
    "\n",
    "我们的目标是构建一个支持**图文混合搜索**的多模态搜索引擎，它能理解用户的复杂、跨模态意图。例如，用户可以上传一张椅子的图片，并输入文本“帮我找一款类似的，但是是皮革材质的红色椅子”。\n",
    "\n",
    "**2. 核心技术栈解析**\n",
    "\n",
    "- **多模态** RAG **(`Multi-Modal RAG`)**: LlamaIndex 提供了构建多模态 RAG 的能力，其核心是能够索引和查询**图像**。这需要一个能同时理解文本和图像的多模态嵌入模型。\n",
    "- **CLIP 嵌入 (`CLIPEmbedding`)**: CLIP (Contrastive Language–Image Pre-training) 是由 OpenAI 开发的、业界最经典的多模态嵌入模型之一。它能将文本和图像映射到同一个向量空间，使得我们可以计算一张图片和一段描述之间的语义相似度。\n",
    "- **`OpenAIAgent`**: 我们需要一个智能体来解析用户的混合输入，并决定是进行文本搜索、图像搜索，还是图文联合搜索。基于 Function Calling 的 `OpenAIAgent` 非常适合这项任务，它能结构化地调用我们提供的各种搜索工具。\n",
    "- **`CohereRerank` (或 `SentenceTransformerRerank`)**: 在初步检索到一组图文混合的结果后，我们需要一个强大的重排器，根据用户的完整意图（结合图片和文本描述）对结果进行精加工，将最匹配的商品排在最前面。\n",
    "\n",
    "**3. 完整代码示例（概念与结构）**\n",
    "\n",
    "构建一个完整的多模态搜索引擎涉及复杂的模型下载和数据准备。下面的代码将重点展示其**核心架构和关键实现逻辑**，并使用占位符代表真实的多模态模型和数据，确保代码结构清晰、逻辑完整，方便您替换为真实组件。\n",
    "\n",
    "**环境依赖**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM 和 Embedding 模型配置完成。\n",
      "\n",
      "--- 正在本地创建商品图片 ---\n",
      "成功在本地创建图片: ./product_images\\chair1_brown_fabric.jpg\n",
      "成功在本地创建图片: ./product_images\\chair2_red_leather.jpg\n",
      "成功在本地创建图片: ./product_images\\chair3_blue_wooden.jpg\n",
      "\n",
      "✅ 成功加载并更新了多模态文档的元数据。\n",
      "\n",
      "✅ ReAct Agent 创建成功。\n",
      "\n",
      "--- 用户提出多模态查询 ---\n",
      "我喜欢这张图 'chair1_brown_fabric.jpg' 里的椅子，但我想要一个红色的、皮革材质的款式。\n",
      "> Running step 8162c8d4-0b28-4b4d-ac14-7ca2927e32a9. Step input: Use the 'product_search' tool. The reference image is located at the path './product_images\\chair1_brown_fabric.jpg'. The user's additional text requirement is 'a red one made of leather'.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use the product_search tool to find a similar product based on the reference image and the user's additional text requirement.\n",
      "Action: product_search\n",
      "Action Input: {'image_path': './product_images\\\\chair1_brown_fabric.jpg', 'text_query': 'a red one made of leather'}\n",
      "\u001b[0m\n",
      "--- [Tool Called] 正在执行多模态搜索 ---\n",
      "--- [Tool Input] 参考图片: ./product_images\\chair1_brown_fabric.jpg\n",
      "--- [Tool Input] 文本要求: a red one made of leather\n",
      "--- [Tool Logic] 模拟检索：检测到 '红色' 和 '皮革' 关键字，匹配成功。\n",
      "\u001b[1;3;34mObservation: [{'name': 'Vintage Leather Seat', 'material': 'leather', 'color': 'red', 'path': './product_images\\\\chair2_red_leather.jpg'}]\n",
      "\u001b[0m> Running step 175b8a32-2174-4062-9901-0c4f1c66617c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: I found a similar product named 'Vintage Leather Seat' that matches your requirements. It is made of leather and is red in color. The image path is './product_images\\chair2_red_leather.jpg'.\n",
      "\u001b[0m\n",
      "\n",
      "----------------------\n",
      "--- Agent 的最终回答 ---\n",
      "----------------------\n",
      "I found a similar product named 'Vintage Leather Seat' that matches your requirements. It is made of leather and is red in color. The image path is './product_images\\chair2_red_leather.jpg'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict\n",
    "\n",
    "# 【重大变更】我们不再需要 requests 库\n",
    "# import requests\n",
    "# 我们现在需要 PIL/Pillow 的更多功能来创建图片\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# -- 核心模块导入 --\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    Document,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    ")\n",
    "from llama_index.embeddings.dashscope import DashScopeEmbedding\n",
    "from llama_index.llms.deepseek import DeepSeek\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.core.agent import ReActAgent\n",
    "\n",
    "# --- 1. 环境与模型配置 ---\n",
    "# 请确保您的 API Key 是有效且未过期的\n",
    "# os.environ[\"DASHSCOPE_API_KEY\"] = \"sk-...\"\n",
    "# os.environ[\"DEEPSEEK_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "# 配置 Embedding 模型\n",
    "Settings.embed_model = DashScopeEmbedding(model_name=\"text-embedding-v2\")\n",
    "# 配置 LLM\n",
    "Settings.llm = DeepSeek(model=\"deepseek-chat\")\n",
    "print(\"✅ LLM 和 Embedding 模型配置完成。\")\n",
    "\n",
    "# --- 2. 准备多模态数据 ---\n",
    "IMAGE_DIR = \"./product_images\"\n",
    "if not os.path.exists(IMAGE_DIR):\n",
    "    os.makedirs(IMAGE_DIR)\n",
    "\n",
    "\n",
    "# 【重大变更】用本地图片生成函数替换网络下载函数\n",
    "def create_placeholder_image(name: str, text: str, color: str, text_color: str = \"white\"):\n",
    "    \"\"\"\n",
    "    在本地直接创建一张带有文字的纯色占位图。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 定义图片尺寸和背景色\n",
    "        img_size = (600, 400)\n",
    "        img = Image.new('RGB', img_size, color=color)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "\n",
    "        # 使用Pillow的默认字体\n",
    "        try:\n",
    "            # 尝试加载一个较大的默认字体\n",
    "            font = ImageFont.load_default(size=48)\n",
    "        except AttributeError:\n",
    "            # 兼容旧版Pillow\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "        # 计算文字位置使其居中\n",
    "        text_bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "        position = ((img_size[0] - text_width) / 2, (img_size[1] - text_height) / 2)\n",
    "\n",
    "        # 在图片上绘制文字\n",
    "        draw.text(position, text, fill=text_color, font=font)\n",
    "\n",
    "        path = os.path.join(IMAGE_DIR, f\"{name}.jpg\")\n",
    "        img.save(path)\n",
    "        print(f\"成功在本地创建图片: {path}\")\n",
    "        return path\n",
    "    except Exception as e:\n",
    "        print(f\"本地创建图片失败 {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# 【重大变更】调用新的本地生成函数\n",
    "print(\"\\n--- 正在本地创建商品图片 ---\")\n",
    "img_path1 = create_placeholder_image(\"chair1_brown_fabric\", \"Brown Fabric Chair\", \"#8B4513\")\n",
    "img_path2 = create_placeholder_image(\"chair2_red_leather\", \"Red Leather Chair\", \"#B22222\")\n",
    "img_path3 = create_placeholder_image(\"chair3_blue_wooden\", \"Blue Wooden Chair\", \"#4682B4\")\n",
    "\n",
    "# 只有当所有图片都创建成功时，才继续执行\n",
    "if all([img_path1, img_path2, img_path3]):\n",
    "    image_documents = SimpleDirectoryReader(IMAGE_DIR).load_data()\n",
    "\n",
    "    for doc in image_documents:\n",
    "        file_name = doc.metadata.get(\"file_name\", \"\")\n",
    "        if \"chair1_brown_fabric\" in file_name:\n",
    "            doc.metadata.update({\"name\": \"Modern Fabric Armchair\", \"material\": \"fabric\", \"color\": \"brown\"})\n",
    "        elif \"chair2_red_leather\" in file_name:\n",
    "            doc.metadata.update({\"name\": \"Vintage Leather Seat\", \"material\": \"leather\", \"color\": \"red\"})\n",
    "        elif \"chair3_blue_wooden\" in file_name:\n",
    "            doc.metadata.update({\"name\": \"Classic Wooden Chair\", \"material\": \"wood\", \"color\": \"blue\"})\n",
    "    print(\"\\n✅ 成功加载并更新了多模态文档的元数据。\")\n",
    "\n",
    "\n",
    "    # --- 3. 定义一个模拟的搜索工具 ---\n",
    "    def find_similar_products(image_path: str, text_query: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        根据一张参考图片和一段文本描述，来查找相似的产品。\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- [Tool Called] 正在执行多模态搜索 ---\")\n",
    "        print(f\"--- [Tool Input] 参考图片: {image_path}\")\n",
    "        print(f\"--- [Tool Input] 文本要求: {text_query}\")\n",
    "\n",
    "        text_query_lower = text_query.lower()\n",
    "        found_red = \"red\" in text_query_lower or \"红\" in text_query_lower\n",
    "        found_leather = \"leather\" in text_query_lower or \"皮\" in text_query_lower\n",
    "\n",
    "        if found_red and found_leather:\n",
    "            print(\"--- [Tool Logic] 模拟检索：检测到 '红色' 和 '皮革' 关键字，匹配成功。\")\n",
    "            return [{\"name\": \"Vintage Leather Seat\", \"material\": \"leather\", \"color\": \"red\", \"path\": img_path2}]\n",
    "        else:\n",
    "            print(f\"--- [Tool Logic] 模拟检索：未完全匹配。\")\n",
    "            return []\n",
    "\n",
    "\n",
    "    product_search_tool = FunctionTool.from_defaults(\n",
    "        fn=find_similar_products,\n",
    "        name=\"product_search\",\n",
    "        description=\"用于根据一张参考图片和一段文字描述（如颜色、材质）来寻找相似商品。\"\n",
    "    )\n",
    "\n",
    "    # --- 4. 构建 Agent ---\n",
    "    agent = ReActAgent.from_tools([product_search_tool], llm=Settings.llm, verbose=True)\n",
    "    print(\"\\n✅ ReAct Agent 创建成功。\")\n",
    "\n",
    "    # --- 5. 提出一个多模态查询 ---\n",
    "    user_query = f\"我喜欢这张图 '{os.path.basename(img_path1)}' 里的椅子，但我想要一个红色的、皮革材质的款式。\"\n",
    "    print(f\"\\n--- 用户提出多模态查询 ---\\n{user_query}\")\n",
    "\n",
    "    agent_query = (\n",
    "        \"Use the 'product_search' tool. \"\n",
    "        f\"The reference image is located at the path '{img_path1}'. \"\n",
    "        f\"The user's additional text requirement is 'a red one made of leather'.\"\n",
    "    )\n",
    "\n",
    "    response = agent.chat(agent_query)\n",
    "\n",
    "    print(\"\\n\\n----------------------\")\n",
    "    print(\"--- Agent 的最终回答 ---\")\n",
    "    print(\"----------------------\")\n",
    "    print(str(response))\n",
    "else:\n",
    "    print(\"\\n❌ 由于图片创建失败，无法继续执行多模态 Agent 示例。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 关键点解析**\n",
    "\n",
    "- **统一的向量空间**：多模态 RAG 的核心魔法在于，像 CLIP 这样的模型能将看似不相关的图像和文本，映射到同一个高维向量空间中。在这个空间里，“一只狗的图片”的向量，会与“一条狗”这段文字的向量非常接近。\n",
    "- **Agent 的角色**：在这个项目中，Agent 扮演了“意图理解者”和“任务调度员”。它首先需要理解用户的自然语言输入，并从中解析出需要调用的工具（`product_search`）以及该工具所需的参数（`image_path` 和 `text_query`）。\n",
    "- **端到端流程**：一个完整的查询流程是：`用户混合输入 -> Agent 解析意图 -> 调用多模态搜索工具 -> 工具内部执行“图文联合检索 + 重排” -> 工具返回商品列表 -> Agent 组织语言并生成最终回答`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 项目三：构建能自我学习的 IT 运维知识库 Agent\n",
    "\n",
    "技术栈：Agentic ETL，自适应 RAG (Self-Correction)，LangGraph，NebulaGraph\n",
    "\n",
    "目标：能自动同步工单系统，构建知识图谱，并在回答不出问题时主动搜索内部文档学习\n",
    "\n",
    "**1. 背景与业务价值**\n",
    "\n",
    "在一个大型企业的 IT 部门，运维问题层出不穷。工单系统（如 Jira, ServiceNow）里记录了海量的历史问题、解决方案和讨论过程。这些数据是极其宝贵的知识财富，但往往以非结构化的形式沉睡在系统中。\n",
    "\n",
    "我们的目标是构建一个**能自我进化的智能运维 Agent**。它不仅能回答工程师提出的技术问题，还能：\n",
    "\n",
    "1. **主动学习**：定期同步新增的已解决工单，自动更新自己的知识库。\n",
    "2. **建立关系**：在学习过程中，自动构建实体（如服务器、应用、错误码）之间的关系，形成知识图谱。\n",
    "3. **自我修正**：当它发现检索到的知识无法很好地回答问题时，能意识到这一点，并尝试用更泛化或不同的方式重新检索。\n",
    "\n",
    "**2. 核心技术栈解析**\n",
    "\n",
    "这是最具挑战性的项目，集成了最前沿的理念：\n",
    "\n",
    "- **Agentic ETL (智能体驱动的 ETL)**：传统的 ETL 流程是固定的、基于规则的。Agentic ETL 则是让一个智能体来负责数据的提取、转换和加载。例如，让一个 Agent 去读取新的工单，用 LLM 的理解能力去总结问题、解决方案和涉及的关键实体，然后结构化地存入知识库。\n",
    "- **自适应 RAG (`Self-Correction`)**: 这是一种让 RAG 系统具备“反思”能力的机制。在生成最终答案之前，会有一个“评估”步骤。如果评估发现检索到的上下文与问题相关性不高，或者内容不足以回答问题，系统就会触发一个“修正”循环，例如用 `Step-Back Prompting` 的方法重新发起一次检索。\n",
    "- **`LangGraph`**: 正如您所偏好，这个项目中的复杂、循环、有状态的工作流（学习->提问->评估->修正->回答），用 `LangGraph` 来编排是最佳选择。它能完美地定义和管理 Agent 的状态（State）和节点之间的转移逻辑。\n",
    "- **`NebulaGraph` (或等效图数据库)**：为了存储实体间的关系，我们需要一个真正的图数据库。`NebulaGraph` 是一个高性能的开源图数据库，非常适合存储和查询知识图谱。\n",
    "\n",
    "**3. 概念架构与伪代码**\n",
    "\n",
    "构建这样一个完整的系统代码量巨大，且依赖复杂的外部服务（Jira, NebulaGraph）。因此，我们将重点放在**展示其核心的 `LangGraph` 架构和工作流逻辑**上，并用伪代码和占位符函数来代表与外部系统的交互。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是一个高级的概念性伪代码，展示了 LangGraph 的架构思想\n",
    "# 完整的运行需要配置 LangGraph, NebulaGraph, Jira API 等\n",
    "# \n",
    "# from typing import List, TypedDict, Annotated\n",
    "# from langgraph.graph import StateGraph, END\n",
    "# \n",
    "# # --- 1. 定义 Agent 的状态 ---\n",
    "# # 这个 State 对象会在图的各个节点之间传递和更新\n",
    "# class ITKBAgentState(TypedDict):\n",
    "#     original_query: str\n",
    "#     retrieved_docs: List[Document]\n",
    "#     generation: str\n",
    "#     is_final: bool\n",
    "#     correction_attempts: int\n",
    "#\n",
    "# # --- 2. 定义图中的节点 (每个节点都是一个函数或可调用对象) ---\n",
    "#\n",
    "# def retrieve_knowledge(state: ITKBAgentState):\n",
    "#     \"\"\"节点1：从 LlamaIndex (向量+图) 中检索知识\"\"\"\n",
    "#     print(\"--- 正在检索知识... ---\")\n",
    "#     # 这是一个 LlamaIndex 的高级检索器，可能融合了向量和图检索\n",
    "#     # retriever = get_hybrid_retriever()\n",
    "#     # docs = retriever.retrieve(state[\"original_query\"])\n",
    "#     # return {\"retrieved_docs\": docs, \"correction_attempts\": 1}\n",
    "#     pass\n",
    "#\n",
    "# def self_correction_check(state: ITKBAgentState):\n",
    "#     \"\"\"节点2：评估检索到的文档质量\"\"\"\n",
    "#     print(\"--- 正在进行自我修正检查... ---\")\n",
    "#     # 使用 LLM 判断 retrieved_docs 是否足够回答 original_query\n",
    "#     # relevance_score = evaluate_relevance(state[\"retrieved_docs\"], state[\"original_query\"])\n",
    "#     # if relevance_score > 0.7:\n",
    "#     #     return {\"is_final\": False} # 足够好，进入生成环节\n",
    "#     # else:\n",
    "#     #     # 不够好，决定是否需要再次尝试\n",
    "#     #     if state[\"correction_attempts\"] >= 2:\n",
    "#     #         return {\"is_final\": True} # 尝试次数过多，放弃\n",
    "#     #     else:\n",
    "#     #         # 在这里可以实现 Step-Back Prompting 或其他查询重写逻辑\n",
    "#     #         print(\"--- 相关性不足，尝试用 Step-Back Query 重写 ---\")\n",
    "#     #         # new_query = generate_step_back_query(state[\"original_query\"])\n",
    "#     #         # return {\"original_query\": new_query, \"correction_attempts\": state[\"correction_attempts\"] + 1}\n",
    "#     pass\n",
    "#\n",
    "# def generate_answer(state: ITKBAgentState):\n",
    "#     \"\"\"节点3：基于检索到的文档生成最终答案\"\"\"\n",
    "#     print(\"--- 正在生成最终答案... ---\")\n",
    "#     # prompt = f\"Context: {state['retrieved_docs']}\\n\\nQuery: {state['original_query']}\\n\\nAnswer:\"\n",
    "#     # answer = llm.complete(prompt)\n",
    "#     # return {\"generation\": answer, \"is_final\": True}\n",
    "#     pass\n",
    "#\n",
    "# # --- 3. 定义条件边，决定工作流的走向 ---\n",
    "# def decide_next_step(state: ITKBAgentState):\n",
    "#     \"\"\"条件边：在自我修正检查后，决定是去生成答案，还是重新检索\"\"\"\n",
    "#     if state.get(\"is_final\", False):\n",
    "#         return \"generate\"\n",
    "#     else:\n",
    "#         return \"retrieve\" # 返回到 retrieve 节点，使用新的 query\n",
    "#\n",
    "# # --- 4. 构建 LangGraph ---\n",
    "# workflow = StateGraph(ITKBAgentState)\n",
    "# workflow.add_node(\"retrieve\", retrieve_knowledge)\n",
    "# workflow.add_node(\"self_correction\", self_correction_check)\n",
    "# workflow.add_node(\"generate\", generate_answer)\n",
    "#\n",
    "# # 设置入口点\n",
    "# workflow.set_entry_point(\"retrieve\")\n",
    "#\n",
    "# # 添加边\n",
    "# workflow.add_edge(\"retrieve\", \"self_correction\")\n",
    "# workflow.add_conditional_edges(\n",
    "#     \"self_correction\",\n",
    "#     decide_next_step,\n",
    "#     {\"generate\": \"generate\", \"retrieve\": \"retrieve\"}\n",
    "# )\n",
    "# workflow.add_edge(\"generate\", END)\n",
    "#\n",
    "# # 编译成可执行的 App\n",
    "# app = workflow.compile()\n",
    "#\n",
    "# # --- 5. 运行 Agentic 工作流 ---\n",
    "# # inputs = {\"original_query\": \"服务器 'apollo-01' 的 CPU 占用率过高是什么原因？\"}\n",
    "# # for output in app.stream(inputs):\n",
    "# #     for key, value in output.items():\n",
    "# #         print(f\"节点 '{key}' 的输出: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. 关键点解析**\n",
    "\n",
    "- **循环与状态**：`LangGraph` 的核心优势在于其原生的循环处理能力。通过条件边（`conditional_edges`），我们可以轻松实现“检索-评估-再检索”这样的自我修正循环，这是传统线性 Agent 难以做到的。\n",
    "- **LlamaIndex** as a **Tool Provider**: 在这个架构中，LlamaIndex 扮演了至关重要的“工具提供者”角色。无论是 `retrieve_knowledge` 节点中的混合检索器，还是 `self_correction_check` 节点中可能用到的评估工具，都是由 LlamaIndex 构建和支持的。\n",
    "- **Agentic ETL 的实现**：一个完整的 Agentic ETL 流程可以被实现为另一个独立的 `LangGraph`。这个图会定期被触发，其节点包括“从 Jira 拉取新工单”、“用 LLM 总结工单”、“提取实体和关系”、“更新向量索引”、“更新图数据库”等。\n",
    "\n",
    "**总章小结**\n",
    "\n",
    "恭喜您！走完这三个项目，您已经将 LlamaIndex 的能力从理论认知，升华为了可以解决真实、复杂、动态问题的工程实践。您不仅掌握了如何构建一个静态的 RAG 系统，更学会了如何设计一个能与外部世界交互、能自我进化、能协同工作的智能体系统。\n",
    "\n",
    "这标志着您已经踏入了高级 LLM 应用开发者的行列。前路漫漫，探索永无止境，但您已拥有了最坚实的地图和最精良的装备。祝您在 AI 的星辰大海中，构建出更多令人惊叹的应用！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
